<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lithology classification using Hugging Face, part 2 | J-M’s “lab notebook”</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lithology classification using Hugging Face, part 2" />
<meta name="author" content="J-M" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring the use of NLP to exploit geologic information. Trying to classify lithologies." />
<meta property="og:description" content="Exploring the use of NLP to exploit geologic information. Trying to classify lithologies." />
<link rel="canonical" href="https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html" />
<meta property="og:url" content="https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html" />
<meta property="og:site_name" content="J-M’s “lab notebook”" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-13T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Lithology classification using Hugging Face, part 2" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"J-M"},"dateModified":"2022-06-13T00:00:00-05:00","datePublished":"2022-06-13T00:00:00-05:00","description":"Exploring the use of NLP to exploit geologic information. Trying to classify lithologies.","headline":"Lithology classification using Hugging Face, part 2","mainEntityOfPage":{"@type":"WebPage","@id":"https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html"},"url":"https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/work-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jmp75.github.io/work-blog/feed.xml" title="J-M's &quot;lab notebook&quot;" /><link rel="shortcut icon" type="image/x-icon" href="/work-blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/work-blog/">J-M&#39;s &quot;lab notebook&quot;</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/work-blog/about/">About Me</a><a class="page-link" href="/work-blog/search/">Search</a><a class="page-link" href="/work-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lithology classification using Hugging Face, part 2</h1><p class="page-description">Exploring the use of NLP to exploit geologic information. Trying to classify lithologies.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-06-13T00:00:00-05:00" itemprop="datePublished">
        Jun 13, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">J-M</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/work-blog/categories/#hugging-face">hugging-face</a>
        &nbsp;
      
        <a class="category-tags-link" href="/work-blog/categories/#NLP">NLP</a>
        &nbsp;
      
        <a class="category-tags-link" href="/work-blog/categories/#lithology">lithology</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/jmp75/work-blog/tree/master/_notebooks/2022-06-13-lithology-classification-hugging-face-2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/work-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          
          
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#About">About </a></li>
<li class="toc-entry toc-h1"><a href="#Kernel-installation">Kernel installation </a></li>
<li class="toc-entry toc-h1"><a href="#Walkthrough">Walkthrough </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Imbalanced-data-sets">Imbalanced data sets </a></li>
<li class="toc-entry toc-h2"><a href="#Subsetting">Subsetting </a></li>
<li class="toc-entry toc-h2"><a href="#Class-imbalance">Class imbalance </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Resample-with-replacement">Resample with replacement </a></li>
<li class="toc-entry toc-h3"><a href="#Dealing-with-imbalanced-classes-with-weights">Dealing with imbalanced classes with weights </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Tokenisation">Tokenisation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Bump-on-the-road;-download-operations-taking-too-long">Bump on the road; download operations taking too long </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Create-dataset-and-tokenisation">Create dataset and tokenisation </a></li>
<li class="toc-entry toc-h2"><a href="#Training?">Training? </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Stocktake-and-conclusion">Stocktake and conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-06-13-lithology-classification-hugging-face-2.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="About">
<a class="anchor" href="#About" aria-hidden="true"><span class="octicon octicon-link"></span></a>About<a class="anchor-link" href="#About"> </a>
</h1>
<p>This is a continuation of <a href="https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html">Lithology classification using Hugging Face, part 1</a>.</p>
<p>We saw in the previous post that the Namoi lithology logs data had their primary (major) lithology mostly completed. A substantial proportion had the label <code>None</code> nevertheless, despite descriptions that looked like they would obviously lead to a categorisation. There were many labels, with a long-tailed frequency histogram.</p>
<p>The aim of this post is (was) to get a classification training happening.</p>
<p><strong>Spoiler alert: it won't</strong>. Almost.</p>
<p>Rather than write a post after the fact pretending it was a totally smooth journey, the following walktrough <em>deliberately</em> keeps and highlights issues, albeit succinctly. <strong>Don't</strong> jump to the conclusion that we will not get there eventually, or that Hugging Face is not good. When you adapt prior work to your own use case, you <strong>will</strong> likely stumble, so this post will make you feel in good company.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Kernel-installation">
<a class="anchor" href="#Kernel-installation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kernel installation<a class="anchor-link" href="#Kernel-installation"> </a>
</h1>
<p>The previous post was about data exploration and used mostly facilities such as pandas, not any deep learning related material. This post will, so we need to install Hugging Face. I did bump into a couple of issues while trying to get an environment going. I will not give the full grubby details, but highlight upfront a couple of things:</p>
<ul>
<li>Do create a new dedicated conda environment for your work with Hugging Face, even if you already have an environment with e.g. pytorch you'd like to reuse.</li>
<li>The version 4.11.3 of HF <code>transformers</code> on the conda channel <code>huggingface</code>, at the time of writing, has a <a href="https://github.com/nlp-with-transformers/notebooks/issues/31">bug</a>. You should install the packages from the <code>conda-forge</code> channel.</li>
</ul>
<p>In a nutshell, for Linux:</p>
<div class="highlight"><pre><span></span><span class="nv">myenv</span><span class="o">=</span>hf
mamba create -n <span class="nv">$myenv</span> <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.9 -c conda-forge
mamba install -n <span class="nv">$myenv</span> --yes ipykernel matplotlib sentencepiece scikit-learn -c conda-forge
mamba install -n <span class="nv">$myenv</span> --yes <span class="nv">pytorch</span><span class="o">=</span><span class="m">1</span>.11 -c pytorch -c nvidia -c conda-forge
mamba install -n <span class="nv">$myenv</span> --yes torchvision torchaudio -c pytorch -c nvidia -c conda-forge
mamba install -n <span class="nv">$myenv</span> --yes -c conda-forge datasets transformers
conda activate <span class="nv">$myenv</span>
python -m ipykernel install --user --name <span class="nv">$myenv</span> --display-name <span class="s2">"Hugging Face"</span>
</pre></div>
<p>and in Windows:</p>
<div class="highlight"><pre><span></span><span class="k">set</span> <span class="nv">myenv</span><span class="p">=</span>hf
mamba create -n <span class="nv">%myenv%</span> python=3.9 -c conda-forge
mamba install -n <span class="nv">%myenv%</span> --yes ipykernel matplotlib sentencepiece scikit-learn -c conda-forge
mamba install -n <span class="nv">%myenv%</span> --yes pytorch=1.11 -c pytorch -c nvidia -c conda-forge
mamba install -n <span class="nv">%myenv%</span> --yes torchvision torchaudio -c pytorch -c nvidia -c conda-forge
mamba install -n <span class="nv">%myenv%</span> --yes -c conda-forge datasets transformers
conda activate <span class="nv">%myenv%</span>
python -m ipykernel install --user --name <span class="nv">%myenv%</span> --display-name <span class="s2">"Hugging Face"</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Walkthrough">
<a class="anchor" href="#Walkthrough" aria-hidden="true"><span class="octicon octicon-link"></span></a>Walkthrough<a class="anchor-link" href="#Walkthrough"> </a>
</h1>
<p>Let's get on with all the imports upfront (not obvious, mind you, but after the fact...)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">ClassLabel</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># Some column string identifiers</span>
<span class="n">MAJOR_CODE</span> <span class="o">=</span> <span class="s2">"MajorLithCode"</span>
<span class="n">MAJOR_CODE_INT</span> <span class="o">=</span> <span class="s2">"MajorLithoCodeInt"</span>  <span class="c1"># We will create a numeric representation of labels, which is (I think?) required by HF.</span>
<span class="n">MINOR_CODE</span> <span class="o">=</span> <span class="s2">"MinorLithCode"</span>
<span class="n">DESC</span> <span class="o">=</span> <span class="s2">"Description"</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fn</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span> <span class="o">/</span> <span class="s2">"data/ela/shp_namoi_river/NGIS_LithologyLog.csv"</span>
<span class="n">litho_logs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="p">{</span><span class="s2">"FromDepth"</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="s2">"ToDepth"</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">MAJOR_CODE</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">MINOR_CODE</span><span class="p">:</span> <span class="nb">str</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># To avoid importing from the ela package, copy a couple of functions:</span>
<span class="c1"># from ela.textproc import token_freq, plot_freq</span>


<span class="k">def</span> <span class="nf">token_freq</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">n_most_common</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">list_most_common</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">n_most_common</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">list_most_common</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"token"</span><span class="p">,</span> <span class="s2">"frequency"</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">plot_freq</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">y_log</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">"token"</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">):</span>
    <span class="sd">"""Plot a sorted histogram of work frequencies</span>

<span class="sd">    Args:</span>
<span class="sd">        dataframe (pandas dataframe): frequency of tokens, typically with colnames ["token","frequency"]</span>
<span class="sd">        y_log (bool): should there be a log scale on the y axis</span>
<span class="sd">        x (str): name of the columns with the tokens (i.e. words)</span>
<span class="sd">        figsize (tuple):</span>
<span class="sd">        fontsize (int):</span>

<span class="sd">    Returns:</span>
<span class="sd">        barplot: plot</span>

<span class="sd">    """</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_log</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">"log"</span><span class="p">,</span> <span class="n">nonposy</span><span class="o">=</span><span class="s2">"clip"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>


<span class="n">litho_classes</span> <span class="o">=</span> <span class="n">litho_logs</span><span class="p">[</span><span class="n">MAJOR_CODE</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">df_most_common</span> <span class="o">=</span> <span class="n">token_freq</span><span class="p">(</span><span class="n">litho_classes</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plot_freq</span><span class="p">(</span><span class="n">df_most_common</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Imbalanced-data-sets">
<a class="anchor" href="#Imbalanced-data-sets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Imbalanced data sets<a class="anchor-link" href="#Imbalanced-data-sets"> </a>
</h2>
<p>From the histogram above, it is pretty clear that labels are also not uniform an we have a class imbalance. Remember to skim <a href="https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html">Lithology classification using Hugging Face, part 1</a> for the initial data exploration if you have not done so already.</p>
<p>For the sake of the exercise in this post, I will reduce arbitrarily the number of labels used in this post, by just "forgetting" the less represented classes.</p>
<p>There are many resources about class imbalances. One of them is <a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset">8 Tactics to combat imbalanced classes in your machine learning dataset</a></p>
<p>Let's see what labels we may want to keep for this post:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sample_desc_for_code</span><span class="p">(</span><span class="n">major_code</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">is_code</span> <span class="o">=</span> <span class="n">litho_logs</span><span class="p">[</span><span class="n">MAJOR_CODE</span><span class="p">]</span> <span class="o">==</span> <span class="n">major_code</span>
    <span class="n">coded</span> <span class="o">=</span> <span class="n">litho_logs</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">is_code</span><span class="p">][</span><span class="n">DESC</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">coded</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_desc_for_code</span><span class="p">(</span><span class="s2">"UNKN"</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The "unknown" category is rather interesting in fact, and worth keeping as a valid class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Subsetting">
<a class="anchor" href="#Subsetting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Subsetting<a class="anchor-link" href="#Subsetting"> </a>
</h2>
<p>Let's keep "only" the main labels, for the sake of this exercise. We will remove None however, despite its potential interest. We will (hopefully) revisit this in another post.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">labels_kept</span> <span class="o">=</span> <span class="n">df_most_common</span><span class="p">[</span><span class="s2">"token"</span><span class="p">][:</span><span class="mi">17</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># 17 first classes somewhat arbitraty</span>
<span class="n">labels_kept</span> <span class="o">=</span> <span class="n">labels_kept</span><span class="p">[</span><span class="n">labels_kept</span> <span class="o">!=</span> <span class="s2">"None"</span><span class="p">]</span>
<span class="n">labels_kept</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">kept</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="ow">in</span> <span class="n">labels_kept</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">litho_classes</span><span class="p">]</span>
<span class="n">litho_logs_kept</span> <span class="o">=</span> <span class="n">litho_logs</span><span class="p">[</span><span class="n">kept</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>  <span class="c1"># avoid warning messages down the track.</span>
<span class="n">litho_logs_kept</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">ClassLabel</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">labels_kept</span><span class="p">)</span>
<span class="n">int_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="n">labels</span><span class="o">.</span><span class="n">str2int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">litho_logs_kept</span><span class="p">[</span><span class="n">MAJOR_CODE</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="p">])</span>
<span class="n">int_labels</span> <span class="o">=</span> <span class="n">int_labels</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span> <span class="c1"># to mimick chapter3 HF so far as I can see</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">litho_logs_kept</span><span class="p">[</span><span class="n">MAJOR_CODE_INT</span><span class="p">]</span> <span class="o">=</span> <span class="n">int_labels</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Class-imbalance">
<a class="anchor" href="#Class-imbalance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Class imbalance<a class="anchor-link" href="#Class-imbalance"> </a>
</h2>
<p>Even our subset of 16 classes is rather imbalanced; the number of "clay" labels is looking more than 30 times that of "coal" just by eyeballing.</p>
<p>The post by Jason Brownlee <a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset">8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset</a>, outlines several approaches. One of them is to resample from labels, perhaps with replacement, to equalise classes. It is a relatively easy approach to implement, but there are issues, growing with the level of imbalance. Notably, if too many rows from underrepresented classes are repeated, there is an increased tendency to overfitting at training.</p>
<p>The video <a href="https://youtu.be/u--UVvH-LIQ?t=669">Simple Training with the 🤗 Transformers Trainer (at 669 seconds)</a> also explains the issues with imbalances and crude resampling. It offers instead a solution with class weighting that is more robust. That approach is evoked in Jason's post, but the video has a "Hugging Face style" implementation ready to repurpose.</p>
<h3 id="Resample-with-replacement">
<a class="anchor" href="#Resample-with-replacement" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resample with replacement<a class="anchor-link" href="#Resample-with-replacement"> </a>
</h3>
<p>Just for information, what we'd do with a relatively crude resampling may be:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">sample_major_lithocode</span><span class="p">(</span><span class="n">dframe</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">dframe</span><span class="p">[</span><span class="n">dframe</span><span class="p">[</span><span class="n">MAJOR_CODE</span><span class="p">]</span> <span class="o">==</span> <span class="n">code</span><span class="p">]</span>
    <span class="n">replace</span> <span class="o">=</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="n">replace</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_major_lithocode</span><span class="p">(</span><span class="n">litho_logs_kept</span><span class="p">,</span> <span class="s2">"CLAY"</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">balanced_litho_logs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">sample_major_lithocode</span><span class="p">(</span><span class="n">litho_logs_kept</span><span class="p">,</span> <span class="n">code</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">code</span> <span class="ow">in</span> <span class="n">labels_kept</span>
<span class="p">]</span>
<span class="n">balanced_litho_logs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">balanced_litho_logs</span><span class="p">)</span>
<span class="n">balanced_litho_logs</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_freq</span><span class="p">(</span><span class="n">token_freq</span><span class="p">(</span><span class="n">balanced_litho_logs</span><span class="p">[</span><span class="n">MAJOR_CODE</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dealing-with-imbalanced-classes-with-weights">
<a class="anchor" href="#Dealing-with-imbalanced-classes-with-weights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dealing with imbalanced classes with weights<a class="anchor-link" href="#Dealing-with-imbalanced-classes-with-weights"> </a>
</h3>
<p>Instead of the resampling above, we adapt the approach creating weights for the Trainer we will run.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sorted_counts</span> <span class="o">=</span> <span class="n">litho_logs_kept</span><span class="p">[</span><span class="n">MAJOR_CODE</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">sorted_counts</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sorted_counts</span> <span class="o">/</span> <span class="n">sorted_counts</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">class_weights</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sorted_counts</span> <span class="o">/</span> <span class="n">sorted_counts</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">values</span>
<span class="n">class_weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We check that cuda is available (of course optional)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On Linux if you have a DELL laptop with an NVIDIA card, but <code>nvidia-smi</code> returns: <code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running</code>, you may need to change your kernel specification file $HOME/.local/share/jupyter/kernels/hf/kernel.json. This behavior seems to depend on the version of Linux kernel you have. It certainly changed out of the blue for me from yesterday, despite no change that I can tell.</p>
<p><code>optirun nvidia-smi</code> returning a proper graphic card report should be a telltale sign you have to update your kernel.json like so:</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
 <span class="nt">"argv"</span><span class="p">:</span> <span class="p">[</span>
  <span class="s2">"optirun"</span><span class="p">,</span>
  <span class="s2">"/home/your_ident/miniconda/envs/hf/bin/python"</span><span class="p">,</span>
  <span class="s2">"-m"</span><span class="p">,</span>
  <span class="s2">"ipykernel_launcher"</span><span class="p">,</span>
  <span class="s2">"-f"</span><span class="p">,</span>
  <span class="s2">"{connection_file}"</span>
 <span class="p">],</span>
 <span class="nt">"display_name"</span><span class="p">:</span> <span class="s2">"Hugging Face"</span><span class="p">,</span>
 <span class="nt">"language"</span><span class="p">:</span> <span class="s2">"python"</span><span class="p">,</span>
 <span class="nt">"metadata"</span><span class="p">:</span> <span class="p">{</span>
  <span class="nt">"debugger"</span><span class="p">:</span> <span class="kc">true</span>
 <span class="p">}</span>
<span class="p">}</span>
</pre></div>
<p>You may need to restart jupyter-lab, or visual studio code, etc., for change to take effect. Restarting the kernel may not be enough, conter-intuitively.</p>
<p>Background details about optirun architecture at [Bumblebee Debian]<a href="https://wiki.debian.org/Bumblebee">https://wiki.debian.org/Bumblebee</a></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">class_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">class_weights</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">class_weights</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_nm</span> <span class="o">=</span> <span class="s2">"microsoft/deberta-v3-small"</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tokenisation">
<a class="anchor" href="#Tokenisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tokenisation<a class="anchor-link" href="#Tokenisation"> </a>
</h2>
<h3 id="Bump-on-the-road;-download-operations-taking-too-long">
<a class="anchor" href="#Bump-on-the-road;-download-operations-taking-too-long" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bump on the road; download operations taking too long<a class="anchor-link" href="#Bump-on-the-road;-download-operations-taking-too-long"> </a>
</h3>
<p>At this point I spent more hours than I wish I had on an issue, perhaps very unusual.</p>
<p>The operation <code>tokz = AutoTokenizer.from_pretrained(model_nm)</code> was taking an awful long time to complete:</p>
<div class="highlight"><pre><span></span>CPU times: user 504 ms, sys: 57.9 ms, total: 562 ms
Wall time: 14min 13s
</pre></div>
<p>To cut a long story short, I managed to figure out what was going on. It is documented on the Hugging Face forum at: <a href="https://discuss.huggingface.co/t/some-hf-operations-take-an-excessively-long-time-to-complete/18986">Some HF operations take an excessively long time to complete</a>. If you have issues where HF operations take a long time, read it.</p>
<p>Now back to the tokenisation story. Note that the local caching may be superflous if you do not encounter the issue just mentioned.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">128</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"./tokz_pretrained"</span><span class="p">)</span>
<span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="n">p</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span> <span class="k">else</span> <span class="n">model_nm</span>
<span class="c1"># https://discuss.huggingface.co/t/sentence-transformers-paraphrase-minilm-fine-tuning-error/9612/4</span>
<span class="n">tokz</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">model_max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
    <span class="n">tokz</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"./tokz_pretrained"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's see what this does on a typical lithology description</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokz</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">"CLAY, VERY SANDY"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well, the vocabulary is probably case sensitive and all the descriptions being uppercase in the source data are likely problematic. Let's check what happens on lowercase descriptions:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokz</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">"clay, very sandy"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This looks better. So let's change the descriptions to lowercase; we are not loosing any relevent information in this case, I think.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">litho_logs_kept</span><span class="p">[</span><span class="n">DESC</span><span class="p">]</span> <span class="o">=</span> <span class="n">litho_logs_kept</span><span class="p">[</span><span class="n">DESC</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">litho_logs_kept_mini</span> <span class="o">=</span> <span class="n">litho_logs_kept</span><span class="p">[[</span><span class="n">MAJOR_CODE_INT</span><span class="p">,</span> <span class="n">DESC</span><span class="p">]]</span>
<span class="n">litho_logs_kept_mini</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-dataset-and-tokenisation">
<a class="anchor" href="#Create-dataset-and-tokenisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create dataset and tokenisation<a class="anchor-link" href="#Create-dataset-and-tokenisation"> </a>
</h2>
<p>We want to create a dataset such that tokenised data is of uniform shape (better for running on GPU)
Applying the technique in <a href="https://youtu.be/_BZearw7f0w?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o&amp;t=150">this segment of the HF course video</a>. Cheating a bit on guessing the length (I know from offline checks that max is 90 tokens)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">litho_logs_kept_mini</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tok_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokz</span><span class="p">(</span>
        <span class="n">x</span><span class="p">[</span><span class="n">DESC</span><span class="p">],</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Youtube video above suggests to use <code>tok_ds = ds.map(tok_func, batched=True)</code> for a faster execution; however I ended up with the foollowing error:</p>
<div class="highlight"><pre><span></span>TypeError: Provided `function` which is applied to all elements of table returns a `dict` of types [&lt;class 'torch.Tensor'&gt;, &lt;class 'torch.Tensor'&gt;, &lt;class 'torch.Tensor'&gt;]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(&lt;class 'list'&gt;, &lt;class 'numpy.ndarray'&gt;)`.
</pre></div>
<p>The following non-batched option works in a reasonable time:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tok_func</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds_tmp</span> <span class="o">=</span> <span class="n">tok_ds</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">tok_ds_tmp</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">tok_ds_tmp</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_kept</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"./model_pretrained"</span><span class="p">)</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="n">p</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span> <span class="k">else</span> <span class="n">model_nm</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
                                                           <span class="c1"># label2id=label2id, id2label=id2label).to(device) </span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># litho_desc_list = [x for x in litho_logs_kept_mini[DESC].values]</span>
<span class="c1"># input_descriptions = tokz(litho_desc_list, padding=True, truncation=True, max_length=256, return_tensors='pt')</span>
<span class="c1"># input_descriptions['input_ids'].shape</span>
<span class="c1"># model(input_descriptions['input_ids'][:5,:], attention_mask=input_descriptions['attention_mask'][:5,:]).logits</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Transformers always assumes that your labels has the column name "labels". Odd, but at least this fosters a consistent system, so why not:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds</span> <span class="o">=</span> <span class="n">tok_ds</span><span class="o">.</span><span class="n">rename_columns</span><span class="p">({</span><span class="n">MAJOR_CODE_INT</span><span class="p">:</span> <span class="s2">"labels"</span><span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds</span> <span class="o">=</span> <span class="n">tok_ds</span><span class="o">.</span><span class="n">remove_columns</span><span class="p">([</span><span class="s1">'Description'</span><span class="p">,</span> <span class="s1">'__index_level_0__'</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Note that HF is supposed to take care of movind data to the GPU if available, so you should not ahve to manually copy the data to the GPU device</span>
<span class="n">tok_ds</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="s2">"torch"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#     evaluation_strategy="epoch", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,</span>
<span class="c1">#     num_train_epochs=epochs, weight_decay=0.01, report_to='none')</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dds</span> <span class="o">=</span> <span class="n">tok_ds</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dds</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">'labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds</span><span class="o">.</span><span class="n">features</span>

<span class="c1"># TODO:</span>
<span class="c1">#     This differs from chapter3 of HF course https://huggingface.co/course/chapter3/4?fw=pt    </span>
<span class="c1"># {'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),</span>
<span class="c1">#  'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),</span>
<span class="c1">#  'labels': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], id=None),</span>
<span class="c1">#  'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_ds</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># def compute_metrics(eval_pred):</span>
<span class="c1">#     logits, labels = eval_pred</span>
<span class="c1">#     predictions = np.argmax(logits, axis=-1)</span>
<span class="c1">#     return metric.compute(predictions=predictions, references=labels)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">WeightedLossTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># Feed inputs to model and extract logits</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logits"</span><span class="p">)</span>
        <span class="c1"># Extract Labels</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"labels"</span><span class="p">)</span>
        <span class="c1"># Define loss function with class weights</span>
        <span class="n">loss_func</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">)</span>
        <span class="c1"># Compute loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_outputs</span> <span class="k">else</span> <span class="n">loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span><span class="o">.</span><span class="n">label_ids</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">eval_pred</span><span class="o">.</span><span class="n">predictions</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">"weighted"</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">"f1"</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">output_dir</span> <span class="o">=</span> <span class="s2">"./hf_training"</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span> <span class="c1"># 128</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">8e-5</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dds</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]),</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">report_to</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda:0"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above nay not be strictly necessary, depending on your version of <code>transformers</code>. I bumped into the following issue, which was probably the transformers <a href="https://github.com/nlp-with-transformers/notebooks/issues/31#issuecomment-1075369210">4.11.3 bug</a>: <code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dds</span><span class="p">[</span><span class="s2">"train"</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dds</span><span class="p">[</span><span class="s2">"test"</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokz</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training?">
<a class="anchor" href="#Training?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training?<a class="anchor-link" href="#Training?"> </a>
</h2>
<p>You did read the introduction and its spoiler alert, right?</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Stocktake-and-conclusion">
<a class="anchor" href="#Stocktake-and-conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stocktake and conclusion<a class="anchor-link" href="#Stocktake-and-conclusion"> </a>
</h1>
<p>So, as announced at the start of this post, we hit a pothole in our journey.</p>
<div class="highlight"><pre><span></span>RuntimeError: The size of tensor a (768) must match the size of tensor b (128) at non-singleton dimension 3
</pre></div>
<p>Where the number (768) comes from is a bit of a mystery. I gather from Googling that this may have to do with the embedding of the Deberta model we are trying to fine tune, but I may be off the mark.</p>
<p>It is probably something at which an experience NLP practitioner will roll their eyes.</p>
<p>That's OK, We'll get there.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="jmp75/work-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/work-blog/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/work-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/work-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/work-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Work-related posts for the so-called grey literature.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jmp75" target="_blank" title="jmp75"><svg class="svg-icon grey"><use xlink:href="/work-blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jmp_oz" target="_blank" title="jmp_oz"><svg class="svg-icon grey"><use xlink:href="/work-blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
