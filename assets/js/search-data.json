{
  
    
        "post0": {
            "title": "Conda package for compiled native libraries",
            "content": "Background . I recently wrote about submitting your first conda package to conda-forge. You’ll find background on the “bigger picture” endeavour in that previous post. . This post is a follow-on with, perhaps, a second submission to conda-forge, this time not of a python package but a C++ library. . I’ll try to package a relatively small C++ codebase MOIRAI: Manage C++ objects lifetime when exposed through a C API. While dealing with very similar needs as refcount (reference counting and memory management), there is no explicit dependency between them. . Market review . moirai grew out of specific projects almost a decade ago, but its inception did not occur without looking first at third party options. There were surprisingly few I could identify, and of the ones I saw licensing or design made it difficult to adopt as they were. Still, in 2022 is there, on conda-forge or not, something making moirai possibly redundant? . It can be tricky to find relevant work without a time consuming research. A cursory scan comes up: . Cppy seems to have intersects, but this is solely Python-centric. | Loki-lib is an (underappreciated) library with reference counting features, but not on conda-forge. I believe Loki-lib was largely written by Andrei Alexandrescu, author of Modern C++ Design, one of the most impressive computer science book I read. | . Maybe there is a place for moirai. Plus the name is not taken… . Walkthrough . I already forked and cloned staged-recipes from the previous post. . cd ~/src/staged-recipes git checkout main git branch moirai . Starting point . grayskull is userful to generate meta.yaml stubs out of python packages, and not applicable in this case. . I learned the hard way that installing conda-build, grayskull and shyaml and conda update condain my conda base environment landed me in a broken mamba and incompatible package versions. So, this time create a new dedicate environment: . conda create -n cf python=3.9 mamba -c conda-forge conda activate cf mamba install -c conda-forge conda-build grayskull # grayskull not for this post, but other future submissions . Is there an example I can start from and work by inference and similarity rather than first principles? . libnetcdf-feedstock may be an appropriate case to start from, even if more sophisticated than my case. Rather than strip down this libnetcdf recipe though, I work from the example in the staged-recipe and add, staying closer to contributing packages . I did bump into a couple of issues, but I got less issues than I thought to get a package compiling . The end result should be something like: . {% set name = &quot;moirai&quot; %} {% set version = &quot;1.1&quot; %} package: name: {{ name|lower }} version: {{ version }} source: # url: https://github.com/moirai/moirai/releases/download/{{ version }}/moirai-{{ version }}.tar.gz # and otherwise fall back to archive: url: https://github.com/csiro-hydroinformatics/moirai/archive/refs/tags/{{ version }}.tar.gz sha256: b329353aee261ec42ddd57b7bb4ca5462186b2d132cdb2c9dacc9325899b85f3 build: number: 0 requirements: build: - cmake - make # [not win] # - pkg-config # [not win] # - gnuconfig # [unix] - {{ compiler(&#39;c&#39;) }} - {{ compiler(&#39;cxx&#39;) }} run: - curl # [win] - libgcc # [unix] test: files: - CMakeLists.txt commands: - ls about: home: https://github.com/csiro-hydroinformatics/moirai summary: &#39;Manage C++ objects lifetime when exposed through a C API&#39; description: | This C++ library is designed to help handling C++ objects from so called opaque pointers, via a C API, featuring: * counting references via the C API to C++ domain objects * handle C++ class inheritance even via opaque pointers * mechanism for resilience to incorrect type casts * thread-safe design license: BSD-3-Clause license_family: BSD license_file: LICENSE.txt doc_url: https://github.com/csiro-hydroinformatics/moirai/blob/master/doc/Walkthrough.md dev_url: https://github.com/csiro-hydroinformatics/moirai extra: recipe-maintainers: - jmp75 . Note that you do need make as a build requirement besides cmake, otherwise you’d end up with : . CMake Error: CMake was unable to find a build program corresponding to &quot;Unix Makefiles&quot;. CMAKE_MAKE_PROGRAM is not set. You probably need to select a different build tool. CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage . build.sh: . #!/bin/bash # Build moirai. mkdir -v -p build cd build # May be needed for unit tests down the track? export TERM= export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PREFIX/lib # cmake -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON .. cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${PREFIX} -DBUILD_SHARED_LIBS=ON ../ make -j 2 install # rm -rf ../build/ # not if we want to run the unit tests . Building locally on linux64 . Note: although it may have changed by the time you read this, at the time I write I may have to export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 to force that image being used. See previous post. . conda activate cf export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 cd ~/src/staged-recipes python ./build-locally.py linux64 . and the build seems OK… . + touch /home/conda/staged-recipes/build_artifacts/conda-forge-build-done . Building locally on win64 . Trying to run build-locally.py with the option win64 returns: ValueError: only Linux/macOS configs currently supported, got win64 . You’ll want to read or re-read the conda-forge documentation Using cmake and particularities on windows . Trying for bld.bat: . REM Build moirai. @REM Credits: some material in this file are courtesy of Kavid Kent (ex Australian Bureau of Meteorology) mkdir build cd build @REM following may be unncessary set PATH=&quot;%PREFIX% bin&quot;;%PATH% :: Configure using the CMakeFiles cmake -G &quot;%CMAKE_GENERATOR%&quot; ^ -DCMAKE_INSTALL_PREFIX:PATH=&quot;%LIBRARY_PREFIX%&quot; ^ -DCMAKE_PREFIX_PATH:PATH=&quot;%LIBRARY_PREFIX%&quot; ^ -DCMAKE_BUILD_TYPE:STRING=Release ^ .. if %errorlevel% neq 0 exit 1 msbuild /p:Configuration=Release /v:q /clp:/v:q &quot;INSTALL.vcxproj&quot; if %errorlevel% neq 0 exit 1 @REM del *.* . Since the local build script cannot emulate a win64 build, I am trying to set up on a Windows box and see what a conda build gives. . I first tried on my Windows desktop where I do have vs 2019 and 2022 installed. Not sure whether they can be used. This section of the conda-forge doc seem to suggest Microsoft Build Tools for Visual Studio 2017 is required, but the link to the Python wiki page on Windows compilers is covering many more versions, so this is rather confusing. See the Appendix for more details on what happens in this case. . Trying again on a windows machine, but installing visual studio build tools 2017. . After creating the conda environment cf, similar to the above on Linux: . cd c: src staged-recipes recipes conda build moirai . “mostly” works. for a minute or two it seems to freeze at a “number of files” line: . Packaging moirai INFO:conda_build.build:Packaging moirai Packaging moirai-1.1-h82bb817_0 INFO:conda_build.build:Packaging moirai-1.1-h82bb817_0 number of files: 11 . then: . PGO: UNKNOWN is not implemented yet! PGO: UNKNOWN is not implemented yet! Unknown format Unknown format Unknown format Unknown format INFO: sysroot: &#39;C:/Windows/&#39; files: &#39;[&#39;zh-CN/winhlp32.exe.mui&#39;, &#39;zh-CN/twain_32.dll.mui&#39;, &#39;zh-CN/regedit.exe.mui&#39;, &#39;zh-CN/notepad.exe.mui&#39;]&#39; &lt;edit: snip&gt; Importing conda-verify failed. Please be sure to test your packages. conda install conda-verify to make this message go away. . conda search -c conda-forge conda-verify indeed returns something. Noted… . &lt;edit: snip&gt; ## Package Plan ## environment location: C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 _test_env The following NEW packages will be INSTALLED: ca-certificates: 2022.4.26-haa95532_0 curl: 7.82.0-h2bbff1b_0 libcurl: 7.82.0-h86230a5_0 libssh2: 1.10.0-hcd4344a_0 moirai: 1.1-h82bb817_0 local openssl: 1.1.1o-h2bbff1b_0 vc: 14.2-h21ff451_1 vs2015_runtime: 14.27.29016-h5e58377_2 zlib: 1.2.12-h8cc25b3_2 . &lt;edit: snip&gt; set PREFIX=C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 _test_env set SRC_DIR=C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 test_tmp (cf) %SRC_DIR%&gt;call &quot;%SRC_DIR% conda_test_env_vars.bat&quot; (cf) %SRC_DIR%&gt;set &quot;CONDA_SHLVL=&quot; &amp;&amp; (cf) %SRC_DIR%&gt;conda activate &quot;%PREFIX%&quot; (%PREFIX%) %SRC_DIR%&gt;IF 0 NEQ 0 exit /B 1 (%PREFIX%) %SRC_DIR%&gt;call &quot;%SRC_DIR% run_test.bat&quot; (%PREFIX%) %SRC_DIR%&gt;ls (%PREFIX%) %SRC_DIR%&gt;IF -1073741511 NEQ 0 exit /B 1 . The latter fails because the command line ls borks with the error message (windows box error message) ls.exe - Entry Point not found. where ls returns C: Users xxxyyy Miniconda3 envs cf Library usr bin ls.exe so this is something that came from conda. . Taking a look at the resulting moirai-1.1-h82bb817_0.tar.bz2 file, the content looks sensible (include header files, bin/moirai.dll) . info/hash_input.json info/index.json info/files info/paths.json info/about.json info/git info/recipe/build.sh info/recipe/meta.yaml.template info/licenses/LICENSE.txt Library/include/moirai/extern_c_api_as_opaque.h Library/include/moirai/extern_c_api_as_transparent.h Library/include/moirai/setup_modifiers.h Library/include/moirai/reference_handle_map_export.h Library/include/moirai/error_reporting.h Library/include/moirai/reference_handle.h info/recipe/conda_build_config.yaml info/recipe/meta.yaml info/test/run_test.bat info/recipe/bld.bat Library/bin/moirai.dll Library/include/moirai/reference_handle_test_helper.hpp Library/include/moirai/opaque_pointers.hpp Library/include/moirai/reference_type_converters.hpp Library/include/moirai/reference_handle.hpp . Recapitulation and summary . In some respects I had less issues with this conda package than the “pure python” one, partly because of prior experience, but not entirely. . I may be in a decent place to submit this to the conda-forge/staged-recipe repository. I may hold off a bit though. First, the unit tests in moirai are not exercised by the meta.yaml file (or build scripts). Second, I may next look at setting up a conda channel to test managing conda dependencies, even if I see moirai as belonging to conda-forge rather than a private channel. Third, there are probably other things I need to tidy up. . To recapitulate on . conda create -n cf python=3.9 mamba -c conda-forge conda activate cf # grayskull not for this post, but other future submissions mamba install -c conda-forge conda-build conda-verify grayskull . The draft conda recipe for moirai at this stage is available there . Acknowledgements . Some of the conda recipe trialled was influenced by prior work by Kavid Kent (ex Australian Bureau of Meteorology). . Appendices . Appendix: if missing ms build tools 2017 . If trying without having installed ms build tools 2017: . %SRC_DIR%&gt;CALL %BUILD_PREFIX% etc conda activate.d vs2017_get_vsinstall_dir.bat Did not find VSINSTALLDIR Windows SDK version found as: &quot;10.0.19041.0&quot; The system cannot find the path specified. Did not find VSINSTALLDIR CMake Error at CMakeLists.txt:16 (PROJECT): Generator Visual Studio 15 2017 Win64 could not find any instance of Visual Studio. . (cf) %SRC_DIR%&gt;set &quot;SCRIPTS=%PREFIX% Scripts&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_PREFIX=%PREFIX% Library&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_BIN=%PREFIX% Library bin&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_INC=%PREFIX% Library include&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_LIB=%PREFIX% Library lib&quot; (cf) %SRC_DIR%&gt;set &quot;c_compiler=vs2017&quot; (cf) %SRC_DIR%&gt;set &quot;fortran_compiler=gfortran&quot; (cf) %SRC_DIR%&gt;set &quot;vc=14&quot; (cf) %SRC_DIR%&gt;set &quot;cxx_compiler=vs2017&quot; . call C: Users xxxyyy Miniconda3 Scripts activate.bat . Note: . base * C: Users xxxyyy Miniconda3 envs cf C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654760072022 _build_env C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654760072022 _h_env . If I try to start with Visual Studio 2022 Developer Command Prompt v17.1.5. Then conda environment activation, still: . %SRC_DIR%&gt;CALL %BUILD_PREFIX% etc conda activate.d vs2017_get_vsinstall_dir.bat Did not find VSINSTALLDIR Windows SDK version found as: &quot;10.0.19041.0&quot; ********************************************************************** ** Visual Studio 2022 Developer Command Prompt v17.1.5 ** Copyright (c) 2022 Microsoft Corporation ********************************************************************** [ERROR:vcvars.bat] Toolset directory for version &#39;14.16&#39; was not found. [ERROR:VsDevCmd.bat] *** VsDevCmd.bat encountered errors. Environment may be incomplete and/or incorrect. *** [ERROR:VsDevCmd.bat] In an uninitialized command prompt, please &#39;set VSCMD_DEBUG=[value]&#39; and then re-run [ERROR:VsDevCmd.bat] vsdevcmd.bat [args] for additional details. [ERROR:VsDevCmd.bat] Where [value] is: [ERROR:VsDevCmd.bat] 1 : basic debug logging [ERROR:VsDevCmd.bat] 2 : detailed debug logging [ERROR:VsDevCmd.bat] 3 : trace level logging. Redirection of output to a file when using this level is recommended. [ERROR:VsDevCmd.bat] Example: set VSCMD_DEBUG=3 [ERROR:VsDevCmd.bat] vsdevcmd.bat &gt; vsdevcmd.trace.txt 2&gt;&amp;1 Did not find VSINSTALLDIR CMake Error at CMakeLists.txt:16 (PROJECT): Generator Visual Studio 15 2017 Win64 could not find any instance of Visual Studio. -- Configuring incomplete, errors occurred! . Resources . Reference Counting in Library Design – Optionally and with Union-Find Optimization | .",
            "url": "https://jmp75.github.io/work-blog/recipes/conda/conda-forge/c++/2022/06/10/conda-packages-conda-forge-2.html",
            "relUrl": "/recipes/conda/conda-forge/c++/2022/06/10/conda-packages-conda-forge-2.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Submitting your first conda package to conda-forge",
            "content": "Background . For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. It is a stack with a core in C++, but accessible via a C API by users from R, Matlab, Python and so on. A whole article could be written about the design rationale, successes and shortcomings of this stack, and the interplay of people, organisations and technologies in using these tools and how. But this will not be this post. . Focussing on the Python side of things, these streamflow forecasting tools are used mostly on Windows and Linux, the core is deployed as dynamic libraries (.dll or .so) on disk, and python packages access these using cffi for interoperability. The python packages contain solely python code; there is no cython or straight C. . I’ve come to appreciate (mostly) conda environments for managing software stacks for various projects. This post is a start to test packaging some of “my” software with conda, in the hope this reduces the surprisingly strong impedance, technical but not only, towards usage by a broader audience. . A bit picture end point would be a corporate equivalent to a conda-forge channel, with the full software stack available for any employee. . Baby steps . I had a look a few weeks ago at how I’d package a substantial but relatively small C++ code MOIRAI: Manage C++ Objects’s lifetime when exposed through a C API. This proved a bit premature for reasons I won’t detail. . So, let’s (re)start with a python-only library, as it happens in the same vein, refcount. Astonishingly, there is still no strict equivalent that I can find in conda-forge dedicated to reference counting external pointers. So, can I claim the spot? . Resources . I started this post thinking first about conda packaging rather than submission per se to conda-forge. Some resources I initially looked at as promising, but from which I backed away (for now): . Building conda packages from scratch | python packaging tutorials Scipy 2018 Tutorial: The Joy of Packaging - conda packages | Activision Game: A tutorial (+ build recipes) describing how to use conda for C, C++, and python package management outlines well the rationale for packaging in conda, and appears didactic. It appears not to have recent commit, although it is not necessarily a problem. | . You’ll see in the walkthrough below (next section) that I reoriented towards an upfront submission to conda-forge: . 3 simple stept to build a python package for conda-forge. This post really got me on a better path. | conda-forge documentation: Contributing packages | . Walkthrough . First trial: conda build locally? . Of course python is necessary and a conda environment a given. I do . ~/config/baseconda because I never have conda activated by default from .bashrc. . I am actually not sure from the conda-build tutorial in which environment I should install conda-build. Let’s try the base environment and see whether we get stuck or not. . mamba install -c conda-forge conda-build (installing from conda-forge is an idiosyncrasy. I strongly recommend mamba) . You should at least skim through the concepts. This is rather dry to read throughly upfront. . The tutorial Building conda packages from scratch quickly confused me; I was trying to transpose it to refcount but this does not look like the right template to start from. The section editing-the-meta-yaml-file appears out of sync with the “correct” meta.yaml file. Baffling. . Preparing a PR to conda-forge/staged-recipes . Enter two new resources: 3 simple stept to build a python package for conda-forge and conda-forge documentation: Contributing packages. From these it becomes clear I should use grayskull to get a starting point as a meta.yaml file . cd ~/src git clone --depth 1 git@github.com:jmp75/staged-recipes.git cd ~/src/staged-recipes/recipes git remote add upstream git@github.com:conda-forge/staged-recipes.git mamba install -c conda-forge grayskull grayskull pypi refcount . #### Initializing recipe for refcount (pypi) #### Recovering metadata from pypi... Starting the download of the sdist package refcount refcount 100% Time: 0:00:00 15.3 MiB/s|###############################################################################################################################################################################################################################################| Recovering information from setup.py Executing injected distutils... Recovering metadata from setup.cfg No data was recovered from setup.py. Forcing to execute the setup.py as script Recovering metadata from setup.cfg Checking &gt;&gt; cffi 100% |##########################################################################################################################################################################################################################################|[Elapsed Time: 0:00:00] Matching license file with database from Grayskull... Match percentage of the license is 59%. Low match percentage could mean that the license was modified. License type: BSD-3-Clause License file: LICENSE.txt Host requirements: - pip - python Run requirements: - cffi - python RED: Missing packages GREEN: Packages available on conda-forge Maintainers: - j-m #### Recipe generated on /home/per202/src/staged-recipes/recipes for refcount #### . The output meta.yaml (which is actually a ninja template file), is a good start, however you should revise it a bit rather than accept wholesale . Mostly fine, however this did not pick up a requirement cffi &gt;=1.11.5, and second guessing from reading this gallon.me post, a minimum python version is necessary to get accepted. . requirements: host: - pip - python &gt;=3.6 run: - cffi &gt;=1.11.5 - python &gt;=3.6 . Perhaps optional, remove a hard-coded package string “refcount “in the source url section. . It is instructive to look at the existing pull requests on staged-recipes. Notably I realise that the github ID extracted by grayskull is not the correct one; I am not the ID j-m, unfortunately (could have been judging by history length). . extra: recipe-maintainers: - j-m . extra: recipe-maintainers: - jmp75 . The end result should be something like: . {% set name = &quot;refcount&quot; %} {% set version = &quot;0.9.3&quot; %} package: name: {{ name|lower }} version: {{ version }} source: url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.zip sha256: bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442 build: noarch: python script: {{ PYTHON }} -m pip install . -vv number: 0 requirements: host: - pip - python &gt;=3.6 run: - cffi &gt;=1.11.5 - python &gt;=3.6 test: imports: - refcount commands: - pip check requires: - pip about: home: https://github.com/csiro-hydroinformatics/pyrefcount summary: A Python package for reference counting and interop with native pointers description: | This package helps you achieve reliable management of memory allocated in native libraries, written for instance in C++. While it boils down to &quot;simply&quot; maintaining a set of counters, it is deceptively complicated to do so properly and not end up with memory leaks or crashes. &lt;https://pyrefcount.readthedocs.io/en/latest/&gt;. dev_url: https://github.com/csiro-hydroinformatics/pyrefcount license: BSD-3-Clause license_family: BSD license_file: LICENSE.txt extra: recipe-maintainers: - jmp75 . So; ready to submit a pull request? Wait, wait. . Building locally . test has a section on Running unit tests. Note that the default conda recipe above has a “pip check”, but nothing more. refcount unit tests use pytest, and has unit tests; tick that. refcount is a pure python package, but it is a package for (mostly) interoperability with native code via a C API. Unit tests do contain some c/c++ code. . Should the recipe run fine upon submission, including unit tests? Before submitting a pull request that may trigger a failed check, let’s experiment with staging tests locally. . so: . cd ~/src/staged-recipes python ./build-locally.py linux64 . File &quot;/home/per202/miniconda/lib/python3.9/subprocess.py&quot;, line 373, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command &#39;[&#39;.scripts/run_docker_build.sh&#39;]&#39; returned non-zero exit status 1. . I tried to conda install -c conda-forge shyaml which seems to be used by the scripts, but this did not alleviate the issue. . That took me some time to find a workaround to this one. The “exit status 1” is actually very misleading. The root cause is a docker run -it that exited with an error code 139. I seem to not be the only one to have bumped into this issue still open. I may have pointed to the workaround in the conda-forge FAQ. . I needed to override the default docker image build-locally.py falls back to with: . export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 python build-locally.py linux64 . the build script works this time, but at some point: . Processing $SRC_DIR Added file://$SRC_DIR to build tracker &#39;/tmp/pip-build-tracker-bkn7ckr9&#39; Running setup.py (path:$SRC_DIR/setup.py) egg_info for package from file://$SRC_DIR Created temporary directory: /tmp/pip-pip-egg-info-68li1y6t Preparing metadata (setup.py): started Running command python setup.py egg_info Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt; File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt; File &quot;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/setup.py&quot;, line 41, in &lt;module&gt; with open(os.path.join(here, &#39;README.md&#39;), encoding=&#39;utf-8&#39;) as f: File &quot;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pl/lib/python3.10/codecs.py&quot;, line 905, in open file = builtins.open(filename, mode, buffering) FileNotFoundError: [Errno 2] No such file or directory: &#39;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/README.md&#39; error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. . This is an issue that may be in my control. . cd ~/src/staged-recipes/build_artifacts/refcount_1654312313810/work ls ## build_env_setup.sh conda_build.sh LICENSE.txt MANIFEST.in metadata_conda_debug.yaml PKG-INFO README.rst refcount refcount.egg-info setup.cfg setup.py . refcount has both a README.md and README.rst, the latter being an export from the former because pypi requires (or used to require) a README.rst to display correctly. The zip archive of the source code on pypi indeed does not have the README.md file included. . I’ve inherited the practice to use in the packages setup.py the following, to limit redundances. . with open(os.path.join(here, &#39;README.md&#39;), encoding=&#39;utf-8&#39;) as f: long_description = f.read() long_description_content_type=&#39;text/markdown&#39; . Previously I needed to convert on the fly to restructured Text, but Markdown is more supported. Still there is a lot of inertia with restructuredText. . I may try to just nuke the README.rst. The only fly on the ointment is: is pypi ok with rendering markdown correctly these days? Probably; the packaging documentation is using README.md by default. . So, build and submit to pypi the updated refcount 0.9.4 with no README.rst. Looks fine, including the zip source archive. . RuntimeError: SHA256 mismatch: &#39;21567918cb1bb30bf8116ce3483d3f431de202618eabbc6887b4814b40a3b94a&#39; != &#39;bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442&#39; Traceback (most recent call last): . Right, I forgot to change the checksum in the meta.yaml file. . And… it seems to complete. . import: &#39;refcount&#39; + pip check No broken requirements found. + exit 0 Resource usage statistics from testing refcount: Process count: 1 CPU time: Sys=0:00:00.0, User=- Memory: 3.0M Disk usage: 28B Time elapsed: 0:00:02.1 TEST END: /home/conda/staged-recipes/build_artifacts/noarch/refcount-0.9.4-pyhd8ed1ab_0.tar.bz2 . Submit the pull request, and pleasantly: . . Conclusion . While there were a couple of bumps along the way, this should end up with a positive outcome. If not with refcount on conda-forge, I’ve a better understanding to tackle conda packaging on the rest of the software stack. . Building a conda package “from scratch” may not be the easiest learning path. Even if you indent to build a conda package not for conda-forge, going through the staged-recipes process may be a most | Some of the reference documentation may need a spruice up. Building conda packages from scratch confused me. First, packaging a pypi package is not starting “from scratch” for most users. Second, inconsistencies in the documentation. I am sure I’ll get back to that resource, but I wish there were more “water tight”, step-by-step tutorials for conda packaging. | .",
            "url": "https://jmp75.github.io/work-blog/recipes/conda/conda-forge/2022/06/04/conda-packages-conda-forge.html",
            "relUrl": "/recipes/conda/conda-forge/2022/06/04/conda-packages-conda-forge.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Lithology classification using Hugging Face, part 1",
            "content": "About . This is (perhaps) the start of a series of posts on using natural language processing for lithology classification. I hope to explore multi-label classification in subsequent posts. . . Background . I&#39;ve been attending virtually the latest Deep Learning course run by Jeremy Howard (See the fast.ai forum for pointers to past courses). Part of the experience can be to find &quot;homeworks&quot;. I traditionally work with point time series data, and it would have been a Pavlov reflex for me to get a use case with this type of data. . However in one of the lessons Jeremy used Hugging Face Transformers applied to the Kaggle competition U.S. Patent Phrase to Phrase Matching. At some point he made a comment about how much NLP progressed over the recent years, and how much potential value creation there was in this. . I do not follow closely NLP research, and am not knowledgeable enough to agree or not, but got an inkling of the potential value a few years back when working on a python package for exploratory lithology analysis, for groundwater characterisation. Lithology is &quot;the study of the general physical characteristics of rocks&quot;. Drilling core soil samples is not cheap; existing records are valuable. . Sections of a core sample drill can be described with sentences such as: . topsoil | shale, slippery back, green seams | sandstone alluvium water bearing | gravel red very clayey water supply | sandstone, red/pale brown, fine-coarse, white clay bands, brownish yellow bands, pebbles (16.9-16.98m) | fill; orange-brown, dry, loose, pebbles to 2.5cm, heterogeneous | clay; light brown, strongly cohesive, contains silicate &amp; carbonate clasts to coarse sand size, no clay smell. | . You would have an inkling that the prose of the writer (the &quot;driller&quot;) can vary in style and detail. One typical use case is to determine the primary and (optionally) secondary lithologies of a record, as it strongly influences how fast water can flow underground. &quot;gravel red very clayey&quot; is on the easier side of the spectrum of difficulty: &quot;gravel&quot; as a primary lithology and &quot;clay&quot; as a secondary lithology (assuming these are valid classes for the context). It can get much trickier to classify of course, and certainly expensive if done very manually. Automating this classification process opens the possibility of an iterative process towards a sound lithology classification fit for purpose. . I co-authored a conference presentation &quot;Comparing regular expression and deep learning methods for the classification of descriptive lithology&quot; (Page 161 of the MODSIM 2019 book of abstracts if you are curious). With all the caveats of a study made on very limited resources, I was surprised by how well NLP performed overall to train and test on a human-labeled dataset. . I am not working on this domain during paid hours these days, but feel like revisiting this for a bit over the course. . Getting acquainted with data . Arbitrarily, I downloaded data for the Namoi catchment from the Australian Groundwater Explorer. While I am not totally new to the domain, I genuinely do need to explore this data from scratch. This will probably be at least the rest of this post. . import pandas as pd from pathlib import Path . fn = Path(&#39;~&#39;).expanduser() / &quot;data/ela/shp_namoi_river/NGIS_LithologyLog.csv&quot; . The types of a few columns lead to warnings: let&#39;s force them to &#39;str&#39; here. I do not anticipate using the depth records for now. . litho_logs = pd.read_csv(fn, dtype={&#39;FromDepth&#39;: str, &#39;ToDepth&#39;: str, &#39;MajorLithCode&#39;: str, &#39;MinorLithCode&#39;: str}) . So how does this data look like? . litho_logs.head() . OBJECTID BoreID HydroCode RefElev RefElevDesc FromDepth ToDepth TopElev BottomElev MajorLithCode MinorLithCode Description Source LogType OgcFidTemp . 0 14 | 10045588 | GW003048.1.1 | None | UNK | 0.0 | 1.22 | None | None | SOIL | NaN | SOIL SANDY CLAY | UNK | 1 | 8666163 | . 1 15 | 10045588 | GW003048.1.1 | None | UNK | 19.51 | 28.65 | None | None | BSLT | NaN | BASALT WATER BEARING | UNK | 1 | 8666164 | . 2 28 | 10060509 | GW006968.1.1 | None | UNK | 0.0 | 3.66 | None | None | TPSL | NaN | TOPSOIL | UNK | 1 | 8666177 | . 3 29 | 10060509 | GW006968.1.1 | None | UNK | 33.53 | 34.75 | None | None | GRVL | NaN | GRAVEL WATER BEARING | UNK | 1 | 8666178 | . 4 30 | 10060509 | GW006968.1.1 | None | UNK | 48.77 | 51.82 | None | None | SHLE | NaN | SLIPPERY BACK | UNK | 1 | 8666179 | . A MajorLithoCode column appears to be populated, so this may be suitable for training a classifier. I have no idea (or forgot) how these lithology codes have been derived and what the corpus of labels is. . len(litho_logs) . 144518 . For the sake of conciseness I will point-blank reuse the text processing utilities in the ela package, without any explanation of the setup (ela comes with many dependencies from NLP to 3d vis that can be tricky to install). . from ela.textproc import token_freq, plot_freq . MAJOR_CODE=&#39;MajorLithCode&#39; MINOR_CODE=&#39;MinorLithCode&#39; DESC=&#39;Description&#39; . Major (primary) lithology codes . litho_classes=litho_logs[MAJOR_CODE].values df_most_common= token_freq(litho_classes, 50) . plot_freq(df_most_common) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . This is a long-tailed distribution; quite a few labels. The limit to 4 characters labels suggest a form of controlled vocabulary. The numbers 20, 19, 23, 8, 1 look odd. &quot;None&quot; is an artefact though not misleading, quite a few records are such that no major lithology could be attributed. . Minor (secondary) lithology codes . litho_classes=litho_logs[MINOR_CODE].values df_most_common= token_freq(litho_classes, 50) . plot_freq(df_most_common) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . Well, no minor lithology codes, so this data set may not be a good start for multi-label classification. Still, I&#39;ll persist with this data set, and reassess later on . Back to major lithology codes . What are the flavour of descriptions leading to the most frequent classes (CLAY, GRVL, etc.), as well as &quot;None&quot; . is_clay = litho_logs[MAJOR_CODE] == &#39;CLAY&#39; . clay_coded = litho_logs.loc[is_clay][DESC] . import numpy as np np.random.seed(123) clay_coded.sample(n=50) . 144320 CLAY 117387 CLAY 18630 CLAY 35565 CLAY SOME SANDY 61997 CLAY 13533 CLAY SANDSTONE BANDS 117529 YELLOW CLAY 82290 CLAY 73280 CLAY GREY SANDY 106748 CLAY STONEY 26265 CLAY YELLOW 16456 CLAY 25032 CLAY 9911 CLAY 135852 CLAY, SANDY GRAVELLY, ORANGE BROWN, MEDIUM PLA... 30596 CLAY 140698 CLAY, SANDY FAT; RED, DRY-MOIST, MEDIUM CONSIS... 70267 CLAY 6534 CLAY YELLOW 69927 CLAY GRAVEL 76476 SANDY CLAY 7329 CLAY GRITTY 51685 CLAY; 40% BROWN, GRAVEL &amp; SAND 60%, MOST 1-3MM 137385 CLAY 35816 CLAY 73972 CLAY 102333 CLAY SOME STONES 29218 CLAY 92266 RED CLAY 26151 CLAY SANDY 107456 CLAY GREY 52550 CLAY - LIGHT GREY, EXTREMELY SANDY (FINE TO CO... 24225 CLAY 120696 CLAY; LIGHT GREY, TRACE OF WHITE 9030 CLAY 14889 CLAY 25834 CLAY LIGHT BROWN GREY SILTY 44078 CLAY 114869 CLAY 109302 CLAY 38107 CLAY PATCHY 30801 CLAY 82362 CLAY/BROWN ORANGE 141490 CLAY; BROWN, PINK, SOME BLUE 23075 CLAY SANDY GRAVEL 42110 CLAY 50493 CLAY, WHITE SEAM, RED 90209 CLAY - DARK BROWN 104019 CLAY 12690 CLAY Name: Description, dtype: object . Nothing too surprising in this, very intuitive, though the tail suggests there may be a few outliers: . clay_coded.tail(10) . 144386 CLAY; LIGHT BROWN, SOME LIGHT GREY, SILTY 144388 CLAY; BROWN 144391 CLAY; LIGHT BROWN 144393 CLAY - BROWN 144394 CLAY - LIGHT BROWN, EXTREMELY SANDY (COARSE) 144398 CONGLOMERATE - WEATHERED BUT STILL HOLDS TOGET... 144401 CLAY; BROWN 144402 CLAY - GREY 144488 CLAY 144503 None Name: Description, dtype: object . Looking at the &quot;sand&quot; code: . def sample_desc_for_code(major_code, n=50, seed=None): is_code = litho_logs[MAJOR_CODE] == major_code coded = litho_logs.loc[is_code][DESC] if seed is not None: np.random.seed(seed) return coded.sample(n=50) . sample_desc_for_code(&#39;SAND&#39;, seed=123) . 72256 SAND GRAVEL FINE WATER SUPPLY 109345 SAND CLAYEY GRAVEL 66816 SAND WATER SUPPLY 85097 SAND - 70% UP TO 2MM, GRAVEL 10% 2-5MM, CLAY 2... 137476 SAND; LIGHT BROWN, MEDIUM TO COARSE, FINE GRAV... 29946 SAND GRAVEL DIRTY 37012 COARSE SAND AND GRAVEL 135985 SAND 74402 SAND WHITE GREY WELL SORTED WATER SUPPLY 42422 SAND GRAVEL 41225 SAND CLAYEY GRAVEL 50700 SAND 33813 BROWN SAND (VERY FINE TO MEDIUM) 38452 SAND GRAVEL MEDIUM WATER SUPPLY 107707 SAND FINE-COARSE 51759 SAND; 60%, SILTY, FINE 10%, MEDIUM 20% &amp; COARS... 102234 SAND GRAVEL WATER SUPPLY 74640 SAND GRAVEL WATER SUPPLY 13123 SAND GRAVEL WATER SUPPLY 23110 SAND 11373 SAND GRAVEL WATER SUPPLY 35508 SAND SILTY GRAVEL WATER SUPPLY 32744 SAND CLAYEY 67310 SAND CLAY 41813 SAND GRAVEL WATER SUPPLY 106457 SAND GRAVEL WATER SUPPLY 85591 SAND AND GRAVEL - CLAYEY AND SILTY 112156 SAND, WHITE (VERY VERY FINE) 111256 SAND/GRAVEL; FINE TO COARSE, BROWN 55808 SAND; POORLY GRADED, COARSE, SUBROUNDED, LIGHT... 91458 SAND, MEDIUM-COARSE; SLIGHTLY SILTY, WET, LOOS... 61608 SAND, GRAVEL &amp; LIMESTONE 96382 SAND CLAYEY 37119 MEDIUM BROWN SAND &amp; GRAVEL SOME LARGE GRAVEL 118473 SAND; 40% 1/20-1/2MM, 40% 1/2-1MM, 20% 1-3MM, ... 70703 SAND GRAVEL WATER SUPPLY 92920 BROWN SAND 80551 SAND, SILTY; AS ABOVE, TRACE CLAY, ORANGE, LOW... 25175 COARSE SAND &amp; FINE GRAVEL WITH ANGULAR FRAGMEN... 38905 SAND FINE WATER BEARING 44248 SAND GRAVEL WATER SUPPLY 32261 SAND DRIFT 34510 SAND CLAYEY COARSE GRAVEL 77431 SAND AND GRAVEL FINE TO COARSE GREY 138362 SAND BROWN 103758 SAND GRAVEL STONES SOME CLAY BANDS 69238 SAND GRAVEL 143566 SAND &amp; GRAVEL; SAND 50% (FINE 30%, MEDIUM 20%,... 135131 SAND 30361 SAND CLAYEY WELL SORTED Name: Description, dtype: object . Rather straighforward, consistent and and intuitive . Major lithology code &quot;None&quot; . This one is likely to be more curly and surprising. Let&#39;s see . sample_desc_for_code(&#39;None&#39;, seed=123) . 131075 W.B. BROWN SHALE 127170 COARSE SAND AND GRAVEL 126145 BLUE CLAY 126889 GREY AND BROWN CLAY 132179 GREY BROWN CLAY 130033 BROWN SANDY CLAY 123436 GRAVEL 128794 CLAY 128242 SAND AND FINE GRAVEL 134156 None 129636 W.B. BASALT 122884 LIGHT ORANGE SILTY CLAY 124580 SHALES 128684 W.B. SHALE 131120 SAND AND GRAVEL 122690 RED RIDGE CLAY 132319 BLACK SOIL 128528 BLACK CLAY 123646 MUDSTONE 129757 SAND &amp; CLAY 124353 SOFT BROWN CLAY 131921 CLAY 133208 BROWN CLAYS 132945 COARSE SAND 122956 SAND AND GRAVEL 123723 CALY AND GRAVEL 122102 TOP SOIL 126455 HARD SANDY BROWN &amp; GREY CLAY WITH STONES 125637 SANDY BLUE AND GREY CLAY 131154 BROWN SANDY CLAY 129931 RIDGE CLAY 123557 RIDGE CLAY 124964 DARK BROWN CLAY 129391 GREY SHALE 132891 WATER 0.37 LS 132006 BROWN CLAY 134673 SEP 123692 BROWN CLAY 126034 REDDISH BROWN CLAY 129690 SOFT GINGER/BROWN CLAY &amp; STONES 125560 FINE SANDY CLAYS 126238 SHALE &amp; (WATER) 123009 SOIL 124236 SOIL 125814 BLUE SHALE 130474 RED &amp; GREY CLAY 127005 BROWN CLAY 134212 BROWN AND 125640 SOIL 127355 SHALE Name: Description, dtype: object . Well, it does not require a fully trained geologist to think there should be obvious primary lithology codes for many of these, so why is there &quot;None&quot; for these descriptions? . Not shown in the 50-odd sample above are descriptions which should indeed be unclassified (e.g. &quot;no strata description&quot;) . This is also an occasion to note the less obvious information and complications in descriptive logs, compared to earlier categories: . CALY as a typo for CLAY | Slang terms and acronyms, e.g. &quot;W.B.&quot; perhaps for &quot;Weathered, broken&quot; | Quantitative and qualitative attributes such as SAND; 60%, SILTY, FINE 10%, MEDIUM 20% &amp; COARSE, which may be valuable for some types of classification | . sample_desc_for_code(&#39;SDSN&#39;, seed=123) . 9423 SANDSTONE SOFT 117911 SANDSTONE, SILTY; AS ABOVE, QUARTZ (+60%) 6613 SAND ROCK GREY 20345 SANDSTONE GREY HARD 106885 SANDSTONE 139088 SANDSTONE; DARK GREY, COARSE GRAINED, HARD 5101 SANDSTONE ROTTEN 26561 SANDSTONE YELLOW 137304 SANDSTONE; COARSE 56627 SANDSTONE; GREY, FINE-MEDIUM GRAINED, MOD WEAK... 98974 SANDSTONE WHITE WATER SUPPLY 87187 SANDSTONE 45141 SANDSTONE, VERY PALE BROWN, HIGH CLAY 70015 SANDSTONE YELLOW 29492 SANDSTONE WATER SUPPLY 92918 SANDSTONE 83555 SANDSTONE; LIGHT GREY, VERY FINE, STRONG 136401 SANDSTONE 48201 SANDSTONE 34060 SANDSTONE, YELLOW, MEDIUM GRAINED, HIGH CLAY M... 63752 SANDSTONE; WHITISH GREY, VERY FINE, ANGULAR, C... 51878 SANDSTONE, MEDIUM, WHITISH YELLOW 59346 SANDSTONE, SOFT 62968 SANDSTONE; OFF-WHITE GREY, VERY FINE, STRONG 140930 SANDSTONE, LIGHT GREY, SOME FRACTURES 6394 SAND ROCK 113546 SANDSTONE - HARD - LIGHT GREY 49539 SANDSTONE; RED GREY, FINE-MEDIUM GRAINED, FRIA... 60198 SANDSTONE; LIGHT GREY, FINE GRAINED 1366 SANDSTONE 35923 SANDSTONE WEATHERED COARSE WATER SUPPLY 141556 SANDSTONE, WHITE, IMPREGNATED WITH WATER WASHE... 10100 SANDSTONE WATER SUPPLY 89138 SANDSTONE; OFF-WHITE GREY, MEDIUM-COARSE GRAIN... 86046 QUARTZ SANDSTONE; MEDIUM GRAINED 76366 SANDSTONE, PALE BROWN, FINE-COARSE, LOW CLAY, ... 46179 SANDSTONE, LIGHTYELLOW/RED BROWN MOTTLED, FINE... 8613 SANDSTONE 4537 SANDSTONE YELLOW 60506 SANDSTONE 8917 SANDSTONE 1372 SANDSTONE WATER SUPPLY 82975 SANDSTONE; LIGHT GREY, FINE GRAINED, MOD STRON... 60166 SANDSTONE, VERY SOFT 31674 SANDSTONE SHALE 8397 SANDSTONE 113310 SANDSTONE; OFF-WHITE, FINE GRAINED, MOD STRONG 27980 SANDSTONE 79933 SANDSTONE WHITE BROKEN 33654 SANDSTONE YELLOW CLAYEY Name: Description, dtype: object . Now, what&#39;s with the weird numbers as lithology codes? . sample_desc_for_code(&#39;20&#39;, seed=123) . 130369 SANDY CLAY; BROWN (COARSE) 122444 CLAY SANDY 127761 CLAY SANDY 125771 SANDY CLAY 127783 CLAY SANDY 133014 SANDY CLAY AND CLAYEY SAND AND GRAVEL 124007 BROWN &amp; GREY SANDY CLAY 123445 CLAY LT BROWN GREY SANDY, MED-COARSE BROWN BANDS 122180 CLAY SANDY 127440 SANDY CLAY 131498 SANDY CLAY 127935 CLAY RED SANDY 129304 SANDY CLAY 130614 SANDY SHALE 125543 SANDY CLAY 125149 SANDY CLAY 124971 SANDY CLAY/GREY BROWN 123939 SANDY CLAY, BROWN 122392 CLAY SANDY 123999 BROWN &amp; GREY SANDY CLAY 129203 SANDY LOAM 127933 CLAY RED SANDY 134474 SANDY CLAYEY MED. BROWN 126071 SANDY CLAY 127927 CLAY SANDY 123587 CLAY SANDY 126559 SANDY CLAY 127928 CLAY GREY SANDY 124643 SANDY CALY/BROWN 122555 CLAY SANDY 132628 SANDY CLAY 129853 SANDY CLAY 130187 SANDY CLAY 129057 SANDY CLAY 133300 SANDY CLAY 122535 CLAY SANDY 122732 CLAY SANDY 127977 CLAY YELLOW SANDY 126568 SANDY CLAY 122339 CLAY SANDY 128386 SANDY CLAY, BROWN 128198 SANDY CLAY, RIDGE 125530 SANDY CLAY 126798 SANDY CLAY 122452 CLAY SANDY 123237 SANDY BROWN CLAYS 126510 SANDY CLAY; 40% LIGHT GREY, EXTREMELY SANDY (C... 132331 SANDY GRAVEL 131807 SANDY CLAY 132895 SANDY CLAY Name: Description, dtype: object . Interesting. There is a clear pattern. I know from my prior exposure that &quot;Clayey sands&quot; and &quot;sandy clays&quot; are not that uncommon (and gradations of mixes of sand and clay matter a great deal to estimate hydraulic conductivity). . Next . This was the initial EDA. Next I&#39;ll probably train a classifier on the major lithology code (or a subset thereof). I am keen to explore multi-label classification, but will have to decide whether to populate the secondary lithology code using regexp classification, or switch to a fully labelled dataset at some point. . This first post illustrated the need to have a look at data. This data was already collated and curated, and I have no doubt many people went through a lot of work to get there. But this may not be a fully labelled dataset amenable to be used for training a classifier. At least, not without further data preparation. .",
            "url": "https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html",
            "relUrl": "/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Victorian Water Measurement Information System",
            "content": "About . Trying to retrieve time series or streamflow measurements via a REST API. Or anything that works for that matter. . This builds on work done by my colleague Andrew Freebairn, which really gave me a leg up. . Web API: which one? . I am not a regular user of online data. Whenever I really needed to fetch data from an online service, this tended to be an experience where reverting back to using a Web GUI with a &quot;download data&quot; button the path of least pain. . Before writing this section I got confused, trying to use kiwis-pie to retrieve data from the VIC data web site. Which is not suitable. I thought all Kisters products would be accessible using a unified web API. But Hydstra has its own separate from WIKSI, so far as I understand. See https://kisters.com.au/webservices.html . Curiously what seems to be the documentation for the Json API to Hydstra is in a page named HYDLLP – Flat DLL Interface to Hydstra. The examples are given in Perl, which is, well, not all that mainstream anymore. . There are visibly data codes to identify the types of data, at least to retrieve time series data. Query parameters such as varfrom and varto are kind of odd in an API though, and would deserve an explanation. It is only after manually downloading a time series from the VIC water data GUI that there is an explanation note about these variable codes. . The VIC data web site offers a way to directly link to URLs for particular data, but this seems not to work when used: https://data.water.vic.gov.au?ppbm=405288|B_405_GOULBURN|WEB_SW&amp;rs&amp;1&amp;rsdf_org . rdmw.qld.gov.au providing documentation for how to build json queries. . import requests import json import urllib import pandas as pd from datetime import date from IPython.core.display import HTML from IPython.display import JSON . def vic_url_builder(json_p, query): endpoint = &#39;http://data.water.vic.gov.au/cgi/webservice.exe&#39; return f&quot;{endpoint}?{json.dumps(json_p)}&amp;{urllib.parse.urlencode(query)}&quot; . The site identifiers of interest are: . site_id_405284 = &#39;405284&#39; # SUNDAY CREEK @ MT DISAPPOINTMENT site_id_405288 = &#39;405288&#39; # SUNDAY CREEK @ MT DISAPPOINTMENT (US SUNDAY CK. STORAGE site_id_405287 = &#39;405287&#39; # WESTCOTT CREEK @ MT DISAPPOINTMENT US SUNDAY CK STORAGE site_id_405239 = &#39;405239&#39; # SUNDAY CREEK @ CLONBINANE . # This took me a bit of time to figure out how to extract cumecs. One has to figure out that the &#39;varfrom&#39; in the query parameter needs to bhe the river level in metres. var_code_level_metres = &#39;100&#39; # var_code_cumecs = &#39;140&#39; # cumecs var_code_mlpd = &#39;141&#39; # ML/Day ver = { &#39;ver&#39;: &#39;2.0&#39; } #station_list = &#39;405288&#39; # don&#39;t set as 1-element list though: fails... station_list = &#39;,&#39;.join([ site_id_405284, site_id_405288, site_id_405287, site_id_405239, ]) def dt_str(x): return x.strftime(&#39;%Y%m%d%H%M%S&#39;) get_ts = { &#39;function&#39;:&#39;get_ts_traces&#39;, &#39;version&#39;:&#39;2&#39;, &#39;params&#39;:{ &#39;site_list&#39;:station_list, &#39;datasource&#39;:&#39;A&#39;, # THere are other codes, but this one is the only that returned something so far I can see. &#39;varfrom&#39;:var_code_level_metres, &#39;varto&#39;:var_code_cumecs, &#39;start_time&#39;:&#39;20220101000000&#39;, &#39;end_time&#39;:&#39;20220529000000&#39;, &#39;interval&#39;:&#39;day&#39;, &#39;multiplier&#39;:&#39;1&#39;, &#39;data_type&#39;:&#39;mean&#39;, } } query = vic_url_builder(get_ts, ver) . response = requests.get(query) . if response.status_code == requests.codes.ok: print(&#39;Return Json&#39;) print(response.headers[&#39;content-type&#39;]) response_json = response.json() # for visualisation in a notebook (not sure about rendered in a blog?) JSON(response_json) . Return Json text/html . &lt;IPython.core.display.JSON object&gt; . type(response_json) . dict . traces = response_json[&#39;return&#39;][&#39;traces&#39;] len(traces) . 2 . We got only trace time series for 2 of the 4 sites. This is because these sites have no recent data, at least not for the preiod of interest. Fair enough. For instance 405284 SUNDAY CREEK @ MT DISAPPOINTMENT functioned from from 23/06/1981 to 05/07/1984 . traces[0].keys() . dict_keys([&#39;error_num&#39;, &#39;compressed&#39;, &#39;site_details&#39;, &#39;quality_codes&#39;, &#39;trace&#39;, &#39;varfrom_details&#39;, &#39;site&#39;, &#39;varto_details&#39;]) . traces[0][&#39;site&#39;] . &#39;405288&#39; . tr = traces[0] tr[&#39;quality_codes&#39;] . {&#39;255&#39;: &#39;&#39;, &#39;100&#39;: &#39;Irregular data, use with caution.&#39;, &#39;2&#39;: &#39;Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction&#39;} . traces[1][&#39;quality_codes&#39;] . {&#39;255&#39;: &#39;&#39;, &#39;150&#39;: &#39;Rating extrapolated above 1.5x maximum flow gauged.&#39;, &#39;149&#39;: &#39;Rating extrapolated 1.5 times the maximum flow gauged.&#39;, &#39;2&#39;: &#39;Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction&#39;} . So this has good quality data, and some missing data (255). I am a bit puzzled by the codes for ratings beyond the max flow gauge, but the value appears to be sometimes zero flows in the data. Odd. . After a bit of iterative discovery, a way to turn json data to pandas series: . import numpy as np def to_dt64(x): return pd.Timestamp(x).to_datetime64() def make_daily_time_series(tr): site_name = tr[&#39;site&#39;] y = pd.DataFrame(tr[&#39;trace&#39;]) y.v = pd.to_numeric(y.v) v = y.v.values q = y.q.values y.t = y.t.apply(str) v[q == 255] = np.nan index = y.t.apply(to_dt64).values ts = pd.Series(v, index=index, name=site_name) return ts . ts = make_daily_time_series(traces[0]) ts.plot(ylabel=&quot;m3/s&quot;, xlabel=&quot;Time&quot;, title = ts.name) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;405288&#39;}, xlabel=&#39;Time&#39;, ylabel=&#39;m3/s&#39;&gt; . ts = make_daily_time_series(traces[1]) ts.plot(ylabel=&quot;m3/s&quot;, xlabel=&quot;Time&quot;, title = ts.name) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;405287&#39;}, xlabel=&#39;Time&#39;, ylabel=&#39;m3/s&#39;&gt; . Related resources . Andrew Freebairn&#39;s python package &#39;bomwater&#39; .",
            "url": "https://jmp75.github.io/work-blog/jupyter/hydstra/2022/05/28/hydstra-rest-data.html",
            "relUrl": "/jupyter/hydstra/2022/05/28/hydstra-rest-data.html",
            "date": " • May 28, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My profile page at the CSIRO | Github profile | . I live on Ngunnawal country . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmp75.github.io/work-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmp75.github.io/work-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}