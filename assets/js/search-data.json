{
  
    
        "post0": {
            "title": "Upgrading a C++ streamflow compilation toolchain",
            "content": "Background . For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. It is a stack with a core in C++. For a variety of reasons (licencing, habits, inertia) it is still compiled with the microsoft VCPP2013 toolchain. The most recent versions of some of the third party dependencies are starting to not be compiling with fthat 10 year old compiler, making maintenance more difficult. . This is not a particularly popular topic, but I am posting to at least organise and plan a migration. . Current status . On Windows, we are relying on vcxproj files. A fair bit of work went into being able to manage switching configurations more easily than via horrendous hard coded paths, which is the baffling default behavior with these files, or at least, was. The supposedly “user-friendly” GUIs to manage project settings foster an unholly mess unless users are on a very tight leash. The Readme at vcpp-commons includes instructions on how to set things up, and how to use it from visual c++ project files. . There are tools in the microsoft ecosystem to manage dependencies and configurations that were not existing 10 years ago. Here is one, Conan, essentially doing what I put in place 10 years ago. There is also vcpkg which I looked at a few years back. Be it as it may, we have a legacy that worked for a while, so we are likely to keep using it. . We are using property files (.props files) to manage centrally some definitions, so that our vcxproj files look like: . &lt;ImportGroup Label=&quot;PropertySheets&quot;&gt; &lt;Import Project=&quot;$(UserRootDir) Microsoft.Cpp.$(Platform).user.props&quot; Condition=&quot;exists(&#39;$(UserRootDir) Microsoft.Cpp.$(Platform).user.props&#39;)&quot; Label=&quot;LocalAppDataPlatform&quot; /&gt; &lt;Import Project=&quot;$(UserProfile)/vcpp_config.props&quot; Condition=&quot;exists(&#39;$(UserProfile)/vcpp_config.props&#39;)&quot; /&gt; &lt;/ImportGroup&gt; &lt;PropertyGroup&gt; &lt;IncludePath&gt;../include;$(LocalIncludePaths);$(IncludePath)&lt;/IncludePath&gt; &lt;ReferencePath&gt;$(VisualLeakDetectorLibPath);$(LocalLibraryPaths);$(ReferencePath)&lt;/ReferencePath&gt; &lt;LibraryPath&gt;$(VisualLeakDetectorLibPath);$(LocalLibraryPaths);$(LibraryPath)&lt;/LibraryPath&gt; &lt;/PropertyGroup&gt; &lt;ItemDefinitionGroup&gt; &lt;Link&gt; &lt;AdditionalLibraryDirectories&gt;$(LocalLibraryPaths);%(AdditionalLibraryDirectories)&lt;/AdditionalLibraryDirectories&gt; &lt;AdditionalDependencies&gt;netcdf.lib;yaml-cpp.lib;moirai.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;!--AdditionalDependencies&gt;vld.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies--&gt; &lt;/Link&gt; &lt;/ItemDefinitionGroup&gt; . A priori this is independent of the version of compiler used. A priori. . &lt;PlatformToolset&gt;v120&lt;/PlatformToolset&gt; . So, is it just a matter of just bulk replacing to &lt;PlatformToolset&gt;v143&lt;/PlatformToolset&gt;? Even if considering only the purely mechanistic aspect (notwithstanding users), probably not. . Dependencies . The following figure gives an overview of the dependencies of the main DLLs. There are already several versions of the MS C runtimes, which is possible so long as libraries interact in a binary compatible way (C API). Usually, C++ level binary compatibility is not achievable. Never, in practice, so far as I recall. . . High level options . There are two main options to upgrade compilers . Give up on compiling using MS VCpp and migrate the MinGW toolchain (gcc). While it would be interesting to trial, the debugging facilities offered by the Microsoft tools and IDEs are a key value added for users. | Upgrade the compilation to a newer version of MS vc++. A quick scan of what is available out there for the dependencies of the stack (boost, netcdf, etc.) suggests that the build tool chain 2017 is prevalent. | . Related to that are 2 key dependencies: . netcdf: does this need to be stuck at msvcr100.dll (not that it is much of a problem) | boost: while boost is largely header only, it has a few libraries. These need to be brought to the same version of VCPP as that to compile our software, contrary to netcdf, because boost is C++, not C. | . Finally, some mostly orthogonal concerns . Move to use cmake to manage compilations, exactly as we do on Linux. | Set up a CI/CD for compilation on windows. | Automation of the migration to various versions of MSVCPP. | . Plan . The rest of this post will be a log of a “dry run” trying to migrate to MS build toolchain 2017 (vcpp v14.x to be determined). I’ll be capturing steps with a view to automate a CI/CD pipeline for windows soon, and also so that we can more easily adjust our target migration if I hit a blocker. . Walkthrough . I already have a recent visual studio installed via my corporate software management system. I do need to install the Microsoft build tools for visual studio 2017. I note two things in what options this selects: the compiler version (perhaps known as platform version) is v141, and Windows SDK 10.0.17763.0. . Boost . Read Boost 1.79.0 for windows. . Compile from source . I first tried to compile from source, out of curiosity, but this failed. For the record a summary follows. . Opening the Visual Studio 2017 Developer Command Prompt v15.0, then 7z x boost_1_79_0.7z. Note that running bootstrap indicated that it was Using &#39;vc143&#39; toolset so this may be an issue. . Then doing: . mkdir c: tmp boost b2.exe --prefix=c: tmp boost . c:/src/tmp/boost_1_79_0/tools/build/src/build targets.jam:617: in start-building from module targets error: Recursion in main target references error: the following target are being built currently: error: ./forward -&gt; ./stage -&gt; ./stage-proper -&gt; ***libs/filesystem/build/stage*** -&gt; libs/filesystem/build/stage-dependencies -&gt; libs/log/build/stage -&gt; libs/log/build/stage-dependencies -&gt; ***libs/filesystem/build/stage*** . Go for the prebuilt windows binaries then… . precompiled Boost binaries . https://www.boost.org/users/download/ leads to boost_1_79_0-msvc-14.1-64.exe on sourceforge . Copying a subset of the libraries: . set vcpp_ver=14.1 set boost_ver=1_79_0 set vcpp_ver_s=141 set src_root_dir=C: local boost_%boost_ver% set src_lib_dir=%src_root_dir% lib64-msvc-%vcpp_ver% set src_header_dir=%src_root_dir% boost set dest_root_dir=C: local set dest_dbg_root_dir=C: localdev set dest_lib_dir=%dest_root_dir% libs 64 set dest_dbg_lib_dir=%dest_dbg_root_dir% libs 64 set dest_header_dir=%dest_root_dir% include :: Cleanup or backup? mkdir %dest_lib_dir% set COPYOPTIONS=/Y /R /D xcopy %src_lib_dir% boost_chrono-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_date_time-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_filesystem-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_system-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_thread-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_regex-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_chrono-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_date_time-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_filesystem-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_system-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_thread-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_regex-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% :: header files mkdir %dest_header_dir% mv %src_header_dir% %dest_header_dir% . Moirai . moirai is the workorse for reference counting opaque pointers. . Open project with ms vstudio 2019 or more does offer an upgrade of the project. There are options to upgrade the windows sdk to a couple of versions.including the 10.0.17763.0 we noted. Note that the Platform toolset option however does not include v141. . So, a manual modification of the .vcxproj file is needed (or via project properties from vsstudio after opening the project, it is available this time) . &lt;WindowsTargetPlatformVersion&gt;10.0.17763.0&lt;/WindowsTargetPlatformVersion&gt; &lt;PlatformToolset&gt;v141&lt;/PlatformToolset&gt; . And this appears to compile. . netCDF . Again may be interesting to compile from source, but not essential. netcdf windows binaries document. Appears to be compiled with vcpp 2017. . Downloading netCDF4.9.0-NC4-64.exe . It appears to run on top of the v140 platform tools.That’s OK since this is a C API. Try. May see if we can compile from source with v141 target later. . xcopy C: tmp netcdf bin *.dll %dest_lib_dir% %COPYOPTIONS% xcopy C: tmp netcdf lib *.lib %dest_lib_dir% %COPYOPTIONS% mkdir %dest_header_dir% netcdf xcopy C: tmp netcdf include *.h %dest_header_dir% netcdf %COPYOPTIONS% . jsoncpp . I have a fork of jsoncpp and a branch with a customised .vcxproj file. Modifying with: . &lt;PlatformToolset&gt;v141&lt;/PlatformToolset&gt; &lt;WindowsTargetPlatformVersion&gt;10.0.17763.0&lt;/WindowsTargetPlatformVersion&gt; . And this compiles fine. Note that without WindowsTargetPlatformVersion in the project file, this failed to compile. . Now, how do I automate the building of these? TODO perhaps streamline the powershell script to deal with jsoncpp too. Even if likely very infrequent. . set jsoncpp_srcdir=C: src jsoncpp set jsoncpp_builddir=%jsoncpp_srcdir% makefiles custom x64 xcopy %jsoncpp_builddir% Release jsoncpp.dll %dest_lib_dir% %COPYOPTIONS% xcopy %jsoncpp_builddir% Release jsoncpp.lib %dest_lib_dir% %COPYOPTIONS% :: Debug also needed, likely. Know of very odd crashes when mixing debug/nondebug in the past. May not be the case anymore. mkdir %dest_dbg_lib_dir% xcopy %jsoncpp_builddir% Debug jsoncpp.dll %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %jsoncpp_builddir% Debug jsoncpp.lib %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %jsoncpp_builddir% Debug jsoncpp.pdb %dest_dbg_lib_dir% %COPYOPTIONS% set robocopy_opt=/MIR /MT:1 /R:2 /NJS /NJH /NFL /NDL /XX robocopy %jsoncpp_srcdir% include json %dest_header_dir% json %robocopy_opt% . yaml-cpp . set yamlcpp_srcdir=C: src yaml-cpp set yamlcpp_builddir=%yamlcpp_srcdir% vsproj x64 xcopy %yamlcpp_builddir% Release yaml-cpp.dll %dest_lib_dir% %COPYOPTIONS% xcopy %yamlcpp_builddir% Release yaml-cpp.lib %dest_lib_dir% %COPYOPTIONS% :: Debug also needed, likely. Know of very odd crashes when mixing debug/nondebug in the past. May not be the case anymore. xcopy %yamlcpp_builddir% Debug yaml-cpp.dll %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %yamlcpp_builddir% Debug yaml-cpp.lib %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %yamlcpp_builddir% Debug yaml-cpp.pdb %dest_dbg_lib_dir% %COPYOPTIONS% robocopy %yamlcpp_srcdir% include yaml-cpp %dest_header_dir% yaml-cpp %robocopy_opt% . uchronia / datatypes . Uchronia - time series handling for ensembles simulations and forecasts in C++ is the bedrock for data handling in our stack. . At this point I do need to copy the files for the catch unit testing framework, from config-utils, which the unit tests rely on. I notice also a bit of a minor issue, because the unit tests compile against the deployed headers, not the ones from source. This may be a gotcha to watch for, because my vcpp_settings.props file is not set up for development mode compilation. Beware when setting a build pipeline. . robocopy C: src config-utils catch include catch %dest_header_dir% catch %robocopy_opt% . The compilation and deploymebnt of this is relatively streamlined by using a high level powershell script. . swift . swift uses wila and its multithreading optimnisers. We need to install a complementary threadpool tool. . robocopy C: src threadpool boost %dest_header_dir% boost %robocopy_opt% . Also uses in-house numerical header-only files: . robocopy C: src numerical-sl-cpp algorithm include sfsl %dest_header_dir% sfsl %robocopy_opt% robocopy C: src numerical-sl-cpp math include sfsl %dest_header_dir% sfsl %robocopy_opt% . Error C2039 &#39;tuple&#39;: is not a member of &#39;boost::math&#39; (compiling source file channel_muskingum_nl.cpp) libswift c: src swift libswift include swift channel_muskingum_nl.h 74 . Seems I just an explicit additional #include &lt;boost/math/tools/tuple.hpp&gt; now with a more recent version of boost. . robocopy C: local include_old tclap %dest_header_dir% tclap %robocopy_opt% :: &lt;!-- and for fogss later on: --&gt; robocopy C: local include_old eigen3 %dest_header_dir% eigen3 %robocopy_opt% . Error C2079 &#39;outfile&#39; uses undefined class &#39;std::basic_ofstream&lt;char,std::char_traits&lt;char&gt;&gt;&#39; swiftcl c: src swift applications swiftcl run_calibration.cpp 120 . #include &lt;fstream&gt; is now needed. . After that, unit tests are mostly as expected, a few watchpoint that may be red herring. . Conclusion, and next . While it took a bit longer than ideal, so far, it looks rather straightforward and successful. Nothing was particularly risky a priori in this migration, but one is usually taken by a few surprises. . I am feeling the need for a more fully automated CI/CD. To be fair to myself, a fair bit of streamline already to build on. A challenge with the CI/CD, aside from some of the logistical cost of setting it up, probably on Azure Devops, is to define the artefacts it produces. Some artefacts the way they are currently would be better served as conda packages, but this is a larger scope. .",
            "url": "https://jmp75.github.io/work-blog/recipes/vcpp/c++/2022/06/26/vcpp-compilation-upgrade.html",
            "relUrl": "/recipes/vcpp/c++/2022/06/26/vcpp-compilation-upgrade.html",
            "date": " • Jun 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Setting up a private conda channel - part 1",
            "content": "Background . I have a conda package for a compiled native library (moirai). While it may be in a position to be submitted to conda-forge, I will also be interested at some point in setting up “my” own conda channel, with a view to distribute packages on my enterprise intranet. I may as well use that library moirai to test setting up a custom conda channel. . This post will be a review of the possible ways to do this. . Resources . conda documentation . The main conda documentation has sections on managing channels and creating custom channels. . fast.ai . Jeremy Howard has written the post “fastchan, a new conda mini-distribution”. It is a good read giving the rationale for setting up this channel. The repo of the conda channel ‘fastconda’ has some elements of pipelines but may be a good starting point for channels hosted by Anaconda. I am not sure I can repurpose this for a private channel. . Reusing conda-forge? . Since I intuit that a process similar to that of conda-forge could be what is emulated internally, an option may be to upfront borrow from the conda-forge documentation . The workhorse of conda-forge appears to be using conda-smithy to manage your CI, though that section actually rather confused me at the first read. The github readme of conda-smithy may be a better starting point. . https://github.com/conda-forge/astra-toolbox-feedstock is a recently accepted recipe. Likely a good template to study for the swift stack. . Commercial offerings . At an enterprise level it may be better to use Anaconda Server rather than cook up something. I probably cannot afford the time to trial standing one up, but my IM&amp;T business unit may look at it. In particular, if there are use cases for sophisticated user based access controls, “free” solutions may not be up to scratch nor to scale. | Anaconda cloud thingy | Channel on anaconda.org | . Other . How To: Set up a local Conda channel for installing the ArcGIS Python API | Building a Private Conda Channel | Setting up a feedstock from scratch | The github repo Private Conda Repository may be a good first step. | . By the way one misleading thing if you google “How do I set up a conda channel?”: the erronously titled video How to create new channel in Anaconda (python) is actually demonstrating the creation of a conda environment. Skip. . Stocktake . The first impression is that there is not a clear, single path to standing up your own conda channel over HTTP(S), no turn-key solution. The closest may be the github repo Private Conda Repository. . A first plan for the next steps, presumably next posts, are: . First, set up a file based channel following the conda documentation for creating custom channels. | Second, try to use PCR: Private Conda Repository. | .",
            "url": "https://jmp75.github.io/work-blog/conda/conda-channel/2022/06/18/conda-channels-prep-work.html",
            "relUrl": "/conda/conda-channel/2022/06/18/conda-channels-prep-work.html",
            "date": " • Jun 18, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Lithology classification using Hugging Face, part 2",
            "content": "About . This is a continuation of Lithology classification using Hugging Face, part 1. . We saw in the previous post that the Namoi lithology logs data had their primary (major) lithology mostly completed. A substantial proportion had the label None nevertheless, despite descriptions that looked like they would obviously lead to a categorisation. There were many labels, with a long-tailed frequency histogram. . The aim of this post is (was) to get a classification training happening. . Spoiler alert: it won&#39;t. Almost. . Rather than write a post after the fact pretending it was a totally smooth journey, the following walktrough deliberately keeps and highlights issues, albeit succinctly. Don&#39;t jump to the conclusion that we will not get there eventually, or that Hugging Face is not good. When you adapt prior work to your own use case, you will likely stumble, so this post will make you feel in good company. . Kernel installation . The previous post was about data exploration and used mostly facilities such as pandas, not any deep learning related material. This post will, so we need to install Hugging Face. I did bump into a couple of issues while trying to get an environment going. I will not give the full grubby details, but highlight upfront a couple of things: . Do create a new dedicated conda environment for your work with Hugging Face, even if you already have an environment with e.g. pytorch you&#39;d like to reuse. | The version 4.11.3 of HF transformers on the conda channel huggingface, at the time of writing, has a bug. You should install the packages from the conda-forge channel. | . In a nutshell, for Linux: . myenv=hf mamba create -n $myenv python=3.9 -c conda-forge mamba install -n $myenv --yes ipykernel matplotlib sentencepiece scikit-learn -c conda-forge mamba install -n $myenv --yes pytorch=1.11 -c pytorch -c nvidia -c conda-forge mamba install -n $myenv --yes torchvision torchaudio -c pytorch -c nvidia -c conda-forge mamba install -n $myenv --yes -c conda-forge datasets transformers conda activate $myenv python -m ipykernel install --user --name $myenv --display-name &quot;Hugging Face&quot; . and in Windows: . set myenv=hf mamba create -n %myenv% python=3.9 -c conda-forge mamba install -n %myenv% --yes ipykernel matplotlib sentencepiece scikit-learn -c conda-forge mamba install -n %myenv% --yes pytorch=1.11 -c pytorch -c nvidia -c conda-forge mamba install -n %myenv% --yes torchvision torchaudio -c pytorch -c nvidia -c conda-forge mamba install -n %myenv% --yes -c conda-forge datasets transformers conda activate %myenv% python -m ipykernel install --user --name %myenv% --display-name &quot;Hugging Face&quot; . Walkthrough . Let&#39;s get on with all the imports upfront (not obvious, mind you, but after the fact...) . import numpy as np import pandas as pd import torch from datasets import Dataset from transformers import AutoModelForSequenceClassification, AutoTokenizer from pathlib import Path from datasets import ClassLabel from transformers import TrainingArguments, Trainer from sklearn.metrics import f1_score from collections import Counter # Some column string identifiers MAJOR_CODE = &quot;MajorLithCode&quot; MAJOR_CODE_INT = &quot;MajorLithoCodeInt&quot; # We will create a numeric representation of labels, which is (I think?) required by HF. MINOR_CODE = &quot;MinorLithCode&quot; DESC = &quot;Description&quot; . fn = Path(&quot;~&quot;).expanduser() / &quot;data/ela/shp_namoi_river/NGIS_LithologyLog.csv&quot; litho_logs = pd.read_csv( fn, dtype={&quot;FromDepth&quot;: str, &quot;ToDepth&quot;: str, MAJOR_CODE: str, MINOR_CODE: str} ) # To avoid importing from the ela package, copy a couple of functions: # from ela.textproc import token_freq, plot_freq def token_freq(tokens, n_most_common=50): list_most_common = Counter(tokens).most_common(n_most_common) return pd.DataFrame(list_most_common, columns=[&quot;token&quot;, &quot;frequency&quot;]) def plot_freq(dataframe, y_log=False, x=&quot;token&quot;, figsize=(15, 10), fontsize=14): &quot;&quot;&quot;Plot a sorted histogram of work frequencies Args: dataframe (pandas dataframe): frequency of tokens, typically with colnames [&quot;token&quot;,&quot;frequency&quot;] y_log (bool): should there be a log scale on the y axis x (str): name of the columns with the tokens (i.e. words) figsize (tuple): fontsize (int): Returns: barplot: plot &quot;&quot;&quot; p = dataframe.plot.bar(x=x, figsize=figsize, fontsize=fontsize) if y_log: p.set_yscale(&quot;log&quot;, nonposy=&quot;clip&quot;) return p litho_classes = litho_logs[MAJOR_CODE].values df_most_common = token_freq(litho_classes, 50) plot_freq(df_most_common) . Imbalanced data sets . From the histogram above, it is pretty clear that labels are also not uniform an we have a class imbalance. Remember to skim Lithology classification using Hugging Face, part 1 for the initial data exploration if you have not done so already. . For the sake of the exercise in this post, I will reduce arbitrarily the number of labels used in this post, by just &quot;forgetting&quot; the less represented classes. . There are many resources about class imbalances. One of them is 8 Tactics to combat imbalanced classes in your machine learning dataset . Let&#39;s see what labels we may want to keep for this post: . def sample_desc_for_code(major_code, n=50, seed=None): is_code = litho_logs[MAJOR_CODE] == major_code coded = litho_logs.loc[is_code][DESC] if seed is not None: np.random.seed(seed) return coded.sample(n=50) . sample_desc_for_code(&quot;UNKN&quot;, seed=123) . The &quot;unknown&quot; category is rather interesting in fact, and worth keeping as a valid class. . Subsetting . Let&#39;s keep &quot;only&quot; the main labels, for the sake of this exercise. We will remove None however, despite its potential interest. We will (hopefully) revisit this in another post. . labels_kept = df_most_common[&quot;token&quot;][:17].values # 17 first classes somewhat arbitraty labels_kept = labels_kept[labels_kept != &quot;None&quot;] labels_kept . kept = [x in labels_kept for x in litho_classes] litho_logs_kept = litho_logs[kept].copy() # avoid warning messages down the track. litho_logs_kept.sample(10) . labels = ClassLabel(names=labels_kept) int_labels = np.array([ labels.str2int(x) for x in litho_logs_kept[MAJOR_CODE].values ]) int_labels = int_labels.astype(np.int8) # to mimick chapter3 HF so far as I can see . litho_logs_kept[MAJOR_CODE_INT] = int_labels . Class imbalance . Even our subset of 16 classes is rather imbalanced; the number of &quot;clay&quot; labels is looking more than 30 times that of &quot;coal&quot; just by eyeballing. . The post by Jason Brownlee 8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset, outlines several approaches. One of them is to resample from labels, perhaps with replacement, to equalise classes. It is a relatively easy approach to implement, but there are issues, growing with the level of imbalance. Notably, if too many rows from underrepresented classes are repeated, there is an increased tendency to overfitting at training. . The video Simple Training with the 🤗 Transformers Trainer (at 669 seconds) also explains the issues with imbalances and crude resampling. It offers instead a solution with class weighting that is more robust. That approach is evoked in Jason&#39;s post, but the video has a &quot;Hugging Face style&quot; implementation ready to repurpose. . Resample with replacement . Just for information, what we&#39;d do with a relatively crude resampling may be: . def sample_major_lithocode(dframe, code, n=10000, seed=None): x = dframe[dframe[MAJOR_CODE] == code] replace = n &gt; len(x) return x.sample(n=n, replace=replace, random_state=seed) . sample_major_lithocode(litho_logs_kept, &quot;CLAY&quot;, n=10, seed=0) . balanced_litho_logs = [ sample_major_lithocode(litho_logs_kept, code, n=10000, seed=0) for code in labels_kept ] balanced_litho_logs = pd.concat(balanced_litho_logs) balanced_litho_logs.head() . plot_freq(token_freq(balanced_litho_logs[MAJOR_CODE].values, 50)) . Dealing with imbalanced classes with weights . Instead of the resampling above, we adapt the approach creating weights for the Trainer we will run. . sorted_counts = litho_logs_kept[MAJOR_CODE].value_counts() sorted_counts . sorted_counts / sorted_counts.sum() . class_weights = (1 - sorted_counts / sorted_counts.sum()).values class_weights . We check that cuda is available (of course optional) . assert torch.cuda.is_available() . On Linux if you have a DELL laptop with an NVIDIA card, but nvidia-smi returns: NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running, you may need to change your kernel specification file $HOME/.local/share/jupyter/kernels/hf/kernel.json. This behavior seems to depend on the version of Linux kernel you have. It certainly changed out of the blue for me from yesterday, despite no change that I can tell. . optirun nvidia-smi returning a proper graphic card report should be a telltale sign you have to update your kernel.json like so: . { &quot;argv&quot;: [ &quot;optirun&quot;, &quot;/home/your_ident/miniconda/envs/hf/bin/python&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;Hugging Face&quot;, &quot;language&quot;: &quot;python&quot;, &quot;metadata&quot;: { &quot;debugger&quot;: true } } . You may need to restart jupyter-lab, or visual studio code, etc., for change to take effect. Restarting the kernel may not be enough, conter-intuitively. . Background details about optirun architecture at [Bumblebee Debian]https://wiki.debian.org/Bumblebee . class_weights = torch.from_numpy(class_weights).float().to(&quot;cuda&quot;) class_weights . model_nm = &quot;microsoft/deberta-v3-small&quot; . Tokenisation . Bump on the road; download operations taking too long . At this point I spent more hours than I wish I had on an issue, perhaps very unusual. . The operation tokz = AutoTokenizer.from_pretrained(model_nm) was taking an awful long time to complete: . CPU times: user 504 ms, sys: 57.9 ms, total: 562 ms Wall time: 14min 13s . To cut a long story short, I managed to figure out what was going on. It is documented on the Hugging Face forum at: Some HF operations take an excessively long time to complete. If you have issues where HF operations take a long time, read it. . Now back to the tokenisation story. Note that the local caching may be superflous if you do not encounter the issue just mentioned. . max_length = 128 . p = Path(&quot;./tokz_pretrained&quot;) pretrained_model_name_or_path = p if p.exists() else model_nm # https://discuss.huggingface.co/t/sentence-transformers-paraphrase-minilm-fine-tuning-error/9612/4 tokz = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=True, max_length=max_length, model_max_length=max_length) if not p.exists(): tokz.save_pretrained(&quot;./tokz_pretrained&quot;) . Let&#39;s see what this does on a typical lithology description . tokz.tokenize(&quot;CLAY, VERY SANDY&quot;) . Well, the vocabulary is probably case sensitive and all the descriptions being uppercase in the source data are likely problematic. Let&#39;s check what happens on lowercase descriptions: . tokz.tokenize(&quot;clay, very sandy&quot;) . This looks better. So let&#39;s change the descriptions to lowercase; we are not loosing any relevent information in this case, I think. . litho_logs_kept[DESC] = litho_logs_kept[DESC].str.lower() . litho_logs_kept_mini = litho_logs_kept[[MAJOR_CODE_INT, DESC]] litho_logs_kept_mini.sample(n=10) . Create dataset and tokenisation . We want to create a dataset such that tokenised data is of uniform shape (better for running on GPU) Applying the technique in this segment of the HF course video. Cheating a bit on guessing the length (I know from offline checks that max is 90 tokens) . ds = Dataset.from_pandas(litho_logs_kept_mini) def tok_func(x): return tokz( x[DESC], padding=&quot;max_length&quot;, truncation=True, max_length=max_length, return_tensors=&quot;pt&quot;, ) . The Youtube video above suggests to use tok_ds = ds.map(tok_func, batched=True) for a faster execution; however I ended up with the foollowing error: . TypeError: Provided `function` which is applied to all elements of table returns a `dict` of types [&lt;class &#39;torch.Tensor&#39;&gt;, &lt;class &#39;torch.Tensor&#39;&gt;, &lt;class &#39;torch.Tensor&#39;&gt;]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(&lt;class &#39;list&#39;&gt;, &lt;class &#39;numpy.ndarray&#39;&gt;)`. . The following non-batched option works in a reasonable time: . tok_ds = ds.map(tok_func) . tok_ds_tmp = tok_ds[:5] tok_ds_tmp.keys() . len(tok_ds_tmp[&quot;input_ids&quot;][0][0]) . num_labels = len(labels_kept) . p = Path(&quot;./model_pretrained&quot;) model_name = p if p.exists() else model_nm model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, max_length=max_length) # label2id=label2id, id2label=id2label).to(device) if not p.exists(): model.save_pretrained(p) . print(type(model)) . # litho_desc_list = [x for x in litho_logs_kept_mini[DESC].values] # input_descriptions = tokz(litho_desc_list, padding=True, truncation=True, max_length=256, return_tensors=&#39;pt&#39;) # input_descriptions[&#39;input_ids&#39;].shape # model(input_descriptions[&#39;input_ids&#39;][:5,:], attention_mask=input_descriptions[&#39;attention_mask&#39;][:5,:]).logits . tok_ds . Transformers always assumes that your labels has the column name &quot;labels&quot;. Odd, but at least this fosters a consistent system, so why not: . tok_ds = tok_ds.rename_columns({MAJOR_CODE_INT: &quot;labels&quot;}) . tok_ds = tok_ds.remove_columns([&#39;Description&#39;, &#39;__index_level_0__&#39;]) . # Note that HF is supposed to take care of movind data to the GPU if available, so you should not ahve to manually copy the data to the GPU device tok_ds.set_format(&quot;torch&quot;) . # evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, # num_train_epochs=epochs, weight_decay=0.01, report_to=&#39;none&#39;) . dds = tok_ds.train_test_split(0.25, seed=42) . dds.keys() . tok_ds.features[&#39;labels&#39;] = labels . tok_ds.features # TODO: # This differs from chapter3 of HF course https://huggingface.co/course/chapter3/4?fw=pt # {&#39;attention_mask&#39;: Sequence(feature=Value(dtype=&#39;int8&#39;, id=None), length=-1, id=None), # &#39;input_ids&#39;: Sequence(feature=Value(dtype=&#39;int32&#39;, id=None), length=-1, id=None), # &#39;labels&#39;: ClassLabel(num_classes=2, names=[&#39;not_equivalent&#39;, &#39;equivalent&#39;], id=None), # &#39;token_type_ids&#39;: Sequence(feature=Value(dtype=&#39;int8&#39;, id=None), length=-1, id=None)} . tok_ds[&#39;input_ids&#39;][0] . . # def compute_metrics(eval_pred): # logits, labels = eval_pred # predictions = np.argmax(logits, axis=-1) # return metric.compute(predictions=predictions, references=labels) . class WeightedLossTrainer(Trainer): def compute_loss(self, model, inputs, return_outputs=False): # Feed inputs to model and extract logits outputs = model(**inputs) logits = outputs.get(&quot;logits&quot;) # Extract Labels labels = inputs.get(&quot;labels&quot;) # Define loss function with class weights loss_func = torch.nn.CrossEntropyLoss(weight=class_weights) # Compute loss loss = loss_func(logits, labels) return (loss, outputs) if return_outputs else loss . def compute_metrics(eval_pred): labels = eval_pred.label_ids predictions = eval_pred.predictions.argmax(-1) f1 = f1_score(labels, predictions, average=&quot;weighted&quot;) return {&quot;f1&quot;: f1} . output_dir = &quot;./hf_training&quot; batch_size = 64 # 128 epochs = 5 lr = 8e-5 . training_args = TrainingArguments( output_dir=output_dir, num_train_epochs=epochs, learning_rate=lr, lr_scheduler_type=&quot;cosine&quot;, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size * 2, weight_decay=0.01, evaluation_strategy=&quot;epoch&quot;, logging_steps=len(dds[&quot;train&quot;]), fp16=True, push_to_hub=False, report_to=&quot;none&quot;, ) . model = model.to(&quot;cuda:0&quot;) . The above nay not be strictly necessary, depending on your version of transformers. I bumped into the following issue, which was probably the transformers 4.11.3 bug: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select) . trainer = Trainer( model=model, args=training_args, train_dataset=dds[&quot;train&quot;], eval_dataset=dds[&quot;test&quot;], tokenizer=tokz, compute_metrics=compute_metrics, ) . Training? . You did read the introduction and its spoiler alert, right? . trainer.train() . Stocktake and conclusion . So, as announced at the start of this post, we hit a pothole in our journey. . RuntimeError: The size of tensor a (768) must match the size of tensor b (128) at non-singleton dimension 3 . Where the number (768) comes from is a bit of a mystery. I gather from Googling that this may have to do with the embedding of the Deberta model we are trying to fine tune, but I may be off the mark. . It is probably something at which an experience NLP practitioner will roll their eyes. . That&#39;s OK, We&#39;ll get there. .",
            "url": "https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html",
            "relUrl": "/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html",
            "date": " • Jun 13, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Conda package for compiled native libraries",
            "content": "Background . I recently wrote about submitting your first conda package to conda-forge. You’ll find background on the “bigger picture” endeavour in that previous post. . This post is a follow-on with, perhaps, a second submission to conda-forge, this time not of a python package but a C++ library. . I’ll try to package a relatively small C++ codebase MOIRAI: Manage C++ objects lifetime when exposed through a C API. While dealing with very similar needs as refcount (reference counting and memory management), there is no explicit dependency between them. . Market review . moirai grew out of specific projects almost a decade ago, but its inception did not occur without looking first at third party options. There were surprisingly few I could identify, and of the ones I saw licensing or design made it difficult to adopt as they were. Still, in 2022 is there, on conda-forge or not, something making moirai possibly redundant? . It can be tricky to find relevant work without a time consuming research. A cursory scan comes up: . Cppy seems to have intersects, but this is solely Python-centric. | Loki-lib is an (underappreciated) library with reference counting features, but not on conda-forge. I believe Loki-lib was largely written by Andrei Alexandrescu, author of Modern C++ Design, one of the most impressive computer science book I read. | . Maybe there is a place for moirai. Plus the name is not taken… . Walkthrough . I already forked and cloned staged-recipes from the previous post. . cd ~/src/staged-recipes git checkout main git branch moirai . Starting point . grayskull is userful to generate meta.yaml stubs out of python packages, and not applicable in this case. . I learned the hard way that installing conda-build, grayskull and shyaml and conda update condain my conda base environment landed me in a broken mamba and incompatible package versions. So, this time create a new dedicate environment: . conda create -n cf python=3.9 mamba -c conda-forge conda activate cf mamba install -c conda-forge conda-build grayskull # grayskull not for this post, but other future submissions . Is there an example I can start from and work by inference and similarity rather than first principles? . libnetcdf-feedstock may be an appropriate case to start from, even if more sophisticated than my case. Rather than strip down this libnetcdf recipe though, I work from the example in the staged-recipe and add, staying closer to contributing packages . I did bump into a couple of issues, but I got less issues than I thought to get a package compiling . The end result should be something like: . {% set name = &quot;moirai&quot; %} {% set version = &quot;1.1&quot; %} package: name: {{ name|lower }} version: {{ version }} source: # url: https://github.com/moirai/moirai/releases/download/{{ version }}/moirai-{{ version }}.tar.gz # and otherwise fall back to archive: url: https://github.com/csiro-hydroinformatics/moirai/archive/refs/tags/{{ version }}.tar.gz sha256: b329353aee261ec42ddd57b7bb4ca5462186b2d132cdb2c9dacc9325899b85f3 build: number: 0 requirements: build: - cmake - make # [not win] # - pkg-config # [not win] # - gnuconfig # [unix] - {{ compiler(&#39;c&#39;) }} - {{ compiler(&#39;cxx&#39;) }} run: - curl # [win] - libgcc # [unix] test: files: - CMakeLists.txt commands: - ls about: home: https://github.com/csiro-hydroinformatics/moirai summary: &#39;Manage C++ objects lifetime when exposed through a C API&#39; description: | This C++ library is designed to help handling C++ objects from so called opaque pointers, via a C API, featuring: * counting references via the C API to C++ domain objects * handle C++ class inheritance even via opaque pointers * mechanism for resilience to incorrect type casts * thread-safe design license: BSD-3-Clause license_family: BSD license_file: LICENSE.txt doc_url: https://github.com/csiro-hydroinformatics/moirai/blob/master/doc/Walkthrough.md dev_url: https://github.com/csiro-hydroinformatics/moirai extra: recipe-maintainers: - jmp75 . Note that you do need make as a build requirement besides cmake, otherwise you’d end up with : . CMake Error: CMake was unable to find a build program corresponding to &quot;Unix Makefiles&quot;. CMAKE_MAKE_PROGRAM is not set. You probably need to select a different build tool. CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage . build.sh: . #!/bin/bash # Build moirai. mkdir -v -p build cd build # May be needed for unit tests down the track? export TERM= export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PREFIX/lib # cmake -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON .. cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${PREFIX} -DBUILD_SHARED_LIBS=ON ../ make -j 2 install # rm -rf ../build/ # not if we want to run the unit tests . Building locally on linux64 . Note: although it may have changed by the time you read this, at the time I write I may have to export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 to force that image being used. See previous post. . conda activate cf export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 cd ~/src/staged-recipes python ./build-locally.py linux64 . and the build seems OK… . + touch /home/conda/staged-recipes/build_artifacts/conda-forge-build-done . Building locally on win64 . Trying to run build-locally.py with the option win64 returns: ValueError: only Linux/macOS configs currently supported, got win64 . You’ll want to read or re-read the conda-forge documentation Using cmake and particularities on windows . Trying for bld.bat: . REM Build moirai. @REM Credits: some material in this file are courtesy of Kavid Kent (ex Australian Bureau of Meteorology) mkdir build cd build @REM following may be unncessary set PATH=&quot;%PREFIX% bin&quot;;%PATH% :: Configure using the CMakeFiles cmake -G &quot;%CMAKE_GENERATOR%&quot; ^ -DCMAKE_INSTALL_PREFIX:PATH=&quot;%LIBRARY_PREFIX%&quot; ^ -DCMAKE_PREFIX_PATH:PATH=&quot;%LIBRARY_PREFIX%&quot; ^ -DCMAKE_BUILD_TYPE:STRING=Release ^ .. if %errorlevel% neq 0 exit 1 msbuild /p:Configuration=Release /v:q /clp:/v:q &quot;INSTALL.vcxproj&quot; if %errorlevel% neq 0 exit 1 @REM del *.* . Since the local build script cannot emulate a win64 build, I am trying to set up on a Windows box and see what a conda build gives. . I first tried on my Windows desktop where I do have vs 2019 and 2022 installed. Not sure whether they can be used. This section of the conda-forge doc seem to suggest Microsoft Build Tools for Visual Studio 2017 is required, but the link to the Python wiki page on Windows compilers is covering many more versions, so this is rather confusing. See the Appendix for more details on what happens in this case. . Trying again on a windows machine, but installing visual studio build tools 2017. . After creating the conda environment cf, similar to the above on Linux: . cd c: src staged-recipes recipes conda build moirai . “mostly” works. for a minute or two it seems to freeze at a “number of files” line: . Packaging moirai INFO:conda_build.build:Packaging moirai Packaging moirai-1.1-h82bb817_0 INFO:conda_build.build:Packaging moirai-1.1-h82bb817_0 number of files: 11 . then: . PGO: UNKNOWN is not implemented yet! PGO: UNKNOWN is not implemented yet! Unknown format Unknown format Unknown format Unknown format INFO: sysroot: &#39;C:/Windows/&#39; files: &#39;[&#39;zh-CN/winhlp32.exe.mui&#39;, &#39;zh-CN/twain_32.dll.mui&#39;, &#39;zh-CN/regedit.exe.mui&#39;, &#39;zh-CN/notepad.exe.mui&#39;]&#39; &lt;edit: snip&gt; Importing conda-verify failed. Please be sure to test your packages. conda install conda-verify to make this message go away. . conda search -c conda-forge conda-verify indeed returns something. Noted… . &lt;edit: snip&gt; ## Package Plan ## environment location: C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 _test_env The following NEW packages will be INSTALLED: ca-certificates: 2022.4.26-haa95532_0 curl: 7.82.0-h2bbff1b_0 libcurl: 7.82.0-h86230a5_0 libssh2: 1.10.0-hcd4344a_0 moirai: 1.1-h82bb817_0 local openssl: 1.1.1o-h2bbff1b_0 vc: 14.2-h21ff451_1 vs2015_runtime: 14.27.29016-h5e58377_2 zlib: 1.2.12-h8cc25b3_2 . &lt;edit: snip&gt; set PREFIX=C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 _test_env set SRC_DIR=C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 test_tmp (cf) %SRC_DIR%&gt;call &quot;%SRC_DIR% conda_test_env_vars.bat&quot; (cf) %SRC_DIR%&gt;set &quot;CONDA_SHLVL=&quot; &amp;&amp; (cf) %SRC_DIR%&gt;conda activate &quot;%PREFIX%&quot; (%PREFIX%) %SRC_DIR%&gt;IF 0 NEQ 0 exit /B 1 (%PREFIX%) %SRC_DIR%&gt;call &quot;%SRC_DIR% run_test.bat&quot; (%PREFIX%) %SRC_DIR%&gt;ls (%PREFIX%) %SRC_DIR%&gt;IF -1073741511 NEQ 0 exit /B 1 . The latter fails because the command line ls borks with the error message (windows box error message) ls.exe - Entry Point not found. where ls returns C: Users xxxyyy Miniconda3 envs cf Library usr bin ls.exe so this is something that came from conda. . Taking a look at the resulting moirai-1.1-h82bb817_0.tar.bz2 file, the content looks sensible (include header files, bin/moirai.dll) . info/hash_input.json info/index.json info/files info/paths.json info/about.json info/git info/recipe/build.sh info/recipe/meta.yaml.template info/licenses/LICENSE.txt Library/include/moirai/extern_c_api_as_opaque.h Library/include/moirai/extern_c_api_as_transparent.h Library/include/moirai/setup_modifiers.h Library/include/moirai/reference_handle_map_export.h Library/include/moirai/error_reporting.h Library/include/moirai/reference_handle.h info/recipe/conda_build_config.yaml info/recipe/meta.yaml info/test/run_test.bat info/recipe/bld.bat Library/bin/moirai.dll Library/include/moirai/reference_handle_test_helper.hpp Library/include/moirai/opaque_pointers.hpp Library/include/moirai/reference_type_converters.hpp Library/include/moirai/reference_handle.hpp . Recapitulation and summary . In some respects I had less issues with this conda package than the “pure python” one, partly because of prior experience, but not entirely. . I may be in a decent place to submit this to the conda-forge/staged-recipe repository. I may hold off a bit though. First, the unit tests in moirai are not exercised by the meta.yaml file (or build scripts). Second, I may next look at setting up a conda channel to test managing conda dependencies, even if I see moirai as belonging to conda-forge rather than a private channel. Third, there are probably other things I need to tidy up. . To recapitulate on the essentials out of post: . conda create -n cf python=3.9 mamba -c conda-forge conda activate cf # grayskull not for this post, but other future submissions mamba install -c conda-forge conda-build conda-verify grayskull . The draft conda recipe for moirai at this stage is available there . Acknowledgements . Some of the conda recipe trialled was influenced by prior work by Kavid Kent (ex Australian Bureau of Meteorology). . Appendices . Appendix: if missing ms build tools 2017 . If trying without having installed ms build tools 2017: . %SRC_DIR%&gt;CALL %BUILD_PREFIX% etc conda activate.d vs2017_get_vsinstall_dir.bat Did not find VSINSTALLDIR Windows SDK version found as: &quot;10.0.19041.0&quot; The system cannot find the path specified. Did not find VSINSTALLDIR CMake Error at CMakeLists.txt:16 (PROJECT): Generator Visual Studio 15 2017 Win64 could not find any instance of Visual Studio. . (cf) %SRC_DIR%&gt;set &quot;SCRIPTS=%PREFIX% Scripts&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_PREFIX=%PREFIX% Library&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_BIN=%PREFIX% Library bin&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_INC=%PREFIX% Library include&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_LIB=%PREFIX% Library lib&quot; (cf) %SRC_DIR%&gt;set &quot;c_compiler=vs2017&quot; (cf) %SRC_DIR%&gt;set &quot;fortran_compiler=gfortran&quot; (cf) %SRC_DIR%&gt;set &quot;vc=14&quot; (cf) %SRC_DIR%&gt;set &quot;cxx_compiler=vs2017&quot; . call C: Users xxxyyy Miniconda3 Scripts activate.bat . Note: . base * C: Users xxxyyy Miniconda3 envs cf C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654760072022 _build_env C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654760072022 _h_env . If I try to start with Visual Studio 2022 Developer Command Prompt v17.1.5. Then conda environment activation, still: . %SRC_DIR%&gt;CALL %BUILD_PREFIX% etc conda activate.d vs2017_get_vsinstall_dir.bat Did not find VSINSTALLDIR Windows SDK version found as: &quot;10.0.19041.0&quot; ********************************************************************** ** Visual Studio 2022 Developer Command Prompt v17.1.5 ** Copyright (c) 2022 Microsoft Corporation ********************************************************************** [ERROR:vcvars.bat] Toolset directory for version &#39;14.16&#39; was not found. [ERROR:VsDevCmd.bat] *** VsDevCmd.bat encountered errors. Environment may be incomplete and/or incorrect. *** [ERROR:VsDevCmd.bat] In an uninitialized command prompt, please &#39;set VSCMD_DEBUG=[value]&#39; and then re-run [ERROR:VsDevCmd.bat] vsdevcmd.bat [args] for additional details. [ERROR:VsDevCmd.bat] Where [value] is: [ERROR:VsDevCmd.bat] 1 : basic debug logging [ERROR:VsDevCmd.bat] 2 : detailed debug logging [ERROR:VsDevCmd.bat] 3 : trace level logging. Redirection of output to a file when using this level is recommended. [ERROR:VsDevCmd.bat] Example: set VSCMD_DEBUG=3 [ERROR:VsDevCmd.bat] vsdevcmd.bat &gt; vsdevcmd.trace.txt 2&gt;&amp;1 Did not find VSINSTALLDIR CMake Error at CMakeLists.txt:16 (PROJECT): Generator Visual Studio 15 2017 Win64 could not find any instance of Visual Studio. -- Configuring incomplete, errors occurred! . Resources . Reference Counting in Library Design – Optionally and with Union-Find Optimization | .",
            "url": "https://jmp75.github.io/work-blog/recipes/conda/conda-forge/c++/2022/06/10/conda-packages-conda-forge-2.html",
            "relUrl": "/recipes/conda/conda-forge/c++/2022/06/10/conda-packages-conda-forge-2.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Submitting your first conda package to conda-forge",
            "content": "Background . For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. It is a stack with a core in C++, but accessible via a C API by users from R, Matlab, Python and so on. A whole article could be written about the design rationale, successes and shortcomings of this stack, and the interplay of people, organisations and technologies in using these tools and how. But this will not be this post. . Focussing on the Python side of things, these streamflow forecasting tools are used mostly on Windows and Linux, the core is deployed as dynamic libraries (.dll or .so) on disk, and python packages access these using cffi for interoperability. The python packages contain solely python code; there is no cython or straight C. . I’ve come to appreciate (mostly) conda environments for managing software stacks for various projects. This post is a start to test packaging some of “my” software with conda, in the hope this reduces the surprisingly strong impedance, technical but not only, towards usage by a broader audience. . A bit picture end point would be a corporate equivalent to a conda-forge channel, with the full software stack available for any employee. . Baby steps . I had a look a few weeks ago at how I’d package a substantial but relatively small C++ code MOIRAI: Manage C++ Objects’s lifetime when exposed through a C API. This proved a bit premature for reasons I won’t detail. . So, let’s (re)start with a python-only library, as it happens in the same vein, refcount. Astonishingly, there is still no strict equivalent that I can find in conda-forge dedicated to reference counting external pointers. So, can I claim the spot? . Resources . I started this post thinking first about conda packaging rather than submission per se to conda-forge. Some resources I initially looked at as promising, but from which I backed away (for now): . Building conda packages from scratch | python packaging tutorials Scipy 2018 Tutorial: The Joy of Packaging - conda packages | Activision Game: A tutorial (+ build recipes) describing how to use conda for C, C++, and python package management outlines well the rationale for packaging in conda, and appears didactic. It appears not to have recent commit, although it is not necessarily a problem. | . You’ll see in the walkthrough below (next section) that I reoriented towards an upfront submission to conda-forge: . 3 simple stept to build a python package for conda-forge. This post really got me on a better path. | conda-forge documentation: Contributing packages | . Walkthrough . First trial: conda build locally? . Of course python is necessary and a conda environment a given. I do . ~/config/baseconda because I never have conda activated by default from .bashrc. . I am actually not sure from the conda-build tutorial in which environment I should install conda-build. Let’s try the base environment and see whether we get stuck or not. . mamba install -c conda-forge conda-build (installing from conda-forge is an idiosyncrasy. I strongly recommend mamba) . You should at least skim through the concepts. This is rather dry to read throughly upfront. . The tutorial Building conda packages from scratch quickly confused me; I was trying to transpose it to refcount but this does not look like the right template to start from. The section editing-the-meta-yaml-file appears out of sync with the “correct” meta.yaml file. Baffling. . Preparing a PR to conda-forge/staged-recipes . Enter two new resources: 3 simple stept to build a python package for conda-forge and conda-forge documentation: Contributing packages. From these it becomes clear I should use grayskull to get a starting point as a meta.yaml file . cd ~/src git clone --depth 1 git@github.com:jmp75/staged-recipes.git cd ~/src/staged-recipes/recipes git remote add upstream git@github.com:conda-forge/staged-recipes.git mamba install -c conda-forge grayskull grayskull pypi refcount . #### Initializing recipe for refcount (pypi) #### Recovering metadata from pypi... Starting the download of the sdist package refcount refcount 100% Time: 0:00:00 15.3 MiB/s|###############################################################################################################################################################################################################################################| Recovering information from setup.py Executing injected distutils... Recovering metadata from setup.cfg No data was recovered from setup.py. Forcing to execute the setup.py as script Recovering metadata from setup.cfg Checking &gt;&gt; cffi 100% |##########################################################################################################################################################################################################################################|[Elapsed Time: 0:00:00] Matching license file with database from Grayskull... Match percentage of the license is 59%. Low match percentage could mean that the license was modified. License type: BSD-3-Clause License file: LICENSE.txt Host requirements: - pip - python Run requirements: - cffi - python RED: Missing packages GREEN: Packages available on conda-forge Maintainers: - j-m #### Recipe generated on /home/per202/src/staged-recipes/recipes for refcount #### . The output meta.yaml (which is actually a ninja template file), is a good start, however you should revise it a bit rather than accept wholesale . Mostly fine, however this did not pick up a requirement cffi &gt;=1.11.5, and second guessing from reading this gallon.me post, a minimum python version is necessary to get accepted. . requirements: host: - pip - python &gt;=3.6 run: - cffi &gt;=1.11.5 - python &gt;=3.6 . Perhaps optional, remove a hard-coded package string “refcount “in the source url section. . It is instructive to look at the existing pull requests on staged-recipes. Notably I realise that the github ID extracted by grayskull is not the correct one; I am not the ID j-m, unfortunately (could have been judging by history length). . extra: recipe-maintainers: - j-m . extra: recipe-maintainers: - jmp75 . The end result should be something like: . {% set name = &quot;refcount&quot; %} {% set version = &quot;0.9.3&quot; %} package: name: {{ name|lower }} version: {{ version }} source: url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.zip sha256: bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442 build: noarch: python script: {{ PYTHON }} -m pip install . -vv number: 0 requirements: host: - pip - python &gt;=3.6 run: - cffi &gt;=1.11.5 - python &gt;=3.6 test: imports: - refcount commands: - pip check requires: - pip about: home: https://github.com/csiro-hydroinformatics/pyrefcount summary: A Python package for reference counting and interop with native pointers description: | This package helps you achieve reliable management of memory allocated in native libraries, written for instance in C++. While it boils down to &quot;simply&quot; maintaining a set of counters, it is deceptively complicated to do so properly and not end up with memory leaks or crashes. &lt;https://pyrefcount.readthedocs.io/en/latest/&gt;. dev_url: https://github.com/csiro-hydroinformatics/pyrefcount license: BSD-3-Clause license_family: BSD license_file: LICENSE.txt extra: recipe-maintainers: - jmp75 . So; ready to submit a pull request? Wait, wait. . Building locally . test has a section on Running unit tests. Note that the default conda recipe above has a “pip check”, but nothing more. refcount unit tests use pytest, and has unit tests; tick that. refcount is a pure python package, but it is a package for (mostly) interoperability with native code via a C API. Unit tests do contain some c/c++ code. . Should the recipe run fine upon submission, including unit tests? Before submitting a pull request that may trigger a failed check, let’s experiment with staging tests locally. . so: . cd ~/src/staged-recipes python ./build-locally.py linux64 . File &quot;/home/per202/miniconda/lib/python3.9/subprocess.py&quot;, line 373, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command &#39;[&#39;.scripts/run_docker_build.sh&#39;]&#39; returned non-zero exit status 1. . I tried to conda install -c conda-forge shyaml which seems to be used by the scripts, but this did not alleviate the issue. . That took me some time to find a workaround to this one. The “exit status 1” is actually very misleading. The root cause is a docker run -it that exited with an error code 139. I seem to not be the only one to have bumped into this issue still open. I may have pointed to the workaround in the conda-forge FAQ. . I needed to override the default docker image build-locally.py falls back to with: . export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 python build-locally.py linux64 . the build script works this time, but at some point: . Processing $SRC_DIR Added file://$SRC_DIR to build tracker &#39;/tmp/pip-build-tracker-bkn7ckr9&#39; Running setup.py (path:$SRC_DIR/setup.py) egg_info for package from file://$SRC_DIR Created temporary directory: /tmp/pip-pip-egg-info-68li1y6t Preparing metadata (setup.py): started Running command python setup.py egg_info Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt; File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt; File &quot;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/setup.py&quot;, line 41, in &lt;module&gt; with open(os.path.join(here, &#39;README.md&#39;), encoding=&#39;utf-8&#39;) as f: File &quot;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pl/lib/python3.10/codecs.py&quot;, line 905, in open file = builtins.open(filename, mode, buffering) FileNotFoundError: [Errno 2] No such file or directory: &#39;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/README.md&#39; error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. . This is an issue that may be in my control. . cd ~/src/staged-recipes/build_artifacts/refcount_1654312313810/work ls ## build_env_setup.sh conda_build.sh LICENSE.txt MANIFEST.in metadata_conda_debug.yaml PKG-INFO README.rst refcount refcount.egg-info setup.cfg setup.py . refcount has both a README.md and README.rst, the latter being an export from the former because pypi requires (or used to require) a README.rst to display correctly. The zip archive of the source code on pypi indeed does not have the README.md file included. . I’ve inherited the practice to use in the packages setup.py the following, to limit redundances. . with open(os.path.join(here, &#39;README.md&#39;), encoding=&#39;utf-8&#39;) as f: long_description = f.read() long_description_content_type=&#39;text/markdown&#39; . Previously I needed to convert on the fly to restructured Text, but Markdown is more supported. Still there is a lot of inertia with restructuredText. . I may try to just nuke the README.rst. The only fly on the ointment is: is pypi ok with rendering markdown correctly these days? Probably; the packaging documentation is using README.md by default. . So, build and submit to pypi the updated refcount 0.9.4 with no README.rst. Looks fine, including the zip source archive. . RuntimeError: SHA256 mismatch: &#39;21567918cb1bb30bf8116ce3483d3f431de202618eabbc6887b4814b40a3b94a&#39; != &#39;bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442&#39; Traceback (most recent call last): . Right, I forgot to change the checksum in the meta.yaml file. . And… it seems to complete. . import: &#39;refcount&#39; + pip check No broken requirements found. + exit 0 Resource usage statistics from testing refcount: Process count: 1 CPU time: Sys=0:00:00.0, User=- Memory: 3.0M Disk usage: 28B Time elapsed: 0:00:02.1 TEST END: /home/conda/staged-recipes/build_artifacts/noarch/refcount-0.9.4-pyhd8ed1ab_0.tar.bz2 . Submit the pull request, and pleasantly: . . Conclusion . While there were a couple of bumps along the way, this should end up with a positive outcome. If not with refcount on conda-forge, I’ve a better understanding to tackle conda packaging on the rest of the software stack. . Building a conda package “from scratch” may not be the easiest learning path. Even if you indent to build a conda package not for conda-forge, going through the staged-recipes process may be a most | Some of the reference documentation may need a spruice up. Building conda packages from scratch confused me. First, packaging a pypi package is not starting “from scratch” for most users. Second, inconsistencies in the documentation. I am sure I’ll get back to that resource, but I wish there were more “water tight”, step-by-step tutorials for conda packaging. | .",
            "url": "https://jmp75.github.io/work-blog/recipes/conda/conda-forge/2022/06/04/conda-packages-conda-forge.html",
            "relUrl": "/recipes/conda/conda-forge/2022/06/04/conda-packages-conda-forge.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Lithology classification using Hugging Face, part 1",
            "content": "About . This is (perhaps) the start of a series of posts on using natural language processing for lithology classification. I hope to explore multi-label classification in subsequent posts. . . Background . I&#39;ve been attending virtually the latest Deep Learning course run by Jeremy Howard (See the fast.ai forum for pointers to past courses). Part of the experience can be to find &quot;homeworks&quot;. I traditionally work with point time series data, and it would have been a Pavlov reflex for me to get a use case with this type of data. . However in one of the lessons Jeremy used Hugging Face Transformers applied to the Kaggle competition U.S. Patent Phrase to Phrase Matching. At some point he made a comment about how much NLP progressed over the recent years, and how much potential value creation there was in this. . I do not follow closely NLP research, and am not knowledgeable enough to agree or not, but got an inkling of the potential value a few years back when working on a python package for exploratory lithology analysis, for groundwater characterisation. Lithology is &quot;the study of the general physical characteristics of rocks&quot;. Drilling core soil samples is not cheap; existing records are valuable. . Sections of a core sample drill can be described with sentences such as: . topsoil | shale, slippery back, green seams | sandstone alluvium water bearing | gravel red very clayey water supply | sandstone, red/pale brown, fine-coarse, white clay bands, brownish yellow bands, pebbles (16.9-16.98m) | fill; orange-brown, dry, loose, pebbles to 2.5cm, heterogeneous | clay; light brown, strongly cohesive, contains silicate &amp; carbonate clasts to coarse sand size, no clay smell. | . You would have an inkling that the prose of the writer (the &quot;driller&quot;) can vary in style and detail. One typical use case is to determine the primary and (optionally) secondary lithologies of a record, as it strongly influences how fast water can flow underground. &quot;gravel red very clayey&quot; is on the easier side of the spectrum of difficulty: &quot;gravel&quot; as a primary lithology and &quot;clay&quot; as a secondary lithology (assuming these are valid classes for the context). It can get much trickier to classify of course, and certainly expensive if done very manually. Automating this classification process opens the possibility of an iterative process towards a sound lithology classification fit for purpose. . I co-authored a conference presentation &quot;Comparing regular expression and deep learning methods for the classification of descriptive lithology&quot; (Page 161 of the MODSIM 2019 book of abstracts if you are curious). With all the caveats of a study made on very limited resources, I was surprised by how well NLP performed overall to train and test on a human-labeled dataset. . I am not working on this domain during paid hours these days, but feel like revisiting this for a bit over the course. . Getting acquainted with data . Arbitrarily, I downloaded data for the Namoi catchment from the Australian Groundwater Explorer. While I am not totally new to the domain, I genuinely do need to explore this data from scratch. This will probably be at least the rest of this post. . import pandas as pd from pathlib import Path . fn = Path(&#39;~&#39;).expanduser() / &quot;data/ela/shp_namoi_river/NGIS_LithologyLog.csv&quot; . The types of a few columns lead to warnings: let&#39;s force them to &#39;str&#39; here. I do not anticipate using the depth records for now. . litho_logs = pd.read_csv(fn, dtype={&#39;FromDepth&#39;: str, &#39;ToDepth&#39;: str, &#39;MajorLithCode&#39;: str, &#39;MinorLithCode&#39;: str}) . So how does this data look like? . litho_logs.head() . OBJECTID BoreID HydroCode RefElev RefElevDesc FromDepth ToDepth TopElev BottomElev MajorLithCode MinorLithCode Description Source LogType OgcFidTemp . 0 14 | 10045588 | GW003048.1.1 | None | UNK | 0.0 | 1.22 | None | None | SOIL | NaN | SOIL SANDY CLAY | UNK | 1 | 8666163 | . 1 15 | 10045588 | GW003048.1.1 | None | UNK | 19.51 | 28.65 | None | None | BSLT | NaN | BASALT WATER BEARING | UNK | 1 | 8666164 | . 2 28 | 10060509 | GW006968.1.1 | None | UNK | 0.0 | 3.66 | None | None | TPSL | NaN | TOPSOIL | UNK | 1 | 8666177 | . 3 29 | 10060509 | GW006968.1.1 | None | UNK | 33.53 | 34.75 | None | None | GRVL | NaN | GRAVEL WATER BEARING | UNK | 1 | 8666178 | . 4 30 | 10060509 | GW006968.1.1 | None | UNK | 48.77 | 51.82 | None | None | SHLE | NaN | SLIPPERY BACK | UNK | 1 | 8666179 | . A MajorLithoCode column appears to be populated, so this may be suitable for training a classifier. I have no idea (or forgot) how these lithology codes have been derived and what the corpus of labels is. . len(litho_logs) . 144518 . For the sake of conciseness I will point-blank reuse the text processing utilities in the ela package, without any explanation of the setup (ela comes with many dependencies from NLP to 3d vis that can be tricky to install). . from ela.textproc import token_freq, plot_freq . MAJOR_CODE=&#39;MajorLithCode&#39; MINOR_CODE=&#39;MinorLithCode&#39; DESC=&#39;Description&#39; . Major (primary) lithology codes . litho_classes=litho_logs[MAJOR_CODE].values df_most_common= token_freq(litho_classes, 50) . plot_freq(df_most_common) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . This is a long-tailed distribution; quite a few labels. The limit to 4 characters labels suggest a form of controlled vocabulary. The numbers 20, 19, 23, 8, 1 look odd. &quot;None&quot; is an artefact though not misleading, quite a few records are such that no major lithology could be attributed. . Minor (secondary) lithology codes . litho_classes=litho_logs[MINOR_CODE].values df_most_common= token_freq(litho_classes, 50) . plot_freq(df_most_common) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . Well, no minor lithology codes, so this data set may not be a good start for multi-label classification. Still, I&#39;ll persist with this data set, and reassess later on . Back to major lithology codes . What are the flavour of descriptions leading to the most frequent classes (CLAY, GRVL, etc.), as well as &quot;None&quot; . is_clay = litho_logs[MAJOR_CODE] == &#39;CLAY&#39; . clay_coded = litho_logs.loc[is_clay][DESC] . import numpy as np np.random.seed(123) clay_coded.sample(n=50) . 144320 CLAY 117387 CLAY 18630 CLAY 35565 CLAY SOME SANDY 61997 CLAY 13533 CLAY SANDSTONE BANDS 117529 YELLOW CLAY 82290 CLAY 73280 CLAY GREY SANDY 106748 CLAY STONEY 26265 CLAY YELLOW 16456 CLAY 25032 CLAY 9911 CLAY 135852 CLAY, SANDY GRAVELLY, ORANGE BROWN, MEDIUM PLA... 30596 CLAY 140698 CLAY, SANDY FAT; RED, DRY-MOIST, MEDIUM CONSIS... 70267 CLAY 6534 CLAY YELLOW 69927 CLAY GRAVEL 76476 SANDY CLAY 7329 CLAY GRITTY 51685 CLAY; 40% BROWN, GRAVEL &amp; SAND 60%, MOST 1-3MM 137385 CLAY 35816 CLAY 73972 CLAY 102333 CLAY SOME STONES 29218 CLAY 92266 RED CLAY 26151 CLAY SANDY 107456 CLAY GREY 52550 CLAY - LIGHT GREY, EXTREMELY SANDY (FINE TO CO... 24225 CLAY 120696 CLAY; LIGHT GREY, TRACE OF WHITE 9030 CLAY 14889 CLAY 25834 CLAY LIGHT BROWN GREY SILTY 44078 CLAY 114869 CLAY 109302 CLAY 38107 CLAY PATCHY 30801 CLAY 82362 CLAY/BROWN ORANGE 141490 CLAY; BROWN, PINK, SOME BLUE 23075 CLAY SANDY GRAVEL 42110 CLAY 50493 CLAY, WHITE SEAM, RED 90209 CLAY - DARK BROWN 104019 CLAY 12690 CLAY Name: Description, dtype: object . Nothing too surprising in this, very intuitive, though the tail suggests there may be a few outliers: . clay_coded.tail(10) . 144386 CLAY; LIGHT BROWN, SOME LIGHT GREY, SILTY 144388 CLAY; BROWN 144391 CLAY; LIGHT BROWN 144393 CLAY - BROWN 144394 CLAY - LIGHT BROWN, EXTREMELY SANDY (COARSE) 144398 CONGLOMERATE - WEATHERED BUT STILL HOLDS TOGET... 144401 CLAY; BROWN 144402 CLAY - GREY 144488 CLAY 144503 None Name: Description, dtype: object . Looking at the &quot;sand&quot; code: . def sample_desc_for_code(major_code, n=50, seed=None): is_code = litho_logs[MAJOR_CODE] == major_code coded = litho_logs.loc[is_code][DESC] if seed is not None: np.random.seed(seed) return coded.sample(n=50) . sample_desc_for_code(&#39;SAND&#39;, seed=123) . 72256 SAND GRAVEL FINE WATER SUPPLY 109345 SAND CLAYEY GRAVEL 66816 SAND WATER SUPPLY 85097 SAND - 70% UP TO 2MM, GRAVEL 10% 2-5MM, CLAY 2... 137476 SAND; LIGHT BROWN, MEDIUM TO COARSE, FINE GRAV... 29946 SAND GRAVEL DIRTY 37012 COARSE SAND AND GRAVEL 135985 SAND 74402 SAND WHITE GREY WELL SORTED WATER SUPPLY 42422 SAND GRAVEL 41225 SAND CLAYEY GRAVEL 50700 SAND 33813 BROWN SAND (VERY FINE TO MEDIUM) 38452 SAND GRAVEL MEDIUM WATER SUPPLY 107707 SAND FINE-COARSE 51759 SAND; 60%, SILTY, FINE 10%, MEDIUM 20% &amp; COARS... 102234 SAND GRAVEL WATER SUPPLY 74640 SAND GRAVEL WATER SUPPLY 13123 SAND GRAVEL WATER SUPPLY 23110 SAND 11373 SAND GRAVEL WATER SUPPLY 35508 SAND SILTY GRAVEL WATER SUPPLY 32744 SAND CLAYEY 67310 SAND CLAY 41813 SAND GRAVEL WATER SUPPLY 106457 SAND GRAVEL WATER SUPPLY 85591 SAND AND GRAVEL - CLAYEY AND SILTY 112156 SAND, WHITE (VERY VERY FINE) 111256 SAND/GRAVEL; FINE TO COARSE, BROWN 55808 SAND; POORLY GRADED, COARSE, SUBROUNDED, LIGHT... 91458 SAND, MEDIUM-COARSE; SLIGHTLY SILTY, WET, LOOS... 61608 SAND, GRAVEL &amp; LIMESTONE 96382 SAND CLAYEY 37119 MEDIUM BROWN SAND &amp; GRAVEL SOME LARGE GRAVEL 118473 SAND; 40% 1/20-1/2MM, 40% 1/2-1MM, 20% 1-3MM, ... 70703 SAND GRAVEL WATER SUPPLY 92920 BROWN SAND 80551 SAND, SILTY; AS ABOVE, TRACE CLAY, ORANGE, LOW... 25175 COARSE SAND &amp; FINE GRAVEL WITH ANGULAR FRAGMEN... 38905 SAND FINE WATER BEARING 44248 SAND GRAVEL WATER SUPPLY 32261 SAND DRIFT 34510 SAND CLAYEY COARSE GRAVEL 77431 SAND AND GRAVEL FINE TO COARSE GREY 138362 SAND BROWN 103758 SAND GRAVEL STONES SOME CLAY BANDS 69238 SAND GRAVEL 143566 SAND &amp; GRAVEL; SAND 50% (FINE 30%, MEDIUM 20%,... 135131 SAND 30361 SAND CLAYEY WELL SORTED Name: Description, dtype: object . Rather straighforward, consistent and and intuitive . Major lithology code &quot;None&quot; . This one is likely to be more curly and surprising. Let&#39;s see . sample_desc_for_code(&#39;None&#39;, seed=123) . 131075 W.B. BROWN SHALE 127170 COARSE SAND AND GRAVEL 126145 BLUE CLAY 126889 GREY AND BROWN CLAY 132179 GREY BROWN CLAY 130033 BROWN SANDY CLAY 123436 GRAVEL 128794 CLAY 128242 SAND AND FINE GRAVEL 134156 None 129636 W.B. BASALT 122884 LIGHT ORANGE SILTY CLAY 124580 SHALES 128684 W.B. SHALE 131120 SAND AND GRAVEL 122690 RED RIDGE CLAY 132319 BLACK SOIL 128528 BLACK CLAY 123646 MUDSTONE 129757 SAND &amp; CLAY 124353 SOFT BROWN CLAY 131921 CLAY 133208 BROWN CLAYS 132945 COARSE SAND 122956 SAND AND GRAVEL 123723 CALY AND GRAVEL 122102 TOP SOIL 126455 HARD SANDY BROWN &amp; GREY CLAY WITH STONES 125637 SANDY BLUE AND GREY CLAY 131154 BROWN SANDY CLAY 129931 RIDGE CLAY 123557 RIDGE CLAY 124964 DARK BROWN CLAY 129391 GREY SHALE 132891 WATER 0.37 LS 132006 BROWN CLAY 134673 SEP 123692 BROWN CLAY 126034 REDDISH BROWN CLAY 129690 SOFT GINGER/BROWN CLAY &amp; STONES 125560 FINE SANDY CLAYS 126238 SHALE &amp; (WATER) 123009 SOIL 124236 SOIL 125814 BLUE SHALE 130474 RED &amp; GREY CLAY 127005 BROWN CLAY 134212 BROWN AND 125640 SOIL 127355 SHALE Name: Description, dtype: object . Well, it does not require a fully trained geologist to think there should be obvious primary lithology codes for many of these, so why is there &quot;None&quot; for these descriptions? . Not shown in the 50-odd sample above are descriptions which should indeed be unclassified (e.g. &quot;no strata description&quot;) . This is also an occasion to note the less obvious information and complications in descriptive logs, compared to earlier categories: . CALY as a typo for CLAY | Slang terms and acronyms, e.g. &quot;W.B.&quot; perhaps for &quot;Weathered, broken&quot; | Quantitative and qualitative attributes such as SAND; 60%, SILTY, FINE 10%, MEDIUM 20% &amp; COARSE, which may be valuable for some types of classification | . sample_desc_for_code(&#39;SDSN&#39;, seed=123) . 9423 SANDSTONE SOFT 117911 SANDSTONE, SILTY; AS ABOVE, QUARTZ (+60%) 6613 SAND ROCK GREY 20345 SANDSTONE GREY HARD 106885 SANDSTONE 139088 SANDSTONE; DARK GREY, COARSE GRAINED, HARD 5101 SANDSTONE ROTTEN 26561 SANDSTONE YELLOW 137304 SANDSTONE; COARSE 56627 SANDSTONE; GREY, FINE-MEDIUM GRAINED, MOD WEAK... 98974 SANDSTONE WHITE WATER SUPPLY 87187 SANDSTONE 45141 SANDSTONE, VERY PALE BROWN, HIGH CLAY 70015 SANDSTONE YELLOW 29492 SANDSTONE WATER SUPPLY 92918 SANDSTONE 83555 SANDSTONE; LIGHT GREY, VERY FINE, STRONG 136401 SANDSTONE 48201 SANDSTONE 34060 SANDSTONE, YELLOW, MEDIUM GRAINED, HIGH CLAY M... 63752 SANDSTONE; WHITISH GREY, VERY FINE, ANGULAR, C... 51878 SANDSTONE, MEDIUM, WHITISH YELLOW 59346 SANDSTONE, SOFT 62968 SANDSTONE; OFF-WHITE GREY, VERY FINE, STRONG 140930 SANDSTONE, LIGHT GREY, SOME FRACTURES 6394 SAND ROCK 113546 SANDSTONE - HARD - LIGHT GREY 49539 SANDSTONE; RED GREY, FINE-MEDIUM GRAINED, FRIA... 60198 SANDSTONE; LIGHT GREY, FINE GRAINED 1366 SANDSTONE 35923 SANDSTONE WEATHERED COARSE WATER SUPPLY 141556 SANDSTONE, WHITE, IMPREGNATED WITH WATER WASHE... 10100 SANDSTONE WATER SUPPLY 89138 SANDSTONE; OFF-WHITE GREY, MEDIUM-COARSE GRAIN... 86046 QUARTZ SANDSTONE; MEDIUM GRAINED 76366 SANDSTONE, PALE BROWN, FINE-COARSE, LOW CLAY, ... 46179 SANDSTONE, LIGHTYELLOW/RED BROWN MOTTLED, FINE... 8613 SANDSTONE 4537 SANDSTONE YELLOW 60506 SANDSTONE 8917 SANDSTONE 1372 SANDSTONE WATER SUPPLY 82975 SANDSTONE; LIGHT GREY, FINE GRAINED, MOD STRON... 60166 SANDSTONE, VERY SOFT 31674 SANDSTONE SHALE 8397 SANDSTONE 113310 SANDSTONE; OFF-WHITE, FINE GRAINED, MOD STRONG 27980 SANDSTONE 79933 SANDSTONE WHITE BROKEN 33654 SANDSTONE YELLOW CLAYEY Name: Description, dtype: object . Now, what&#39;s with the weird numbers as lithology codes? . sample_desc_for_code(&#39;20&#39;, seed=123) . 130369 SANDY CLAY; BROWN (COARSE) 122444 CLAY SANDY 127761 CLAY SANDY 125771 SANDY CLAY 127783 CLAY SANDY 133014 SANDY CLAY AND CLAYEY SAND AND GRAVEL 124007 BROWN &amp; GREY SANDY CLAY 123445 CLAY LT BROWN GREY SANDY, MED-COARSE BROWN BANDS 122180 CLAY SANDY 127440 SANDY CLAY 131498 SANDY CLAY 127935 CLAY RED SANDY 129304 SANDY CLAY 130614 SANDY SHALE 125543 SANDY CLAY 125149 SANDY CLAY 124971 SANDY CLAY/GREY BROWN 123939 SANDY CLAY, BROWN 122392 CLAY SANDY 123999 BROWN &amp; GREY SANDY CLAY 129203 SANDY LOAM 127933 CLAY RED SANDY 134474 SANDY CLAYEY MED. BROWN 126071 SANDY CLAY 127927 CLAY SANDY 123587 CLAY SANDY 126559 SANDY CLAY 127928 CLAY GREY SANDY 124643 SANDY CALY/BROWN 122555 CLAY SANDY 132628 SANDY CLAY 129853 SANDY CLAY 130187 SANDY CLAY 129057 SANDY CLAY 133300 SANDY CLAY 122535 CLAY SANDY 122732 CLAY SANDY 127977 CLAY YELLOW SANDY 126568 SANDY CLAY 122339 CLAY SANDY 128386 SANDY CLAY, BROWN 128198 SANDY CLAY, RIDGE 125530 SANDY CLAY 126798 SANDY CLAY 122452 CLAY SANDY 123237 SANDY BROWN CLAYS 126510 SANDY CLAY; 40% LIGHT GREY, EXTREMELY SANDY (C... 132331 SANDY GRAVEL 131807 SANDY CLAY 132895 SANDY CLAY Name: Description, dtype: object . Interesting. There is a clear pattern. I know from my prior exposure that &quot;Clayey sands&quot; and &quot;sandy clays&quot; are not that uncommon (and gradations of mixes of sand and clay matter a great deal to estimate hydraulic conductivity). . Next . This was the initial EDA. Next I&#39;ll probably train a classifier on the major lithology code (or a subset thereof). I am keen to explore multi-label classification, but will have to decide whether to populate the secondary lithology code using regexp classification, or switch to a fully labelled dataset at some point. . This first post illustrated the need to have a look at data. This data was already collated and curated, and I have no doubt many people went through a lot of work to get there. But this may not be a fully labelled dataset amenable to be used for training a classifier. At least, not without further data preparation. .",
            "url": "https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html",
            "relUrl": "/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Victorian Water Measurement Information System",
            "content": "About . Trying to retrieve time series or streamflow measurements via a REST API. Or anything that works for that matter. . This builds on work done by my colleague Andrew Freebairn, which really gave me a leg up. . Web API: which one? . I am not a regular user of online data. Whenever I really needed to fetch data from an online service, this tended to be an experience where reverting back to using a Web GUI with a &quot;download data&quot; button the path of least pain. . Before writing this section I got confused, trying to use kiwis-pie to retrieve data from the VIC data web site. Which is not suitable. I thought all Kisters products would be accessible using a unified web API. But Hydstra has its own separate from WIKSI, so far as I understand. See https://kisters.com.au/webservices.html . Curiously what seems to be the documentation for the Json API to Hydstra is in a page named HYDLLP – Flat DLL Interface to Hydstra. The examples are given in Perl, which is, well, not all that mainstream anymore. . There are visibly data codes to identify the types of data, at least to retrieve time series data. Query parameters such as varfrom and varto are kind of odd in an API though, and would deserve an explanation. It is only after manually downloading a time series from the VIC water data GUI that there is an explanation note about these variable codes. . The VIC data web site offers a way to directly link to URLs for particular data, but this seems not to work when used: https://data.water.vic.gov.au?ppbm=405288|B_405_GOULBURN|WEB_SW&amp;rs&amp;1&amp;rsdf_org . rdmw.qld.gov.au providing documentation for how to build json queries. . import requests import json import urllib import pandas as pd from datetime import date from IPython.core.display import HTML from IPython.display import JSON . def vic_url_builder(json_p, query): endpoint = &#39;http://data.water.vic.gov.au/cgi/webservice.exe&#39; return f&quot;{endpoint}?{json.dumps(json_p)}&amp;{urllib.parse.urlencode(query)}&quot; . The site identifiers of interest are: . site_id_405284 = &#39;405284&#39; # SUNDAY CREEK @ MT DISAPPOINTMENT site_id_405288 = &#39;405288&#39; # SUNDAY CREEK @ MT DISAPPOINTMENT (US SUNDAY CK. STORAGE site_id_405287 = &#39;405287&#39; # WESTCOTT CREEK @ MT DISAPPOINTMENT US SUNDAY CK STORAGE site_id_405239 = &#39;405239&#39; # SUNDAY CREEK @ CLONBINANE . # This took me a bit of time to figure out how to extract cumecs. One has to figure out that the &#39;varfrom&#39; in the query parameter needs to bhe the river level in metres. var_code_level_metres = &#39;100&#39; # var_code_cumecs = &#39;140&#39; # cumecs var_code_mlpd = &#39;141&#39; # ML/Day ver = { &#39;ver&#39;: &#39;2.0&#39; } #station_list = &#39;405288&#39; # don&#39;t set as 1-element list though: fails... station_list = &#39;,&#39;.join([ site_id_405284, site_id_405288, site_id_405287, site_id_405239, ]) def dt_str(x): return x.strftime(&#39;%Y%m%d%H%M%S&#39;) get_ts = { &#39;function&#39;:&#39;get_ts_traces&#39;, &#39;version&#39;:&#39;2&#39;, &#39;params&#39;:{ &#39;site_list&#39;:station_list, &#39;datasource&#39;:&#39;A&#39;, # THere are other codes, but this one is the only that returned something so far I can see. &#39;varfrom&#39;:var_code_level_metres, &#39;varto&#39;:var_code_cumecs, &#39;start_time&#39;:&#39;20220101000000&#39;, &#39;end_time&#39;:&#39;20220529000000&#39;, &#39;interval&#39;:&#39;day&#39;, &#39;multiplier&#39;:&#39;1&#39;, &#39;data_type&#39;:&#39;mean&#39;, } } query = vic_url_builder(get_ts, ver) . response = requests.get(query) . if response.status_code == requests.codes.ok: print(&#39;Return Json&#39;) print(response.headers[&#39;content-type&#39;]) response_json = response.json() # for visualisation in a notebook (not sure about rendered in a blog?) JSON(response_json) . Return Json text/html . &lt;IPython.core.display.JSON object&gt; . type(response_json) . dict . traces = response_json[&#39;return&#39;][&#39;traces&#39;] len(traces) . 2 . We got only trace time series for 2 of the 4 sites. This is because these sites have no recent data, at least not for the preiod of interest. Fair enough. For instance 405284 SUNDAY CREEK @ MT DISAPPOINTMENT functioned from from 23/06/1981 to 05/07/1984 . traces[0].keys() . dict_keys([&#39;error_num&#39;, &#39;compressed&#39;, &#39;site_details&#39;, &#39;quality_codes&#39;, &#39;trace&#39;, &#39;varfrom_details&#39;, &#39;site&#39;, &#39;varto_details&#39;]) . traces[0][&#39;site&#39;] . &#39;405288&#39; . tr = traces[0] tr[&#39;quality_codes&#39;] . {&#39;255&#39;: &#39;&#39;, &#39;100&#39;: &#39;Irregular data, use with caution.&#39;, &#39;2&#39;: &#39;Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction&#39;} . traces[1][&#39;quality_codes&#39;] . {&#39;255&#39;: &#39;&#39;, &#39;150&#39;: &#39;Rating extrapolated above 1.5x maximum flow gauged.&#39;, &#39;149&#39;: &#39;Rating extrapolated 1.5 times the maximum flow gauged.&#39;, &#39;2&#39;: &#39;Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction&#39;} . So this has good quality data, and some missing data (255). I am a bit puzzled by the codes for ratings beyond the max flow gauge, but the value appears to be sometimes zero flows in the data. Odd. . After a bit of iterative discovery, a way to turn json data to pandas series: . import numpy as np def to_dt64(x): return pd.Timestamp(x).to_datetime64() def make_daily_time_series(tr): site_name = tr[&#39;site&#39;] y = pd.DataFrame(tr[&#39;trace&#39;]) y.v = pd.to_numeric(y.v) v = y.v.values q = y.q.values y.t = y.t.apply(str) v[q == 255] = np.nan index = y.t.apply(to_dt64).values ts = pd.Series(v, index=index, name=site_name) return ts . ts = make_daily_time_series(traces[0]) ts.plot(ylabel=&quot;m3/s&quot;, xlabel=&quot;Time&quot;, title = ts.name) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;405288&#39;}, xlabel=&#39;Time&#39;, ylabel=&#39;m3/s&#39;&gt; . ts = make_daily_time_series(traces[1]) ts.plot(ylabel=&quot;m3/s&quot;, xlabel=&quot;Time&quot;, title = ts.name) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;405287&#39;}, xlabel=&#39;Time&#39;, ylabel=&#39;m3/s&#39;&gt; . Related resources . Andrew Freebairn&#39;s python package &#39;bomwater&#39; .",
            "url": "https://jmp75.github.io/work-blog/jupyter/hydstra/2022/05/28/hydstra-rest-data.html",
            "relUrl": "/jupyter/hydstra/2022/05/28/hydstra-rest-data.html",
            "date": " • May 28, 2022"
        }
        
    
  

  
  
      ,"page0": {
          "title": "Setting up a private conda channel - part 1",
          "content": "Background . I have a conda package for compiled native libraries. While it may be in a position to be submitted to conda-forge, I will also be interested at some point in setting up “my” own conda channel, with a view to distribute packages on my enterprise intranet. . This post will be a review of the possible ways to do this. . Resources . conda documentation . The main conda documentation has sections on managing channels and creating custom channels. . fast.ai . Jeremy Howard has written the post “fastchan, a new conda mini-distribution”. It is a good read giving the rationale for setting up this channel. The repo of the conda channel ‘fastconda’ has some elements of pipelines but may be a good starting point for channels hosted by Anaconda. I am not sure I can repurpose this for a private channel. . Reusing conda-forge? . Since I intuit that a process similar to that of conda-forge could be what is emulated internally, an option may be to upfront borrow from the conda-forge documentation . The workhorse of conda-forge appears to be using conda-smithy to manage your CI, though that section actually rather confused me at the first read. The github readme of conda-smithy may be a better starting point. . https://github.com/conda-forge/astra-toolbox-feedstock is a recently accepted recipe. Likely a good template to study for the swift stack. . Commercial offerings . At an enterprise level it may be better to use Anaconda Server rather than cook up something. I probably cannot afford the time to trial standing one up, but my IM&amp;T business unit may look at it. In particular, if there are use cases for sophisticated user based access controls, “free” solutions may not be up to scratch nor to scale. | Anaconda cloud thingy | Channel on anaconda.org | . Other . How To: Set up a local Conda channel for installing the ArcGIS Python API | Building a Private Conda Channel | Setting up a feedstock from scratch | The github repo Private Conda Repository may be a good first step. | . By the way one misleading thing if you google “How do I set up a conda channel?”: the erronously titled video How to create new channel in Anaconda (python) is actually demonstrating the creation of a conda environment. Skip. . Stocktake . The first impression is that there is not a clear, single path to standing up your own conda channel over HTTP(S), no turnkey solution. The closest may be the github repo Private Conda Repository. . Plan . First, set up a file based channel following the conda doc for creating custom channels. Second, try to use PCR: Private Conda Repository. . The rest of this post will be whatever is done before dinner… . Setting up a file based local channel . So, trying to apply creating custom channels to the moirai library. The document is hardly a tutorial since it pops references to package-1.0-0.tar.bz2 out of nowhere. Unless you are already familiar enough with conda-build, this leaves the reader puzzled. Anyway. . conda env list conda activate cf # as per a previous post which conda-build . Building conda packages from scratch . mkdir -p ~/src/tmp/moirai/recipe cd ~/src/tmp/moirai/recipe # git@github.com:jmp75/staged-recipes.git cp ~/src/staged-recipes/recipes/moirai/* ./ ls . returns bld.bat build.sh meta.yaml . conda build . results in PermissionError: [Errno 13] Permission denied: &#39;/home/conda&#39;. This is rather poor form for a default behavior. . mkdir -p ~/src/tmp/moirai/pkgtarball/ # cd ~/src/tmp/moirai/recipe fails ??? First tried cd ~/src/tmp/moirai/ conda build --build-only --output-folder ~/src/tmp/moirai/pkgtarball . . conda.CondaError: Unable to create prefix directory &#39;/home/conda/staged-recipes/build_artifacts/moirai_1655544194767/_h_env_placehold_LOTSOFTHESEPLACEHOLD_placehold_placehold_placehold_plac&#39;. Check that you have sufficient permissions. . Seriously? Leave my /home directory alone. . Looking at this issue (not quite the same context), I am trying just in case: . mkdir -p ~/src/tmp/moirai/pkgtarball/ conda build --build-only --output-folder ~/src/tmp/moirai/pkgtarball --croot ~/src/tmp/moirai/build . . My hopes were not very high, but thankfully this seem to complete: . Total time: 0:01:57.9 CPU usage: sys=0:00:00.2, user=0:00:13.2 Maximum memory usage observed: 675.3M Total disk usage observed (not including envs): 5.2K . cd ../pkgtarball/ ; ls shows: . channeldata.json icons index.html linux-64 noarch . but ls linux-64 returns current_repodata.json current_repodata.json.bz2 index.html repodata_from_packages.json repodata_from_packages.json.bz2 repodata.json repodata.json.bz2. Where is the expected file moirai-1.1-h27087fc_0.tar.bz2? . try abain, marginally different option (I don’t get how the upload would occur anyway) . mkdir -p ~/src/tmp/moirai/pkgtarball/ cd ~/src/tmp/moirai # NOTE: not recipe. May make a difference. Confused by prior attempt conda build --no-anaconda-upload --output-folder ~/src/tmp/moirai/pkgtarball --croot ~/src/tmp/moirai/build . &gt; log.txt 2&gt;&amp;1 . This time it looks like there is something out. Maybe I misunderstood the effect of --build-only… . cd ~/src/tmp/moirai/pkgtarball ls linux-64/ # current_repodata.json index.html repodata_from_packages.json repodata.json # current_repodata.json.bz2 moirai-1.1-h3fd9d12_0.tar.bz2 repodata_from_packages.json.bz2 repodata.json.bz2 . cd ~/src/tmp/moirai/pkgtarball other_arch=&quot;osx-64 linux-32 win-32 win-64 all&quot; for f in $other_arch ; do mkdir -p $f; done cd ~/src/tmp/moirai/pkgtarball f=all mkdir -p ./outputdir/ conda convert --platform $f ./linux-64/moirai-1.1-h3fd9d12_0.tar.bz2 -o ./outputdir/ . WARNING: Package moirai-1.1-h3fd9d12_0.tar.bz2 contains C extensions; skipping conversion. Use -f to force conversion. . Well, at least this is correct. So how to I build for a different architecture? the section “Converting a package for use on all platforms” is implicitely only for python-only code. . Wrapping up .",
          "url": "https://jmp75.github.io/work-blog/drafts/2022-06-XX-conda-channels-local-channel.html",
          "relUrl": "/drafts/2022-06-XX-conda-channels-local-channel.html",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "About Me",
          "content": "My profile page at the CSIRO | Github profile | . I live on Ngunnawal country . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmp75.github.io/work-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmp75.github.io/work-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}