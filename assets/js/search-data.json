{
  
    
        "post0": {
            "title": "Compiling C++ libraries to WebAssembly - Part 1",
            "content": ". Introduction . Several previous posts are refering to a software stack for streamflow forecasting. The core of it is C++, and this is one of the primary target languages for WebAssembly - WASM to define a minimal viable product. I’ve been meaning for a while to acquaint myself a bit more with WebAssembly. While our forecasting software may be a big step to start with, it has the merit of being “real”. This post will be an initial foray in targetting WASM: let’s try and see. . Waklthrough . Googling for starting examples, I somewhat arbitrarily ended up watching a bit of video 21 Compiling C/C++ to WebAssembly - Introduction to Google Colab for Research. That got me onto the emscripten setup instructions. Later on, I realised it may not be the only path, but I’ll get back to that. . On Linux, the following worked just fine. There are Debian packages for emscripten, but the emscripten setup instructions recommends the following. . git clone --depth=1 https://github.com/emscripten-core/emsdk.git cd emsdk/ ./emsdk install latest # takes a minute or so ./emsdk activate latest source ./emsdk_env.sh em++ --help # good, finds it . Going throught the Emscripten Tutorial, just the “hello world” part of it. . Trying on one of my c++ codebase . The library MOIRAI: Manage C++ Objects’s lifetime when exposed through a C API may be a natural starting point, as it is a workhorse for the C API of many larger libraries. It is small in size, but non-trivial, and the unit tests already feature techniques like C API with opaque pointers and template programming. It can be compiled using cmake. . Adapting emscripten instructions for projects to my case (where emcmake $CM below is emcmake cmake -DBLAH_LOTS_OF_OPTIONS .., we’ll get back to this) . cd ${GITHUB_REPOS}/moirai mkdir -p embuild ; cd embuild emcmake $CM . I get: . CMake Error at /home/per202/src/emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake:136 (message): System LLVM compiler cannot be used to build with Emscripten! Check Emscripten&#39;s LLVM toolchain location in .emscripten configuration file, and make sure to point CMAKE_C_COMPILER to where emcc is located. (was pointing to &quot;gcc&quot;) . Googling for “System LLVM compiler cannot be used to build with Emscripten” was rather confusing me. My full expanded, failing command line looks like: . emcmake cmake -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON .. . Maybe removing most arguments? . emcmake cmake -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON .. . Nope, still not, same message. Well, how about trying to “point CMAKE_C_COMPILER to where emcc is located”? . emcmake cmake -DCMAKE_CXX_COMPILER=/home/per202/src/emsdk/upstream/emscripten/em++ -DCMAKE_C_COMPILER=/home/per202/src/emsdk/upstream/emscripten/emcc -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON .. . Surprise, it works. Albeit with a warning. . CMake Warning (dev) at CMakeLists.txt:145 (ADD_LIBRARY): ADD_LIBRARY called with SHARED option but the target platform does not support dynamic linking. Building a STATIC library instead. This may lead to problems. . Bring it on. Battle scarred. Let’s plodder and see. . emmake make . results in: . Scanning dependencies of target moirai [ 6%] Building CXX object CMakeFiles/moirai.dir/src/reference_handle.cpp.o [ 12%] Building CXX object CMakeFiles/moirai.dir/src/reference_handle_map_export.cpp.o [ 18%] Linking CXX static library libmoirai.a [ 18%] Built target moirai # etc. etc. Scanning dependencies of target moirai_test_api [ 81%] Building CXX object CMakeFiles/moirai_test_api.dir/tests/moirai_test_api/main.cpp.o [ 87%] Linking CXX executable moirai_test_api.js wasm-ld: error: duplicate symbol: free_string &gt;&gt;&gt; defined in libmoirai_test_lib_a.a(c_interop_api.cpp.o) &gt;&gt;&gt; defined in libmoirai_test_lib.a(c_interop_api.cpp.o) . Ok, not bad, really, compiling all the object files succeeds already. It is at linking stage that this falters, and given the above warning about shared/static libraries, fair enough. . Taking stock . Our existing software stack is architectured for good reasons as a set of shared libraries (shared objects on Linux, “DLLs” on Windows). If emscripten does not support this, a significant reengineering may be on the card in order to target WASM. . But, really, shared libraries not supported? Google a bit. . I come across the WebAssembly System Interface - WASI. Its documentation page Writing WebAssembly - C/C++ and LLVM - WebAssembly lld port make me think that shared libraries. Noting that flags related to dynamic linking, such -shared and --export-dynamic are not yet stable and are expected to change behavior in the future. . Other resources for next steps . Very interested in Porting libffi to pure WebAssembly, and also the company profile of the author. | minimal-cmake-emscripten-project | Compiling an Existing C Module to WebAssembly | The Rust community seems to be quite actively interested in WASM | Bytecode Alliance | .",
            "url": "https://jmp75.github.io/work-blog/c++/wasm/webassembly/emscripten/wasi/2022/09/11/cpp-code-wasm.html",
            "relUrl": "/c++/wasm/webassembly/emscripten/wasi/2022/09/11/cpp-code-wasm.html",
            "date": " • Sep 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Generate programming language bindings to a C API",
            "content": ". Introduction . The codebase Uchronia - time series handling for ensembles simulations and forecasts in C++ comprises a C++ core, and a C Application Programming Interface with Python, R, Matlab (and other) bindings. This is part of a software stack for streamflow forecasting. Some elements of the software stack similar to Uchronia have large C APIs. Even with a very stable C API (and they often were not at some point), it is mind-numbing, inefficient and bug-prone to generate language bindings (glue code) manually. . What I am writing about in this post is utility code for interoperability code generation that grew out of scientific software, and perhaps should have advertised in its own right. . History . In 2014 after settling for C++ for a modelling core. I had R, Matlab and Python users, some need to interop with C# code. And some constraints on using different C++ compilers in the stack. So I decided on designing C APIs, the lingua franca of in-process interop. I had some prior exposure to the Mono C core and interop with R. But not extensive first hand. . My first port of call for glue code was a no-brainer: SWIG, which I first encountered in an internship in… 1997. I did not document my tribulations in details, but I encountered many difficulties when testing feasibility. Basically, I concluded that SWIG was just not a good fit for a C API like mine, especially one with opaque pointers (void*). . There were other offering for code generation for R, or Python. But apart from SWIG, I do not recall locating any suitable glue code generation for multiple target languages. I may revisit in more details the tribulations in another post, but this was the reason for the inception of c-api-wrapper-generation. . Glue code generation - context and review . Foreword - Google search . Before starting this post, I wanted to check out what was the state of play out there and I was googling using the terms “generate bindings for a C API”. Our c-api-wrapper-generation was coming up in the first few items. I thought Google was biasing the search given the wealth of information it has, I mean, come on, there must be thousands of similar endeavours out there. No way. . But, after some friends help and simulating searches from another geolocation, it seems Google is not cheating. . Not sure what to make of this yet. Oh, and searching “generating binding to a c api” instead of “generating bindings to a c api” wields a different list… . Anyway, a quick review before looking at c-api-wrapper-generation. . Brief scan of the state of the art . There is an excellent post Automatic Language Bindings by Andrew Weissflog, which I recommend. Many statements and observations resonate with me. Related directly or indirectly to the post above (I intuit) I found C2CS. I am very intrigued by their use of an abstract syntax tree (AST), I think. My approach was more crude (but perhaps pragmatic). . From the Rust community this thread and rust-bindgen . A wealth of resources in BindingCodeToLua . pybind11 is something I already looked at 3 years ago in this post. I probably would have settled on it if it was purely about Python bindings, directly to the C++ code. . Sample usage . Let’s start with a sample usage. This is based on code that has evolved over time rather than something purely didactic, but should give an idea of the process. I may update this post if I reshape things to be more didactic. . C API . The codebase uchronia has a C API with functions such as: . DATATYPES_API char** GetEnsembleDatasetDataIdentifiers(ENSEMBLE_DATA_SET_PTR dataLibrary, int* size); DATATYPES_API ENSEMBLE_FORECAST_TIME_SERIES_PTR CreateEnsembleForecastTimeSeries(date_time_to_second start, int length, const char* timeStepName); DATATYPES_API void GetTimeSeriesValues(TIME_SERIES_PTR timeSeries, double * values, int arrayLength); . The low-level Python binding generated from parsing this C API result in the following type of code. Note that opaque pointers such as ENSEMBLE_DATA_SET_PTR have corresponding higher level Python class equivalents such as TimeSeriesLibrary, even in the “low-level” glue code. . def GetEnsembleDatasetDataIdentifiers_py(dataLibrary:&#39;TimeSeriesLibrary&#39;): dataLibrary_xptr = wrap_as_pointer_handle(dataLibrary) size = marshal.new_int_scalar_ptr() values = uchronia_so.GetEnsembleDatasetDataIdentifiers(dataLibrary_xptr.ptr, size) result = charp_array_to_py(values, size[0], True) return result def CreateEnsembleForecastTimeSeries_py(start:datetime, length:int, timeStepName:str) -&gt; &#39;EnsembleForecastTimeSeries&#39;: # glue code def GetTimeSeriesValues_py(timeSeries:&#39;TimeSeries&#39;, values:np.ndarray, arrayLength:int) -&gt; None: # glue code . For information, the equivalent generated glue code for an R package uchronia are in files src/rcpp_generated.cpp and R/uchronia-pkg-wrap-generated.r. . Outline generating the python glue code . See the wrapper generator c-api-wrapper-generation. The README.md has setup instructions for dotnet, if need be. . There is a history behind why the conversion engine runs on .NET (and is written in C#), but I will not dwell on it in this post. Note that as of August 2022, even on Linux packages from the .NET ecosystem are readily available from Microsoft or even Ubuntu official package repositories. . If you have dotnet installed dotnet --info returns as of August 2022: . .NET SDK (reflecting any global.json): Version: 6.0.302 Commit: c857713418 Runtime Environment: OS Name: debian OS Version: 11 OS Platform: Linux RID: debian.11-x64 Base Path: /usr/share/dotnet/sdk/6.0.302/ global.json file: Not found Host: Version: 6.0.7 Architecture: x64 Commit: 0ec02c8c96 .NET SDKs installed: 6.0.302 [/usr/share/dotnet/sdk] .NET runtimes installed: Microsoft.AspNetCore.App 6.0.7 [/usr/share/dotnet/shared/Microsoft.AspNetCore.App] Microsoft.NETCore.App 3.1.27 [/usr/share/dotnet/shared/Microsoft.NETCore.App] Microsoft.NETCore.App 6.0.7 [/usr/share/dotnet/shared/Microsoft.NETCore.App] . The codegen engine is C#, and we’ll give an overview later. Compiling it is done like so: . cd ~/src/c-api-wrapper-generation cd engine/ApiWrapperGenerator dotnet restore ApiWrapperGenerator.sln dotnet build --configuration Debug --no-restore ApiWrapperGenerator.sln . This will create the library c-api-wrapper-generation/engine/ApiWrapperGenerator/bin/Debug/netstandard2.0/ApiWrapperGenerator.dll, which contains the glue code generation engine. More details about it later, but for now we will walk through the usage first. . One handy interactive option to “script” binding generation for a library is the F# language. A script to generate the python bindings for uchronia is as follow. Note that most of the F# scripting code below is also (re-)used for other C APIs than uchronia’s . Note: the following code is not yet publicly available. It should be in a revised post . open System.Collections.Generic #r &quot;/home/per202/src/c-api-wrapper-generation/engine/ApiWrapperGenerator/bin/Debug/netstandard2.0/ApiWrapperGenerator.dll&quot; // #r @&quot;C: src github_jm c-api-wrapper-generation engine ApiWrapperGenerator bin Debug netstandard2.0 ApiWrapperGenerator.dll&quot; open ApiWrapperGenerator #load &quot;gen_py_common.fsx&quot; open Gen_py_common . gen_py_common.fsx . gen_py_common.fsx is a way to reuse boilerplate to generate code across several codebases. Typically, it includes a templated header with mostly similar functions with a few variations across use cases. . let prependHeaderTemplate = &quot;################## # # *** THIS FILE IS GENERATED **** # DO NOT MODIFY IT MANUALLY, AS YOU ARE VERY LIKELY TO LOSE WORK # ################## # Some module imports such as: from datetime import datetime from refcount.interop import CffiData, OwningCffiNativeHandle, DeletableCffiNativeHandle, wrap_as_pointer_handle from cinterop.cffi.marshal import as_bytes, TimeSeriesGeometryNative # imports with string templating, for your specific package from {0}.wrap.ffi_interop import marshal, {1}, check_exceptions # Additional specific imports for this package {3} # Some low-level conversion functions that will be called in the generated code: def char_array_to_py(values:CffiData, dispose:bool=True) -&gt; str: pystring = marshal.c_string_as_py_string(values) if dispose: {1}.DeleteAnsiString(values) return pystring ## etc. etc. &quot; . {0} will be replace with your python module name, {1} with the cffi wrapper interface to invoke the native library, e.g. mylibrary_so. This templated header is then used by a function createPyWrapGen that creates and configures the code generator: . let createPyWrapGen (pkgName:string, cffiObjName:string, wrapperClassNames:string, otherImports:string, nativeDisposeFunction:string, typeMap:Dictionary&lt;string, string&gt;) : PythonCffiWrapperGenerator = let pyCffiGen = PythonCffiWrapperGenerator() . The body of createPyWrapGen consists mostly of definitions mapping types across languages, C and Python in this case, so that the generator pyCffiGen knows how to convert from Python to C before calling a native C function, or how to convert or wrap a function returned from a C API call. There is provision for custom templates for “peculiar” functions. . pyCffiGen.SetTypeMap(&quot;char*&quot;, &quot;str&quot;) pyCffiGen.SetReturnedValueConversion(&quot;char*&quot;, &quot;char_array_to_py(C_ARGNAME, dispose=True)&quot;) pyCffiGen.SetTransientArgConversion( &quot;char*&quot; , &quot;_charp&quot;, &quot;C_ARGNAME = wrap_as_pointer_handle(as_bytes(RCPP_ARGNAME))&quot;, &quot;# no cleanup for char*?&quot;) let returnscharptrptr = pyCffiGen.ReturnsCharPtrPtrWrapper() pyCffiGen.AddCustomWrapper(returnscharptrptr) . Back to the main script . We can then call this pyCffiGen function from the main script customised for the uchronia package: . let pkgName = &quot;uchronia&quot; // what goes into {0} let cffiObjName = &quot;uchronia_so&quot; // higher level package classes can be injected into the generated lower level binding, to make them more intelligible for user. let wrapperClassNames=&quot; from uchronia.classes import ( EnsembleTimeSeries, TimeSeriesLibrary, EnsembleForecastTimeSeries, TimeSeries, EnsemblePtrTimeSeries, TimeSeriesProvider, ) &quot; let typeMap = Dictionary&lt;string, string&gt;() typeMap.Add(&quot;DATATYPES_ENSEMBLE_TIME_SERIES_DOUBLE_PTR&quot;, &quot;&#39;EnsembleTimeSeries&#39;&quot;) typeMap.Add(&quot;ENSEMBLE_DATA_SET_PTR&quot;, &quot;&#39;TimeSeriesLibrary&#39;&quot;) typeMap.Add(&quot;ENSEMBLE_FORECAST_TIME_SERIES_PTR&quot;, &quot;&#39;EnsembleForecastTimeSeries&#39;&quot;) typeMap.Add(&quot;TIME_SERIES_PTR&quot;, &quot;&#39;TimeSeries&#39;&quot;) // etc. etc. let otherImports = &quot;&quot; let nativeDisposeFunction = &quot;DisposeSharedPointer_py&quot; let pyCffiGen = createPyWrapGen (pkgName, cffiObjName, wrapperClassNames, otherImports, nativeDisposeFunction, typeMap) // Specify a marker that identifiers a function as being part of the C API let exportModifierPattern = [|&quot;DATATYPES_API&quot;|] apiFilter.ContainsAny &lt;- exportModifierPattern // In this instance, this marker is not part of the C ANSI 99 function, so this is one of the strings to strip out of lines. apiFilter.ToRemove &lt;- exportModifierPattern let gen = WrapperGenerator(pyCffiGen, apiFilter) // Path to input C API file let root = srcDir +/ &quot;datatypes&quot; let apiHeaderFile = root +/ &quot;datatypes&quot; +/ &quot;include&quot; +/ &quot;datatypes&quot; +/ &quot;extern_c_api.h&quot; let outfile = root +/ &quot;bindings&quot; +/ &quot;python&quot; +/ &quot;uchronia&quot; +/ &quot;uchronia&quot; +/ &quot;wrap&quot; +/ &quot;uchronia_wrap_generated.py&quot; gen.CreateWrapperHeader(apiHeaderFile, outfile) . This may seem like a lot of work to generate relatively little code, but trust me this is much preferable for one’s sanity compared to a “bovine”, manual glue code writing and maintenance over the long term. . Architecture . Below is a simplified class diagram of the code generation engine ApiWrapperGenerator. While fairly complicated this is a “low-brow” approach relying on string manipulation. There are probably smarter ways to do this, some perhaps not readily available back in 2014. But it works. . . Conclusion . This code generation tool has emerged from a specific context, after failing to apply SWIG to handle a C API with opaque pointers. There are brillant code gen tools other than SWIG out there, but all those I have come across are centered around generating in a single high-level language. . I have seen use cases in my organisation suggesting there is a potential for reusing these codegen tools. I wonder what communities and forums should be engaged to test this hypothesis and grow it. .",
            "url": "https://jmp75.github.io/work-blog/code%20generation/interop/c++/python/2022/09/03/c-api-wrapper-generation.html",
            "relUrl": "/code%20generation/interop/c++/python/2022/09/03/c-api-wrapper-generation.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Presenting doxygen C++ API documentation via MkDocs with doxybook2",
            "content": "Background . The codebase Uchronia - time series handling for ensembles simulations and forecasts in C++ comprises a C++ core with Python and R bindings. The Python package documentation is using MkDocs to generate its API documentation, in line with its dependencies c-interop and Python refcount. . The C++ core is (to some extent) documented in the source code with formatted comments, and are extracted with Doxygen, a long established de-facto standard. . While the Python, R and C++ parts of the code base usually address different types of users, I was wondering whether it is possible to host at least the Python and C++ API documentation jointly in a MkDocs based site. While it is somewhat subjective, one purpose is to host the documentation via a pleasant Web based interface such as this. . Resources . Scouting for bridges between Doxygen and MkDocs, I found a few resources. The most active and adopted one appears to be doxybook2. This appears worth a try on the Uchronia code base. . Summary walkthrough . While I could host the documentation on github pages via the existing Uchronia codebase, to avoid growing its scope let us create a dedicated project uchronia-ts-doc. . Installing software . Doxygen is mainstream and on a Debian linux available with sudo apt install doxygen . doxybook2 . doxybook2 has precompiled Windows binaries. For Linux, installing from the source code appears the way to go. . git clone --depth=1 https://github.com/matusnovak/doxybook2.git cd doxybook2/ . The instructions are to use Microsoft vcpkg to manage dependencies, which worried me at first. Usually I would assume dependencies would be available via apt, but the list catch2 fmt dirent fmt nlohmann-json inja cxxopts spdlog makes me think there would be some not readily available. But, vcpkg appears to be multi-platform these days; I had not looked closely for years. Let’s give it a try. . cd $HOME/src git clone https://github.com/microsoft/vcpkg ./vcpkg/bootstrap-vcpkg.sh . This appears to work fine. . Now on to building doxybook2. I had to adjust a bit the instructions given by the doxybook2 Readme. There are probably some assumptions as to how vcpkg was bootstrapped, perhaps with sudo. Anyway, figuring out was not that difficult; I had to locate vcpkg.cmake, and the executable doxybook2 output was not in the place I expected it to be. . cmake -B ./build -G &quot;Unix Makefiles&quot; -DCMAKE_BUILD_TYPE=MinSizeRel -DCMAKE_TOOLCHAIN_FILE=${HOME}/src/vcpkg/scripts/buildsystems/vcpkg.cmake cmake --build ./build --config MinSizeRel # could do with a -j4 perhaps ${HOME}/src/doxybook2/build/src/DoxybookCli/doxybook2 --help . Let’s define a shorcut: . DX2=${HOME}/src/doxybook2/build/src/DoxybookCli/doxybook2 . Configuration . Doxygen settings . Adjust the doxygen settings in the Doxyfile. Below are some settings changed compared to the default config file, some found only after the fact (i.e. seeing the final output through MkDocs). . PROJECT_NAME = &quot;uchronia&quot; ### Also PROJECT_BRIEF, etc. OUTPUT_DIRECTORY = &quot;./doxyoutput&quot; ### INPUT = ../datatypes/datatypes/include/datatypes ### FULL_PATH_NAMES = NO ### INPUT_FILTER = &quot; sed &#39;s/^ s*DATATYPES_API s*//g;s/^ s*DATATYPES_DLL_LIB s*//g;s/DATATYPES_DLL_LIB//g&#39; &quot; ### GENERATE_XML = YES ### XML_OUTPUT = xml . FULL_PATH_NAMES = NO is useful to avoid having your full machine-specific file paths shown in the resulting output. INPUT_FILTER is less obvious, and relate to one technique in C libraries that may not be all that unusual (since derived from a Microsoft best practice as I recall). uchronia headers use macros such as #define DATATYPES_DLL_LIB __declspec(dllexport) on Windows, necessary to define which functions are exported (or imported). This results in C / C++ code such as : . DATATYPES_API char** GetEnsembleDatasetDataIdentifiers(ENSEMBLE_DATA_SET_PTR dataLibrary, int* size); //, or: class DATATYPES_DLL_LIB TimeSeriesChecks{ // } . This confuses doxygen and/or doxybook2, resulting in noisy or even incorrect rendered HTML output. INPUT_FILTER is here to strip out these macros prior to parsing. You may need to find such statement for your library. Consider using regex101 if you, like me, are shamefully not on top of regular expressions. . doxygen Doxyfile . doxybook2 settings . The doxybook2 repo suggests looking at the MkDocs + Material theme example, as this is the theme I use. . The config-doxybook2.json requires one change for the links. It took me a few trial and errors to figure out that I needed to set &quot;baseUrl&quot;: &quot;/uchronia-ts-doc/cpp/&quot; for URL links to work in the final API documentation generated on github pages. . { &quot;baseUrl&quot;: &quot;/uchronia-ts-doc/cpp/&quot;, &quot;indexInFolders&quot;: true, &quot;linkSuffix&quot;: &quot;/&quot;, &quot;indexClassesName&quot;: &quot;index&quot;, &quot;indexFilesName&quot;: &quot;index&quot;, &quot;indexGroupsName&quot;: &quot;index&quot;, &quot;indexNamespacesName&quot;: &quot;index&quot;, &quot;indexRelatedPagesName&quot;: &quot;index&quot;, &quot;indexExamplesName&quot;: &quot;index&quot;, &quot;mainPageInRoot&quot;: true, &quot;mainPageName&quot;: &quot;index&quot; } . This &quot;baseUrl&quot;: &quot;/uchronia-ts-doc/cpp/&quot; deserves a bit of an explanation so that you hopefully don’t have to trial much (it is not rocket science, but…). . The markdown documentation for the site will be located under a docs folder, the mkdocs default. The C++ derived markdown documents will be under a subfolder docs/cpp. doxygen did not know about this upfront (though there may be a way to do it, but nevermind), so doxybook2 is where this prefix can be added to building URLs. . The MkDocs settings mkdocs.yml will have a site_url: https://csiro-hydroinformatics.github.io/uchronia-ts-doc/ specified. See the MkDocs configuration doc for explanations. To be honest I am very sketchy in Web URL things, and not sure how to explain reliably what is going on in detail. Just know that &quot;baseUrl&quot;: &quot;/uchronia-ts-doc/cpp/&quot; was required in the doxybook2 config file, with leading and trailing / characters, for links to work. I hope this helps you speed up finding your own exact settings. . Running doxybook2 . So, our doxygen Doxyfile has created a doxyoutput folder. Now let us convert to markdown: . mkdir -p docs/cpp $DX2 --input ./doxyoutput/xml --output ./docs/cpp --config config-doxybook2.json . # . ~/config/baseconda # conda activate mkdocsenv mkdocs build --clean --site-dir _build/html --config-file mkdocs.yml mkdocs serve . Finally . Updating the github pages site is simply: . mkdocs gh-deploy --clean --site-dir _build/html --config-file mkdocs.yml . Some rough corners left, but it largely works: . .",
            "url": "https://jmp75.github.io/work-blog/documentation/c++/python/mkdocs/2022/08/22/doxygen-doxybook-mkdocs.html",
            "relUrl": "/documentation/c++/python/mkdocs/2022/08/22/doxygen-doxybook-mkdocs.html",
            "date": " • Aug 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Runtime profiling of python bindings for a C/C++ library",
            "content": "Background . A software stack for streamflow forecasting consists of a C++ modelling engine, accessible via a C API from various interactive language bindings including python. I have not done some benchmark performance profiling for a while on this stack. I expect the C++ core to be pretty much as good as it was 8 years ago, but as I bed down some of the python bindings, I notice some lags when interacting. This is not a surprise, and I expect these to be either in python code, or perhaps at the boundary with the C API. This intuition needs to be confirmed by objective measurements. . It is one of these things that are rarely a blocker for quite a while, but a bit of instrumentation and performance tuning improves the user experience, a little bit every day. Also, some use cases where substantial stochastic data is generated in Python and passed to the C++ core would be very penalised by inefficiencies in Python or Python/C interoperability code. . Foreword: in the case below, we do not need to profile C++ code per se after all, so if this is what you are after specifically. Read on for the Python side of the story. . Python profilers . 9 fine libraries for profiling python code is a recent article, as I write. Of these, Palanteer is interesting in its mixed mode profiling capabilities (Python and C++). I&#39;ll have to explore it, perhaps not just yet though. A priori the cProfiler coming with the Python base library is all I need for this immediate use case. . Runtime profiling . I will skip on the details of the imported libraries here. . import pandas as pd import numpy as np import xarray as xr from swift2.simulation import create_subarea_simulation, create_catchment # etc. etc, other imports . Creating synthethic hydrologic model . The overall purpose of the exercise will be to measure performance under various conditions: model structure, size, time steps, etc. We will not do all that in this post; suffice to say we define a set of functions to create synthetic model setups. We do not show the full definitions. To give a flavour . from cinterop.timeseries import mk_hourly_xarray_series def create_pulses(nTimeSteps, start) : return mk_hourly_xarray_series( createPulse(nTimeSteps, 5.0, 48), start) def create_uniform(nTimeSteps, start) : return mk_hourly_xarray_series( createUniform(nTimeSteps, 1.0), start) def set_test_input(ms:&#39;Simulation&#39;, subAreaName:&#39;str&#39;, rainVarName, petVarName, rain_ts, pet_ts): p_id = mk_full_data_id(&#39;subarea&#39;, subAreaName, rainVarName) e_id = mk_full_data_id(&#39;subarea&#39;, subAreaName, petVarName) ms.play_input( rain_ts, p_id) ms.play_input( pet_ts, e_id) # def create_line_system(n_time_steps, n_links, rain_varname = &quot;P&quot;, pet_varname = &quot;E&quot;, area_km2 = 1): # et caetera . We are now ready to create our catchment simulation. Before we plunge into cProfiler let&#39;s use a simpler way to assess the runtime from notebooks: . Using notebook&#39;s %%time . Let&#39;s create a setup with 15 subareas, hourly time step over 10 years. . %%time n_time_step = 10 * 365 * 24 ms = create_line_system(n_time_step, 15) . CPU times: user 888 ms, sys: 8.94 ms, total: 897 ms Wall time: 897 ms . Well, this was only the create of the baseline model, not even execution, and this already takes close to a second. Granted, there are a fair few hours in a decade. Still, a whole second! . What about the simulation runtime? Let&#39;s parameterise minimally to avoid possible artefacts, and execute. . from swift2.doc_helper import configure_hourly_gr4j, get_free_params configure_hourly_gr4j(ms) p = create_parameteriser(&#39;Generic subarea&#39;, get_free_params(&quot;GR4J&quot;)) p.apply_sys_config(ms) . Double check we are indeed running hourly over 10 years: . ms.get_simulation_span() . {&#39;start&#39;: datetime.datetime(1989, 1, 1, 0, 0), &#39;end&#39;: datetime.datetime(1998, 12, 29, 23, 0), &#39;time step&#39;: &#39;hourly&#39;} . %%time ms.exec_simulation() . CPU times: user 314 ms, sys: 691 µs, total: 315 ms Wall time: 314 ms . This is actually quite good, and &quot;unexpectedly&quot; less than the model creation itself. This is actually not all that surprising. All of the model execution happens in C++ land. The model setup involves much more operations in python. . Let&#39;s look at an operation exchanging data from the C++ engine for display in Python. The model simulation has some of its states receiving input time series: . ms.get_played_varnames()[:6] . [&#39;subarea.1.E&#39;, &#39;subarea.1.P&#39;, &#39;subarea.10.E&#39;, &#39;subarea.10.P&#39;, &#39;subarea.11.E&#39;, &#39;subarea.11.P&#39;] . Let&#39;s see what happens in the retrieval of one of these input time series: . %%time ts = ms.get_played(&quot;subarea.1.E&quot;) . CPU times: user 441 ms, sys: 106 µs, total: 441 ms Wall time: 440 ms . This is substantial; more than the native execution over a catchment with 15 subareas. So: . Can we identify the hotspot(s)? | Can we do something to improve it. | . Profiling . Enter cProfile, as we will stick with this in this post. Adapting some of the sample code shown in the Python documentation on profilers . import cProfile . import pstats, io pr = cProfile.Profile() pr.enable() ts = ms.get_played(&quot;subarea.1.E&quot;) pr.disable() . s = io.StringIO() sortby = pstats.SortKey.CUMULATIVE ps = pstats.Stats(pr, stream=s).sort_stats(sortby) . We will print only the top 5 % of the list of function calls, and see if we can spot the likely hotspot . ps.print_stats(.05) print(s.getvalue()) . 6137 function calls (5964 primitive calls) in 0.445 seconds Ordered by: cumulative time List reduced from 588 to 29 due to restriction &lt;0.05&gt; ncalls tottime percall cumtime percall filename:lineno(function) 2 0.000 0.000 0.445 0.222 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3361(run_code) 2 0.000 0.000 0.445 0.222 {built-in method builtins.exec} 1 0.000 0.000 0.445 0.445 /tmp/ipykernel_34182/2688031072.py:4(&lt;cell line: 4&gt;) 1 0.000 0.000 0.445 0.445 /home/per202/src/swift/bindings/python/swift2/swift2/classes.py:471(get_played) 1 0.000 0.000 0.445 0.445 /home/per202/src/swift/bindings/python/swift2/swift2/play_record.py:190(get_played) 1 0.000 0.000 0.444 0.444 /home/per202/src/swift/bindings/python/swift2/swift2/internal.py:94(internal_get_played_tts) 1 0.000 0.000 0.444 0.444 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/data_set.py:10(get_multiple_time_series_from_provider) 1 0.000 0.000 0.444 0.444 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:69(internal_get_multiple_time_series) 1 0.000 0.000 0.441 0.441 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:76(&lt;listcomp&gt;) 1 0.000 0.000 0.441 0.441 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:74(f) 1 0.000 0.000 0.441 0.441 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:79(internal_get_single_model_time_series) 1 0.002 0.002 0.441 0.441 /home/per202/src/swift/bindings/python/swift2/swift2/wrap/swift_wrap_custom.py:216(get_played_pkg) 1 0.000 0.000 0.438 0.438 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:386(geom_to_xarray_time_series) 1 0.000 0.000 0.368 0.368 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:367(ts_geom_to_even_time_index) 1 0.000 0.000 0.368 0.368 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py:22(create_even_time_index) 1 0.368 0.368 0.368 0.368 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py:25(&lt;listcomp&gt;) 16 0.000 0.000 0.071 0.004 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/dataarray.py:365(__init__) 1 0.000 0.000 0.070 0.070 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:356(create_ensemble_series) 9 0.000 0.000 0.070 0.008 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:74(as_variable) 1 0.000 0.000 0.070 0.070 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/dataarray.py:90(_infer_coords_and_dims) 36 0.000 0.000 0.070 0.002 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:181(as_compatible_data) 35/31 0.060 0.002 0.060 0.002 {built-in method numpy.asarray} 1 0.000 0.000 0.009 0.009 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:172(_possibly_convert_objects) 1 0.000 0.000 0.009 0.009 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/series.py:323(__init__) 2 0.000 0.000 0.009 0.005 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/construction.py:470(sanitize_array) 2 0.000 0.000 0.009 0.005 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/construction.py:695(_try_cast) 1 0.000 0.000 0.009 0.009 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1466(maybe_infer_to_datetimelike) 2 0.000 0.000 0.006 0.003 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py:1994(_sequence_to_dt64ns) 1 0.000 0.000 0.006 0.006 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1499(try_datetime) . in a file rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py the function create_even_time_index appears to be where a lengthy operation occurs. More precisely, when this function does a list comprehension (listcomp). Note that I infer this because in the cumtime (cumulative time) column there is a drop from 0.396 ms for listcomp to 0.071 for the rest of the operations under this function. I think this is the right way to interpret it in this case, but it may not be the case in other profiling context. . The code for create_even_time_index is at this permalink . def create_even_time_index(start:ConvertibleToTimestamp, time_step_seconds:int, n:int) -&gt; List: start = as_timestamp(start) delta_t = np.timedelta64(time_step_seconds, &#39;s&#39;) return [start + delta_t * i for i in range(n)] . [start + delta_t * i for i in range(n)] is the bulk of the time, .396 out of a total 0.477 ms. . This list is created as the basis for a time index for the creation of the xarray object returned by the overall get_played function. So, is there a faster way to create this time series index? . Performance tuning . start = pd.Timestamp(year=2000, month=1, day=1) . n= 24*365*10 . def test_index_creation(start, n:int) -&gt; List: start = as_timestamp(start) time_step_seconds = 3600 delta_t = np.timedelta64(time_step_seconds, &#39;s&#39;) return [start + delta_t * i for i in range(n)] . %%time a = test_index_creation(start, n) . CPU times: user 352 ms, sys: 562 µs, total: 353 ms Wall time: 351 ms . start is a pandas Timestamp, and we add to it an object of type np.timedelta64 87600 times. I doubt this is the main issue, but let&#39;s operate in numpy types as much as we can by converting the pd.Timestamp once: . start . Timestamp(&#39;2000-01-01 00:00:00&#39;) . start.to_datetime64() . numpy.datetime64(&#39;2000-01-01T00:00:00.000000000&#39;) . def test_index_creation(start, n:int) -&gt; List: start = as_timestamp(start).to_datetime64() time_step_seconds = 3600 delta_t = np.timedelta64(time_step_seconds, &#39;s&#39;) return [start + delta_t * i for i in range(n)] . %%time a = test_index_creation(start, n) . CPU times: user 293 ms, sys: 8.48 ms, total: 301 ms Wall time: 300 ms . This is actually more of an improvement than I anticipated. OK. What else can we do? . Pandas has the helpful Time series / date functionality page in its documentation. The function from which we started is generic, but for important cases such as hourly and daily time steps, there are options to use the freq argument to the date_range function . %%time pd.date_range(start, periods=n, freq=&quot;H&quot;) . CPU times: user 242 µs, sys: 686 µs, total: 928 µs Wall time: 520 µs . DatetimeIndex([&#39;2000-01-01 00:00:00&#39;, &#39;2000-01-01 01:00:00&#39;, &#39;2000-01-01 02:00:00&#39;, &#39;2000-01-01 03:00:00&#39;, &#39;2000-01-01 04:00:00&#39;, &#39;2000-01-01 05:00:00&#39;, &#39;2000-01-01 06:00:00&#39;, &#39;2000-01-01 07:00:00&#39;, &#39;2000-01-01 08:00:00&#39;, &#39;2000-01-01 09:00:00&#39;, ... &#39;2009-12-28 14:00:00&#39;, &#39;2009-12-28 15:00:00&#39;, &#39;2009-12-28 16:00:00&#39;, &#39;2009-12-28 17:00:00&#39;, &#39;2009-12-28 18:00:00&#39;, &#39;2009-12-28 19:00:00&#39;, &#39;2009-12-28 20:00:00&#39;, &#39;2009-12-28 21:00:00&#39;, &#39;2009-12-28 22:00:00&#39;, &#39;2009-12-28 23:00:00&#39;], dtype=&#39;datetime64[ns]&#39;, length=87600, freq=&#39;H&#39;) . It is two order of magnitude faster... Definitely worth a re-engineering of the features in the timeseries.py we started from. . This probably does not solve the issue for many other cases (irregular time steps, e.g. monthly), but there are many cases where we could benefit. The date_range documentation specifies that an arbitrary DateOffset to its freq argument (freq: str or DateOffset, default ‘D’). How efficient is this operation on our 87600 data points? . d_offset = pd.tseries.offsets.DateOffset(minutes=15) . start + d_offset . Timestamp(&#39;2000-01-01 00:15:00&#39;) . %%time pd.date_range(start, periods=n, freq=d_offset) . CPU times: user 1.5 s, sys: 1.32 ms, total: 1.5 s Wall time: 1.5 s . DatetimeIndex([&#39;2000-01-01 00:00:00&#39;, &#39;2000-01-01 00:15:00&#39;, &#39;2000-01-01 00:30:00&#39;, &#39;2000-01-01 00:45:00&#39;, &#39;2000-01-01 01:00:00&#39;, &#39;2000-01-01 01:15:00&#39;, &#39;2000-01-01 01:30:00&#39;, &#39;2000-01-01 01:45:00&#39;, &#39;2000-01-01 02:00:00&#39;, &#39;2000-01-01 02:15:00&#39;, ... &#39;2002-07-01 09:30:00&#39;, &#39;2002-07-01 09:45:00&#39;, &#39;2002-07-01 10:00:00&#39;, &#39;2002-07-01 10:15:00&#39;, &#39;2002-07-01 10:30:00&#39;, &#39;2002-07-01 10:45:00&#39;, &#39;2002-07-01 11:00:00&#39;, &#39;2002-07-01 11:15:00&#39;, &#39;2002-07-01 11:30:00&#39;, &#39;2002-07-01 11:45:00&#39;], dtype=&#39;datetime64[ns]&#39;, length=87600, freq=&#39;&lt;DateOffset: minutes=15&gt;&#39;) . It looks like in this case this is actually a fair bit slower than my original implementation. Interesting. And using start.to_datetime64() makes no difference too. . Conclusion . This demonstrated a relatively simple, but real case where cProfiler helps alleviate usually small but pervasive runtime inefficiencies. In this case, so far we have not needed to look close to the Python/C interoperability layer. The key bottleneck was pure python. I envisage I may post something later on looking at trickier situation. . To be honest, I did second guess a few things upfront. But the cProfiler and simpler %%time brought at least confirmation and sometimes useful, conter-intuitive insight. &quot;You cannot manage what you cannot measure&quot; as the saying goes. .",
            "url": "https://jmp75.github.io/work-blog/c++/python/performance/runtime/2022/08/09/python-profiling-interop.html",
            "relUrl": "/c++/python/performance/runtime/2022/08/09/python-profiling-interop.html",
            "date": " • Aug 9, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Azure devops CI pipeline for hydrologic forecasting software",
            "content": "Background . For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. Recently I explored updating the compilation to a more recent Microsoft visual c++ compiler in another post. . This post is an overview of the process to set up a build pipeline on Azure Devops. Given the fairly complicated codebase dealt with, quite a few technical aspects will be touched on, but not in great details for the sake of keeping it a blog post rather than a full report. . Pipeline inputs and outputs . Outputs . Debian packages for installing C++ pre-compiled libraries and header files | Windows DLLs of these C++ pre-compiled libraries, compiled with Microsoft Visual C++ 2019. | Python wheels of packages accessing these C++ libraries | R packages accessing these C++ libraries. Source tarballs for Linux, and binary packages for Windows | Matlab functions accessing these C++ libraries | . Conda packages are an option for later - see appendix. . The outputs of this/these pipelines is used for installation as described for instance in these instructions . Inputs . These C++, Python and R packages are in the following source code repositories. . Third party libraries: boost libraries, netcdf, yaml-cpp and jsoncpp, threadpool, Catch headers. Boost C++ | netCDF4 | yaml-cpp | jsoncpp | threadpool | . | Open source repositories with generic capabilities: rcpp-interop-commons: supporting C API interoperability with Python, R, etc. | moirai: Manage C++ Objects’s lifetime when exposed through a C API | pyrefcount: Reference counting facilities for Python | wila: C++ header-only metaheuristics optimisation | config-utils | efts | efts-python | mhplot: R visualisation for metaheuristics optimisation | uchronia-time-series: time series handling for ensembles simulations and forecasts in C++ | . | Closed source, domain specific: swift: Streamflow Water Information Forecasting Tools | FoGSS: A model for generated forecast guided stochastic scenarios of monthly streamflows out to 12 months | A repository with semi-automated build scripts | . | . Another repository worth mentioning even if I do not envisage it being used is c-api-wrapper-generation. It contains some fairly sophisticated capabilities for generating language bindings on top of C APIs. . Walkthrough . Pipelines code repositories . A clone of the the azure devops pipelines described in this post are now mirrored at: . hydro-forecast-linux-pipeline | hydro-forecast-windows-pipeline | . Build platforms . For Linux I actually ended up building a Debian docker image on top of the ubuntu-latest image Azure Devops offers. This was partly an arbitrary choice (habits, and starting by reusing another pipeline). . For Windows, the windows-2019 image is more or less a given. Note that if I had chosen to, or had to, build with an older compiler version (for example, Python and conda on Windows I believe is at least recommending the 2017 toolchain currently), installing the Microsoft build toolchain would have been needed. . Checking out source code . There is a description of how to check out multiple repositories in your pipeline, which I may move to in the future but frankly felt cumbersome when I gave it a try, for that many repositories. Besides, I am starting from existing build scripts (bash or DOS) which I have an incentive to reuse. . Which brings us to dealing safely with checking out closed source code, requiring authentication. . Using Personal Access Tokens (PAT) . Disclaimer: I believe the following recipe to be in line with best practices, but it is up to you to decide. . When googling for the documentation, Use personal access tokens tends to be what you land on, but this is more confusing than helpful. Set secret variables contains the key recipe to set up a secret PAT and pass it on to a pipeline task. Assuming you already have a PAT that you defined in a pipeline variable SWIFT_PAT, for which you checked Keep this value secret, you need to map this value to pass it to the script checking out source code: . - script: | call dos-setup.bat call checkout.bat env: SWIFT_PAT_ENV_VAR: $(SWIFT_PAT) # the recommended way to map to an env variable # NOTE: you cannot have the same name: # SWIFT_PAT: $(SWIFT_PAT) # &lt;-- fails with a circular definition issue. . In your checkout.bat script you can retrieve set MY_PAT=%SWIFT_PAT_ENV_VAR% and use it as part of the URL portion e.g. set MY_BITBUCKET_URL_ROOT=https://%YOUR_USERNAME%:%MY_PAT%@bitbucket.csiro.au/scm . A gotcha with Bitbucket PATs . if your closed source code is on Bitbucket, the bitbucket documentation HTTP access tokens is quite clear, but fails to document something. Bitbucket PAT generation tends to create them with slash characters “/” which is a special character for URLs so it needs to be replace with “%2F” before you use it as a value for your Azure Pipeline, otherwise you end up with an invalid URL error when checking out. Note also that %2 is fraught when used in DOS scripts, but if you use the recipe above, you should not come across an issue with this. . Building C++ code . Linux . Compilation on Linux is performed using cmake. The pipeline is not calling cmake and make commands directly, but building debian packages. One purpose of this build pipeline is to produce binary installable packages. It took me a fair bit of trial and error, two years ago, to find a suitable working recipe for Debian packaging of “my” libraries. There is documentation and plenty of examples out there, but plenty of variances in how it is done. An example can be found in the Moirai repository, and the pipeline build commands are in build_debian_pkgs.sh. . Windows . Compilation is done using microsoft visual c++ 2019, using visual studio solution files. Years ago, after enough frustrations invikig MSBuild.exe from DOS scripts, I moved to using powershell. Currently I am using the powershell Invoke-MsBuild as a third party tool to invoke compilation, rather than mdsbuild.exe directly. This is a nifty basis to build my own powershell module (not open source yet), with cmdlets such as: . Install-SharedLibsMultiCfg -Solutions $lvlTwoSlns -LibsDirs $libsDirs -BuildPlatforms $buildPlatforms -BuildMode $buildMode -ToolsVersion $toolsVersion -LibNames $lvlTwoLibnames . which is roughly an equivalent of make &amp;&amp; make install on Linux. The script for the step is build-stack.ps1, and the bulk of the runtime in the pipeline. . Unit tests . All C++ unit tests are run on the Windows platform. (Debian to follow soon). There are some lessons learnt in setting these up in a pipeline, but for now most of the code remains closed source and may be explored in another post. . Python wheels, R package tarballs . The python package involved are “pure python” by design, and the wheels built are platform-agnostic and built only once on Linux (build_python_pkgs.sh). Most packages do deal a lot with interoperability with native libraries with a C API, but this is done via cffi as a runtime step. . R source packages can be built on either platform. Except R Windows binary packages, which are much preferable for Windows. build_r_pkgs.sh for the Linux pipeline builds these, also building tutorials (“vignettes”) which are contributing to the testing regime, notably for the interoperability with the native libraries of the stack. . I decided not to build the R tarballs on Windows, but try instead to downloadm, from the Window pipeline, the last output resulting the Linux pipeline. Partly for the sake of gaining know-how. I ended up with way more confusion and frustration with this than I anticipated, as reflected in my stackoverflow answer and previous blog post. . Downloading the artifact from Windows is still done via a shell script fetch-pkgs.sh. The script is run mapping a AZURE_DEVOPS_EXT_PAT environment variable. The rather stunted Sign in with a personal access token (PAT) page somewhat explains why this is used. . Bash on windows in Azure Pipelines: a gotcha with backslash directories . This is an occasion to point to an annoyance of backslashes as directory separators on Windows. I had defined a pipeline variable linux_packages_dir: $(Build.ArtifactStagingDirectory) swift_linux, which would have been something like D: a s a swift_linux. In the script of a pipeline task task: Bash@3, if you use the variable evaluation $(linux_packages_dir) you loose the backslash separators. I had to rebuild the path and replace with forward slashes using the environment variable BUILD_ARTIFACTSTAGINGDIRECTORY, which is adding to the entropy. . inputs: targetType: &#39;inline&#39; script: | # using $(linux_packages_dir) here fails; dir separators are lost. # linux_packages_dir=$(linux_packages_dir) # linux_packages_fwd_dir=&quot;${linux_packages_dir// //}&quot; # instead have to do: linux_packages_fwd_dir=&quot;${BUILD_ARTIFACTSTAGINGDIRECTORY// //}/swift_linux&quot; ./fetch-pkgs.sh ${linux_packages_fwd_dir} env: AZURE_DEVOPS_EXT_PAT: $(AZ_ARTIFACT_DL_PAT) . Azure Artifacts . Outputs are published as collections of files in a “Universal Package”. I have (had - may have changed) not found in the azure devops API way to query the artifacts for the version without downloading the whole artifact. A workaround is to publish two packages: a small one with the version number, and the real artifact. . To get a custom $(Build.BuildNumber), and r is a counter reset to 1 every change of the major/minor versions, use name: &#39;0.1.$(Rev:r)&#39; on your pipeline. Then the publishing tasks can be: . - task: UniversalPackages@0 displayName: Publish output bundle inputs: command: publish publishDirectory: &#39;$(Build.ArtifactStagingDirectory) release&#39; vstsFeedPublish: &#39;my_project_name/hydro_forecast_win&#39; vstsFeedPackagePublish: &#39;swift_win&#39; versionOption: custom versionPublish: &#39;$(Build.BuildNumber)&#39; packagePublishDescription: &#39;Windows packages for swift and co.&#39; - task: UniversalPackages@0 displayName: Publish output bundle version inputs: command: publish publishDirectory: &#39;$(Build.ArtifactStagingDirectory) version&#39; vstsFeedPublish: &#39;my_project_name/hydro_forecast_win&#39; vstsFeedPackagePublish: &#39;swift_win_version&#39; versionOption: custom versionPublish: &#39;$(Build.BuildNumber)&#39; packagePublishDescription: &#39;Version number for windows swift and co. bundle&#39; . Conclusion . While the level of overall build streamlining was always adequate for this software stack, previous pipelines run on Jenkins could not be maintained. Azure Devops appearing to be the main corporate standard, we’ve trialed setting up Azure pipelines. Despite some notable user frustrations with the Azure Devops environment, its peculiarities and access arrangements, this should have substantial dividends to expand the user base and enable new projects. . Appendix . Other possibilities . I have explored using conda packaging to distribute the whole software stack. This appears feasible but is a leap I am not ready to do for several reasons (time, resources and confirmed user demands being the main ones). There is a substantial learning curve, technical and in terms of governance of a private conda channel. | Caching some of the downloaded test data and third party libraries. | Publishing a linux debian docker image ready to use as a baseline. | . Resources . URLs which may or may not have influenced this work: . Conda-forge default azure pipeline | azure devops pipeline variables | visual-studio-build-tools-2017-silent-install may be handy if 2017 builds are needed e.g for conda. | Windows2019 image environment | Azure pipelines for Marian NMT | Trigger Build Task | .",
            "url": "https://jmp75.github.io/work-blog/recipes/vcpp/c++/debian/windows/python%20packaging/r%20packaging/2022/07/31/azure-pipeline-vcpp-stack.html",
            "relUrl": "/recipes/vcpp/c++/debian/windows/python%20packaging/r%20packaging/2022/07/31/azure-pipeline-vcpp-stack.html",
            "date": " • Jul 31, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "A whinge about Azure Devops",
            "content": "Foreword: You’ll find a minimal working example towards the end of the post, if you want to skip the rants and googling tribulations . How was your day? . Today, Azure Devops took five-ish hours of my life I’ll never get back. Not for the first time. I’ll try to keep expletive out of this post. . I’ve been using Azure Devops for about a year to set up build and deployment pipelines. Not on a regular basis, but several instances. This week I am trying to finalise a new build pipeline. And I feel like venting some frustration. It may make some other sufferers feel less alone. Earlier this week I had a training webinar on taking on board UX (user experience) techniques in our work, implicitely for the delivery of digital products. This adds irony to my resentment against Microsoft for landing Azure Devops where it stands. . I don’t have the time or energy to capture in compelling details all the frustrations or puzzlements in this post. After another week more than I would have liked to spend on it, Azure Devops feels obtuse, alienating, unpleasant. I have not tried many other similar offering recently, so I don’t know if the grass is greener over the fence. And I doubt it. . Today’s tribulations. . Getting a pipeline to publish a package in a project feed . I was putting the final step of a long-running build pipeline (C++, R), to publish an artifact using the task UniversalPackages@0 in the azure-pipeline.yml. And I had managed to do that in another pipeline some months ago. After bumping into an issue of my making (need to create a new feed in the project to host these artifacts), I end up with the glorious error message: . {&quot;@t&quot;:&quot;2022-07-26T10:22:42.4886223Z&quot;,&quot;@m&quot;:&quot;An error occurred on the service. User &#39;aaaaaaaa-2efe-46ec-b780-ffffffffffff&#39; lacks permission to complete this action. You need to have &#39;AddPackage&#39;.&quot;,&quot;@i&quot;:&quot;bbbbbbbb&quot;,&quot;@l&quot;:&quot;Error&quot;,&quot;SourceContext&quot;:&quot;ArtifactTool.Program&quot;,&quot;UtcTimestamp&quot;:&quot;2022-07-26 10:22:42.488Z&quot;} . “You”? Who? Me? Who is user ‘aaaaaaaa-2efe-46ec-b780-ffffffffffff’? Is it me? I am admin in this organisation, and owner of the pipeline. . And on with the wild goose chase. . First port of call, compare every setting to the other, working pipeline. I notice that the group Project Collection Build Service (my-az-organisation) is not present with a contributor Role. Fix that. . Nope, still the same. (Later, Looking back, I must have missed something in the feed permission list (Collaborator vs. Contributor role)) . Help! Google. . This StackOverflow thread has hints about needing to add a Build Service with a Contributor role. There are also references at needing to go to an “…” button to change an allowed scope, but as of 2022 the user interface has changed. Other posts such as this and this suggest there is no shortage of confusion besides myself. . In the Microsoft documentation maze Manage build service account permissions is the wrong place to look at. Configure feed settings is the right place, but despite not being incorrect it is too generic and unhelpful for many users who just want a specific, working recipe for their use case. . In the end I managed to find a recipe after setting a minimal example from scratch. . A minimal, working example . Since at least the user interface has changed since the 2019 Stack Overflow posts: . Caution: these screen captures below are taken from an Azure Devops organisation where the name of the single project in that organisation is the same as the organisation. These are blacked out, but you can see it partly. This stems from my employer’s policies. Not something I can do anything about, but not the best context for explanation. In your case your organisation name and project name may differ, and probably should to avoid confusion. . First, create the new feed in the artifact section. If you already have a feed, I am not sure. . . Create this new feed, setting the scope to the project, which is recommended anyway. This is important for subsequent permission settings: if you choose organisation, this example may not pan out. The visibility option may not matter for this example, but am not sure. . . By default, the permission list created for the new feed is as below. Note that the [project_name] Build Service ([organisation_name]) user or group has the role Collaborator by default. This may be the key stumbling block users trip over. . . As I write, you cannot change the role; you have to remove the [project_name] Build Service ([organisation_name]) user or group and add it again with the Contributor role. . . With that in place, the following pipeline works: . trigger: - main resources: - repo: self variables: tag: &#39;$(Build.BuildId)&#39; # to get a custom &#39;$(Build.BuildNumber)&#39;, and &#39;r&#39; is a counter reset to 1 every change of the major/minor versions name: &#39;0.1.$(Rev:r)&#39; stages: - stage: Build displayName: Build packages jobs: - job: Build displayName: Build packages pool: vmImage: windows-2019 steps: - checkout: self - script: | mkdir $(Build.ArtifactStagingDirectory) release cd $(Build.ArtifactStagingDirectory) release echo &quot;test&quot; &gt; blah.txt displayName: &#39;create mock artifact&#39; - task: UniversalPackages@0 displayName: Publish output bundle inputs: command: publish publishDirectory: &#39;$(Build.ArtifactStagingDirectory) release&#39; vstsFeedPublish: &#39;OD222236-DigWaterAndLandscapes/sandpit_win&#39; vstsFeedPackagePublish: &#39;sandpit_win&#39; versionOption: custom versionPublish: &#39;$(Build.BuildNumber)&#39; packagePublishDescription: &#39;Test package publication&#39; . Parting words . This was but one illustration of the inadequate UX with Azure Devops. Looking back on this one, this is a relatively simple and common use case, and after the fact I feel I should have second-guessed the settings. But I and visibly many others have been derailed and confused by the process, trying to find a didactic example to work from by similarity. It seem that the Microsoft documentation draws you into having to absorb a web of overly complicated and abstract concepts that are overwhelming unless you are a full-time devops. .",
            "url": "https://jmp75.github.io/work-blog/azure/devops/microsoft/2022/07/27/azure-devops-negative-experience.html",
            "relUrl": "/azure/devops/microsoft/2022/07/27/azure-devops-negative-experience.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Lithology classification using Hugging Face, part 3",
            "content": "About . This is a continuation of Lithology classification using Hugging Face, part 2. . The &quot;Part 2&quot; post ended up on an error on calling trainer.train, with incompatible tensor dimensions in a tensor multiplication. It was not clear at all (to me) what the root issue was. After getting back to basics and looking at the HF Text classification how-to, I noticed that my Dataset contained pytorch tensors or lists thereof, where the how-do just had simple data types. . Long story short, I removed the tokernizer&#39;s parameter return_tensors=&quot;pt&quot;, and did not call tok_ds.set_format(&quot;torch&quot;), and surprised, it worked. I had added these because the initial trial complained about a mix of GPU and CPU data. . Plan . At this stage, it is worthwhile laying out a roadmap of where this line of work may go: . Complete a classification on at least a subset of the Namoi dataset (this post) | Upload a trained model to Hugging Face Hub, or perhaps fastai X Hugging Face Group 2022 | Set up a Gradio application on HF Spaces | Project proposal at work. Weekend self-teaching can only go so far. | . Walkthrough . Much of the code in this section is very similar to Lithology classification using Hugging Face, part 2, so blocks will be less commented. . import numpy as np import pandas as pd import torch from datasets import Dataset from transformers import AutoModelForSequenceClassification, AutoTokenizer from pathlib import Path from datasets import ClassLabel from transformers import TrainingArguments, Trainer from sklearn.metrics import f1_score from sklearn.metrics import roc_curve,confusion_matrix,auc import matplotlib.pyplot as plt from collections import Counter # Some column string identifiers MAJOR_CODE = &quot;MajorLithCode&quot; MAJOR_CODE_INT = &quot;MajorLithoCodeInt&quot; # We will create a numeric representation of labels, which is (I think?) required by HF. MINOR_CODE = &quot;MinorLithCode&quot; DESC = &quot;Description&quot; fn = Path(&quot;~&quot;).expanduser() / &quot;data/ela/shp_namoi_river/NGIS_LithologyLog.csv&quot; litho_logs = pd.read_csv( fn, dtype={&quot;FromDepth&quot;: str, &quot;ToDepth&quot;: str, MAJOR_CODE: str, MINOR_CODE: str} ) def token_freq(tokens, n_most_common=50): list_most_common = Counter(tokens).most_common(n_most_common) return pd.DataFrame(list_most_common, columns=[&quot;token&quot;, &quot;frequency&quot;]) litho_classes = litho_logs[MAJOR_CODE].values df_most_common = token_freq(litho_classes, 50) NUM_CLASSES_KEPT=17 labels_kept = df_most_common[&quot;token&quot;][:NUM_CLASSES_KEPT].values labels_kept = labels_kept[labels_kept != &quot;None&quot;] labels_kept . array([&#39;CLAY&#39;, &#39;GRVL&#39;, &#39;SAND&#39;, &#39;SHLE&#39;, &#39;SDSN&#39;, &#39;BSLT&#39;, &#39;TPSL&#39;, &#39;SOIL&#39;, &#39;ROCK&#39;, &#39;GRNT&#39;, &#39;SDCY&#39;, &#39;SLSN&#39;, &#39;CGLM&#39;, &#39;MDSN&#39;, &#39;UNKN&#39;, &#39;COAL&#39;], dtype=object) . kept = [x in labels_kept for x in litho_classes] litho_logs_kept = litho_logs[kept].copy() # avoid warning messages down the track. labels = ClassLabel(names=labels_kept) int_labels = np.array([ labels.str2int(x) for x in litho_logs_kept[MAJOR_CODE].values ]) int_labels = int_labels.astype(np.int8) # to mimick chapter3 HF so far as I can see litho_logs_kept[MAJOR_CODE_INT] = int_labels . We will fine tune a smaller version of DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing available on the Hugging Face model repository. . STARTING_MODEL = &quot;microsoft/deberta-v3-small&quot; . Dealing with imbalanced classes with weights . sorted_counts = litho_logs_kept[MAJOR_CODE].value_counts() class_weights = (1 - sorted_counts / sorted_counts.sum()).values class_weights = torch.from_numpy(class_weights).float().to(&quot;cuda&quot;) . Tokenisation . p = Path(&quot;./tokz_pretrained&quot;) pretrained_model_name_or_path = p if p.exists() else STARTING_MODEL # Tokenizer max length max_length = 128 # https://discuss.huggingface.co/t/sentence-transformers-paraphrase-minilm-fine-tuning-error/9612/4 tokz = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=True, max_length=max_length, model_max_length=max_length) if not p.exists(): tokz.save_pretrained(&quot;./tokz_pretrained&quot;) . We know from the previous post that we should work with lowercase descriptions to have a more sensible tokenisation . litho_logs_kept[DESC] = litho_logs_kept[DESC].str.lower() litho_logs_kept_mini = litho_logs_kept[[MAJOR_CODE_INT, DESC]] litho_logs_kept_mini.sample(n=10) . MajorLithoCodeInt Description . 88691 3 | shale | . 77323 11 | siltstone | . 42318 0 | clay fine sandy water supply | . 85089 1 | gravel; as above, except gravels 70% 2-10mm, 3... | . 112223 0 | clay; 70%, light brown. coarse sand to fine gr... | . 35510 0 | clay | . 106351 0 | clay | . 80478 0 | clay; ligth grey with brown streaks - with som... | . 20290 1 | gravel | . 23426 0 | clay, gravelly, blueish | . Create dataset and tokenisation . We will use a subset sample of the full dataset to train on, for the sake of execution speed, for now . len(litho_logs_kept_mini) . 123657 . litho_logs_kept_mini_subset = litho_logs_kept_mini.sample(len(litho_logs_kept_mini) // 4) len(litho_logs_kept_mini_subset) . 30914 . ds = Dataset.from_pandas(litho_logs_kept_mini_subset) def tok_func(x): return tokz( x[DESC], padding=&quot;max_length&quot;, truncation=True, max_length=max_length, # return_tensors=&quot;pt&quot;, ## IMPORTANT not to use return_tensors=&quot;pt&quot; here, perhaps conter-intuitively ) . tok_ds = ds.map(tok_func) num_labels = len(labels_kept) . Parameter &#39;function&#39;=&lt;function tok_func at 0x7f49f6e17a60&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn&#39;t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won&#39;t be showed. . p = Path(&quot;./model_pretrained&quot;) model_name = p if p.exists() else STARTING_MODEL model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, max_length=max_length) # label2id=label2id, id2label=id2label).to(device) if not p.exists(): model.save_pretrained(p) . print(type(model)) . &lt;class &#39;transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification&#39;&gt; . tok_ds = tok_ds.rename_columns({MAJOR_CODE_INT: &quot;labels&quot;}) . tok_ds = tok_ds.remove_columns([&#39;__index_level_0__&#39;]) # tok_ds = tok_ds.remove_columns([&#39;Description&#39;, &#39;__index_level_0__&#39;]) . # tok_ds.features[&#39;labels&#39;] = labels dds = tok_ds.train_test_split(test_size=0.25, seed=42) . class WeightedLossTrainer(Trainer): def compute_loss(self, model, inputs, return_outputs=False): # Feed inputs to model and extract logits outputs = model(**inputs) logits = outputs.get(&quot;logits&quot;) # Extract Labels labels = inputs.get(&quot;labels&quot;) # Define loss function with class weights loss_func = torch.nn.CrossEntropyLoss(weight=class_weights) # Compute loss loss = loss_func(logits, labels) return (loss, outputs) if return_outputs else loss . def compute_metrics(eval_pred): labels = eval_pred.label_ids predictions = eval_pred.predictions.argmax(-1) f1 = f1_score(labels, predictions, average=&quot;weighted&quot;) return {&quot;f1&quot;: f1} . output_dir = &quot;./hf_training&quot; batch_size = 64 # 128 causes a CUDA out of memory exception... Maybe I shoudl consider dynamic padding instead. Later. epochs = 3 # low, but for didactic purposes will do. lr = 8e-5 # inherited, no idea whether appropriate. is there an lr_find in hugging face? . training_args = TrainingArguments( output_dir=output_dir, num_train_epochs=epochs, learning_rate=lr, lr_scheduler_type=&quot;cosine&quot;, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size * 2, weight_decay=0.01, evaluation_strategy=&quot;epoch&quot;, logging_steps=len(dds[&quot;train&quot;]), fp16=True, push_to_hub=False, report_to=&quot;none&quot;, ) . model = model.to(&quot;cuda:0&quot;) . The above nay not be strictly necessary, depending on your version of transformers. I bumped into the following issue, which was probably the transformers 4.11.3 bug: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select) . trainer = Trainer( model=model, args=training_args, train_dataset=dds[&quot;train&quot;], eval_dataset=dds[&quot;test&quot;], tokenizer=tokz, compute_metrics=compute_metrics, ) . Using amp half precision backend . Training . trainer.train() . The following columns in the training set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. /home/per202/miniconda/envs/hf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 23185 Num Epochs = 3 Instantaneous batch size per device = 64 Total train batch size (w. parallel, distributed &amp; accumulation) = 64 Gradient Accumulation steps = 1 Total optimization steps = 1089 . . [1089/1089 04:57, Epoch 3/3] Epoch Training Loss Validation Loss F1 . 1 | No log | 0.072295 | 0.983439 | . 2 | No log | 0.063188 | 0.985492 | . 3 | No log | 0.061934 | 0.986534 | . &lt;/div&gt; &lt;/div&gt; The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 7729 Batch size = 128 Saving model checkpoint to ./hf_training/checkpoint-500 Configuration saved in ./hf_training/checkpoint-500/config.json Model weights saved in ./hf_training/checkpoint-500/pytorch_model.bin tokenizer config file saved in ./hf_training/checkpoint-500/tokenizer_config.json Special tokens file saved in ./hf_training/checkpoint-500/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 7729 Batch size = 128 Saving model checkpoint to ./hf_training/checkpoint-1000 Configuration saved in ./hf_training/checkpoint-1000/config.json Model weights saved in ./hf_training/checkpoint-1000/pytorch_model.bin tokenizer config file saved in ./hf_training/checkpoint-1000/tokenizer_config.json Special tokens file saved in ./hf_training/checkpoint-1000/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 7729 Batch size = 128 Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=1089, training_loss=0.16952454397938907, metrics={&#39;train_runtime&#39;: 297.9073, &#39;train_samples_per_second&#39;: 233.479, &#39;train_steps_per_second&#39;: 3.655, &#39;total_flos&#39;: 2304099629568000.0, &#39;train_loss&#39;: 0.16952454397938907, &#39;epoch&#39;: 3.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Exploring results . This part is newer compared to the previous post, so I will elaborate a bit. . I am not across the high level facilities to assess model predictions (visualisation, etc.) so what follows may be sub-optimal and idiosyncratic. . test_pred = trainer.predict(trainer.eval_dataset) . The following columns in the test set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Prediction ***** Num examples = 7729 Batch size = 128 . . [61/61 00:08] test_pred . PredictionOutput(predictions=array([[-0.519 , -2.127 , 0.61 , ..., -1.751 , -1.145 , -2.832 ], [ 0.8223, 2.123 , 9.27 , ..., -3.254 , -0.8325, -1.929 ], [-1.003 , -0.469 , -1.233 , ..., -1.084 , -0.7856, -0.4966], ..., [-0.53 , -1.396 , -0.5615, ..., -2.506 , -1.985 , -3.44 ], [-0.453 , -1.442 , -0.621 , ..., -2.424 , -1.973 , -3.44 ], [-1.388 , -2.346 , -1.186 , ..., -1.94 , -0.6084, -2.22 ]], dtype=float16), label_ids=array([4, 2, 8, ..., 5, 5, 6]), metrics={&#39;test_loss&#39;: 0.061934199184179306, &#39;test_f1&#39;: 0.9865336898918051, &#39;test_runtime&#39;: 8.7643, &#39;test_samples_per_second&#39;: 881.873, &#39;test_steps_per_second&#39;: 6.96}) . This is lower level than I anticipated. The predictions array appear to be the logits. Note that I was not sure label_ids was, and it is not the predicted label, but the &quot;true&quot; label. . test_df = trainer.eval_dataset.to_pandas() y_true = test_df.labels.values.astype(int) y_true . array([4, 2, 8, ..., 5, 5, 6]) . To get the predicted labels, I seem to need to do the following song and dance: . preds_tf = torch.asarray(test_pred.predictions, dtype=float) predictions = torch.nn.functional.softmax(preds_tf, dim=-1) highest = np.argmax(predictions, axis=1) y_pred = np.array(highest) y_pred . array([4, 2, 8, ..., 5, 5, 6]) . differ = np.logical_not(y_true == y_pred) print(&quot;There are {0} records in the validation data set that differ from true labels&quot;.format(np.sum(differ))) . There are 104 records in the validation data set that differ from true labels . Let&#39;s look at where we fail to match the labels: . differing = test_df[differ] . lbl_true = labels.int2str(differing.labels.values) descriptions = differing.Description.values lbl_pred = labels.int2str(y_pred[differ]) . pd.options.display.max_colwidth = 150 pd.options.display.max_rows = 110 . pd.DataFrame.from_dict({ &quot;label_true&quot;: lbl_true, &quot;label_pred&quot;: lbl_pred, &quot;desc&quot;: descriptions, }) . label_true label_pred desc . 0 TPSL | GRNT | topoil; granite, grey | . 1 TPSL | CLAY | none | . 2 SDSN | CLAY | none | . 3 SHLE | CLAY | clay multicoloured sandy | . 4 TPSL | CLAY | none | . 5 SAND | GRNT | granite sand | . 6 SDSN | CLAY | gray | . 7 TPSL | CLAY | none | . 8 CLAY | SDCY | sandy clay | . 9 SDSN | CLAY | none | . 10 TPSL | CLAY | clay - brown, silty | . 11 SDSN | CLAY | brown | . 12 CLAY | SHLE | grey | . 13 SLSN | SDSN | grey soft silstone | . 14 SAND | SDCY | sandy bands, brown | . 15 SDCY | CLAY | clay sandy | . 16 BSLT | CLAY | none | . 17 UNKN | CLAY | carbonaceous wood - dark bluish black to black with associated associated minor khaki and dark grey clay. sample retained for analysis | . 18 GRNT | SAND | grantie; ligh pinkish grey, medium, fragments of quartz, hornblende &amp; mica, increased pink feldspar | . 19 UNKN | CLAY | none | . 20 SDSN | CLAY | none | . 21 ROCK | UNKN | missing | . 22 SLSN | SDSN | silstone | . 23 CLAY | GRVL | light grey medium to coarse sandy gravel - 30%, and gravelly clay - 70%. gravel mainly basalt and jasper | . 24 CLAY | SDCY | sandy clay | . 25 SDSN | CLAY | none | . 26 GRVL | SAND | sand and gravel | . 27 SAND | SOIL | soil + sand | . 28 UNKN | CLAY | none | . 29 SHLE | CLAY | brown | . 30 CLAY | SAND | clayey sand (brown) - fine-medium | . 31 UNKN | CLAY | white puggy some slightly hard | . 32 GRNT | CLAY | none | . 33 SHLE | CLAY | none | . 34 BSLT | CLAY | none | . 35 GRVL | CLAY | none | . 36 SHLE | CLAY | none | . 37 CLAY | SDCY | sandy and gravel aquifer with bands of clay | . 38 SDCY | CLAY | clay sandy | . 39 CLAY | SDSN | silty | . 40 CLAY | SDCY | sandy clay, light grey, fine | . 41 BSLT | SHLE | blue bassalt | . 42 CLAY | SDCY | sandy brown clay | . 43 CLAY | SAND | sand - silty up to 1mm, clayey | . 44 SOIL | CLAY | none | . 45 GRVL | SAND | wash alluvial | . 46 CLAY | SDCY | sandy clay | . 47 GRNT | CLAY | none | . 48 UNKN | CLAY | none | . 49 SDSN | SAND | sand - mostly white very fine to very coarse gravel | . 50 GRVL | CLAY | gravelly sandy clay | . 51 SOIL | CLAY | none | . 52 BSLT | SDSN | brown weathered | . 53 GRVL | SAND | brown sand and fine gravel | . 54 GRVL | SAND | course sand and gravel, w/b | . 55 SDCY | GRNT | silt, sandy/silty sand | . 56 TPSL | BSLT | blue basalt | . 57 GRVL | CLAY | stones clay | . 58 ROCK | CLAY | ochrs yellow | . 59 GRVL | ROCK | stone, clayed to semi formed sandstone | . 60 UNKN | SAND | soak water bearing | . 61 BSLT | SDSN | balsalt: weathered | . 62 SOIL | CLAY | none | . 63 UNKN | CLAY | very | . 64 GRVL | CLAY | gravelly sandy clay | . 65 SDSN | GRNT | granite sand | . 66 SHLE | CLAY | none | . 67 ROCK | UNKN | water bearing | . 68 BSLT | SAND | h/frac, quartz | . 69 MDSN | COAL | coal 80% &amp; mudstone, 20%; dark grey, strong, carbonaceous | . 70 GRVL | CLAY | as above | . 71 CLAY | GRVL | gravelly clay | . 72 GRVL | SAND | sand + gravel (water) | . 73 CLAY | GRVL | with gravel | . 74 SAND | SDCY | sandy yellow | . 75 CLAY | SOIL | brown soil and clay | . 76 CLAY | SDCY | sandy clay | . 77 UNKN | CLAY | hard slightly stoney | . 78 SDSN | CLAY | none | . 79 SDSN | ROCK | sandsstone | . 80 TPSL | CLAY | none | . 81 SOIL | CLAY | none | . 82 SHLE | BSLT | shae (brown) | . 83 BSLT | CLAY | none | . 84 CLAY | SDCY | sandy clay | . 85 SAND | SOIL | surface soil | . 86 GRVL | CLAY | none | . 87 SAND | CLAY | none | . 88 SDSN | CLAY | none | . 89 CLAY | SHLE | grey | . 90 SDCY | CLAY | clay sandy water supply | . 91 GRVL | BSLT | blue/dark mixed | . 92 GRVL | SAND | sand + gravel + white clay | . 93 UNKN | SHLE | grey very hard | . 94 UNKN | CLAY | white fine, and clay, nodular | . 95 CLAY | SLSN | yellow clayey siltstone | . 96 SDSN | CLAY | none | . 97 SDSN | SAND | brown sand + stones (clean) | . 98 SDSN | CLAY | yellow | . 99 BSLT | UNKN | broken | . 100 CLAY | SDCY | sandy clay stringers | . 101 SAND | CLAY | none | . 102 SDSN | ROCK | bedrock - sandstone; whitish greyish blue, highly weathered, fine grains, angular to subangular, predominantly clear quartz. very small amounts of... | . 103 TPSL | CLAY | none | . Observations . The error rate is rather low for a first trial, though admittedly we know that many descriptions are fairly unambiguous. If we examine the failed predictions, we can make a few observations: . There are many none descriptions that are picked up as CLAY, but given that the true labels are not necessarily UNKN for these, one cannot complain too much about the model. The fact that some true labels are set to CLAY for these hints at the use of contextual information, perhaps nearby lithology log entries being classified as CLAY. | The model picks up several sandy clay as SDCY, which is a priori more suited than the true labels, at least without other information context explaining why the &quot;true&quot; classification ends up being another category such as CLAY | Typographical errors such as ssandstone are throwing the model off, which is extected. A production pipeline would need to have an orthographic correction step. | grammatically unusual expressions such as clay sandy and clayey/gravel brown are also a challenge for the model. | More nuanced descriptions such as light grey medium to coarse sandy gravel - 30%, and gravelly clay - 70%. gravel mainly basalt and jasper where a human reads that the major class is clay, not gravel, or broken rock is more akin to gravel than rock. | . Still, the confusion matrix is overall really encouraging. Let&#39;s have a look: . import seaborn as sns from matplotlib.ticker import FixedFormatter def plot_cm(y_true, y_pred, title, figsize=(10,10), labels=None): &#39;&#39;&#39;&#39; input y_true-Ground Truth Labels y_pred-Predicted Value of Model title-What Title to give to the confusion matrix Draws a Confusion Matrix for better understanding of how the model is working return None &#39;&#39;&#39; cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true)) cm_sum = np.sum(cm, axis=1, keepdims=True) cm_perc = cm / cm_sum.astype(float) * 100 annot = np.empty_like(cm).astype(str) nrows, ncols = cm.shape for i in range(nrows): for j in range(ncols): c = cm[i, j] p = cm_perc[i, j] if i == j: s = cm_sum[i] annot[i, j] = &#39;%.1f%% n%d/%d&#39; % (p, c, s) elif c == 0: annot[i, j] = &#39;&#39; else: annot[i, j] = &#39;%.1f%% n%d&#39; % (p, c) cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true)) cm.index.name = &#39;Actual&#39; cm.columns.name = &#39;Predicted&#39; fig, ax = plt.subplots(figsize=figsize) ff = FixedFormatter(labels) ax.yaxis.set_major_formatter(ff) ax.xaxis.set_major_formatter(ff) plt.title(title) sns.heatmap(cm, cmap= &quot;YlGnBu&quot;, annot=annot, fmt=&#39;&#39;, ax=ax) def roc_curve_plot(fpr,tpr,roc_auc): plt.figure() lw = 2 plt.plot(fpr, tpr, color=&#39;darkorange&#39;, lw=lw, label=&#39;ROC curve (area = %0.2f)&#39; %roc_auc) plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=lw, linestyle=&#39;--&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;Receiver operating characteristic example&#39;) plt.legend(loc=&quot;lower right&quot;) plt.show() . plot_cm(y_true, y_pred, title=&quot;Test set confusion matrix&quot;, figsize=(16,16), labels=labels.names) . /tmp/ipykernel_29992/2038836972.py:37: UserWarning: FixedFormatter should only be used together with FixedLocator ax.yaxis.set_major_formatter(ff) /tmp/ipykernel_29992/2038836972.py:38: UserWarning: FixedFormatter should only be used together with FixedLocator ax.xaxis.set_major_formatter(ff) . Conclusion, Next . Despite quite a few arbitrary shortcuts in the overall pipeline, we have a working template to fine-tune a pre-trained classification model to classify primary lithologies. . I&#39;ll probably have to pause on this work for a few weeks, though perhaps a teaser Gradio app on Hugging Face spaces in the same vein as this one is diable with relatively little work. . Appendix . # Later on, in another post, for predictions on the CPU: # model_cpu = model.to(&quot;cpu&quot;) # from transformers import TextClassificationPipeline # tokenizer = tokz # pipe = TextClassificationPipeline(model=model_cpu, tokenizer=tokenizer, return_all_scores=True) # # outputs a list of dicts like [[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.0001223755971295759}, {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998776316642761}]] # pipe(&quot;clayey sand&quot;) # raw_inputs = [ # &quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;, # &quot;I hate this so much!&quot;, # ] # inputs = tokz(raw_inputs, padding=True, truncation=True, return_tensors=&quot;pt&quot;) # print(inputs) # pipe(&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;) . &lt;/div&gt; .",
            "url": "https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/07/03/lithology-classification-hugging-face-3.html",
            "relUrl": "/hugging-face/nlp/lithology/2022/07/03/lithology-classification-hugging-face-3.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Upgrading a C++ streamflow compilation toolchain",
            "content": "Background . For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. It is a stack with a core in C++. For a variety of reasons (licencing, habits, inertia) it is still compiled with the microsoft VCPP2013 toolchain. The most recent versions of some of the third party dependencies are starting to not be compiling with fthat 10 year old compiler, making maintenance more difficult. . This is not a particularly popular topic, but I am posting to at least organise and plan a migration. . Current status . On Windows, we are relying on vcxproj files. A fair bit of work went into being able to manage switching configurations more easily than via horrendous hard coded paths, which is the baffling default behavior with these files, or at least, was. The supposedly “user-friendly” GUIs to manage project settings foster an unholly mess unless users are on a very tight leash. The Readme at vcpp-commons includes instructions on how to set things up, and how to use it from visual c++ project files. . There are tools in the microsoft ecosystem to manage dependencies and configurations that were not existing 10 years ago. Here is one, Conan, essentially doing what I put in place 10 years ago. There is also vcpkg which I looked at a few years back. Be it as it may, we have a legacy that worked for a while, so we are likely to keep using it. . We are using property files (.props files) to manage centrally some definitions, so that our vcxproj files look like: . &lt;ImportGroup Label=&quot;PropertySheets&quot;&gt; &lt;Import Project=&quot;$(UserRootDir) Microsoft.Cpp.$(Platform).user.props&quot; Condition=&quot;exists(&#39;$(UserRootDir) Microsoft.Cpp.$(Platform).user.props&#39;)&quot; Label=&quot;LocalAppDataPlatform&quot; /&gt; &lt;Import Project=&quot;$(UserProfile)/vcpp_config.props&quot; Condition=&quot;exists(&#39;$(UserProfile)/vcpp_config.props&#39;)&quot; /&gt; &lt;/ImportGroup&gt; &lt;PropertyGroup&gt; &lt;IncludePath&gt;../include;$(LocalIncludePaths);$(IncludePath)&lt;/IncludePath&gt; &lt;ReferencePath&gt;$(VisualLeakDetectorLibPath);$(LocalLibraryPaths);$(ReferencePath)&lt;/ReferencePath&gt; &lt;LibraryPath&gt;$(VisualLeakDetectorLibPath);$(LocalLibraryPaths);$(LibraryPath)&lt;/LibraryPath&gt; &lt;/PropertyGroup&gt; &lt;ItemDefinitionGroup&gt; &lt;Link&gt; &lt;AdditionalLibraryDirectories&gt;$(LocalLibraryPaths);%(AdditionalLibraryDirectories)&lt;/AdditionalLibraryDirectories&gt; &lt;AdditionalDependencies&gt;netcdf.lib;yaml-cpp.lib;moirai.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;!--AdditionalDependencies&gt;vld.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies--&gt; &lt;/Link&gt; &lt;/ItemDefinitionGroup&gt; . A priori this is independent of the version of compiler used. A priori. . &lt;PlatformToolset&gt;v120&lt;/PlatformToolset&gt; . So, is it just a matter of just bulk replacing to &lt;PlatformToolset&gt;v143&lt;/PlatformToolset&gt;? Even if considering only the purely mechanistic aspect (notwithstanding users), probably not. . Dependencies . The following figure gives an overview of the dependencies of the main DLLs. There are already several versions of the MS C runtimes, which is possible so long as libraries interact in a binary compatible way (C API). Usually, C++ level binary compatibility is not achievable. Never, in practice, so far as I recall. . . High level options . There are two main options to upgrade compilers . Give up on compiling using MS VCpp and migrate the MinGW toolchain (gcc). While it would be interesting to trial, the debugging facilities offered by the Microsoft tools and IDEs are a key value added for users. | Upgrade the compilation to a newer version of MS vc++. A quick scan of what is available out there for the dependencies of the stack (boost, netcdf, etc.) suggests that the build tool chain 2017 is prevalent. | . Related to that are 2 key dependencies: . netcdf: does this need to be stuck at msvcr100.dll (not that it is much of a problem) | boost: while boost is largely header only, it has a few libraries. These need to be brought to the same version of VCPP as that to compile our software, contrary to netcdf, because boost is C++, not C. | . Finally, some mostly orthogonal concerns . Move to use cmake to manage compilations, exactly as we do on Linux. | Set up a CI/CD for compilation on windows. | Automation of the migration to various versions of MSVCPP. | . Plan . The rest of this post will be a log of a “dry run” trying to migrate to MS build toolchain 2017 (vcpp v14.x to be determined). I’ll be capturing steps with a view to automate a CI/CD pipeline for windows soon, and also so that we can more easily adjust our target migration if I hit a blocker. . Walkthrough . I already have a recent visual studio installed via my corporate software management system. I do need to install the Microsoft build tools for visual studio 2017. I note two things in what options this selects: the compiler version (perhaps known as platform version) is v141, and Windows SDK 10.0.17763.0. . Boost . Read Boost 1.79.0 for windows. . Compile from source . I first tried to compile from source, out of curiosity, but this failed. For the record a summary follows. . Opening the Visual Studio 2017 Developer Command Prompt v15.0, then 7z x boost_1_79_0.7z. Note that running bootstrap indicated that it was Using &#39;vc143&#39; toolset so this may be an issue. . Then doing: . mkdir c: tmp boost b2.exe --prefix=c: tmp boost . c:/src/tmp/boost_1_79_0/tools/build/src/build targets.jam:617: in start-building from module targets error: Recursion in main target references error: the following target are being built currently: error: ./forward -&gt; ./stage -&gt; ./stage-proper -&gt; ***libs/filesystem/build/stage*** -&gt; libs/filesystem/build/stage-dependencies -&gt; libs/log/build/stage -&gt; libs/log/build/stage-dependencies -&gt; ***libs/filesystem/build/stage*** . Go for the prebuilt windows binaries then… . precompiled Boost binaries . https://www.boost.org/users/download/ leads to boost_1_79_0-msvc-14.1-64.exe on sourceforge . Copying a subset of the libraries: . set vcpp_ver=14.1 set boost_ver=1_79_0 set vcpp_ver_s=141 set src_root_dir=C: local boost_%boost_ver% set src_lib_dir=%src_root_dir% lib64-msvc-%vcpp_ver% set src_header_dir=%src_root_dir% boost set dest_root_dir=C: local set dest_dbg_root_dir=C: localdev set dest_lib_dir=%dest_root_dir% libs 64 set dest_dbg_lib_dir=%dest_dbg_root_dir% libs 64 set dest_header_dir=%dest_root_dir% include :: Cleanup or backup? mkdir %dest_lib_dir% set COPYOPTIONS=/Y /R /D xcopy %src_lib_dir% boost_chrono-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_date_time-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_filesystem-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_system-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_thread-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% boost_regex-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_chrono-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_date_time-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_filesystem-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_system-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_thread-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% xcopy %src_lib_dir% libboost_regex-vc%vcpp_ver_s%* %dest_lib_dir% %COPYOPTIONS% :: header files mkdir %dest_header_dir% mv %src_header_dir% %dest_header_dir% . Moirai . moirai is the workorse for reference counting opaque pointers. . Open project with ms vstudio 2019 or more does offer an upgrade of the project. There are options to upgrade the windows sdk to a couple of versions.including the 10.0.17763.0 we noted. Note that the Platform toolset option however does not include v141. . So, a manual modification of the .vcxproj file is needed (or via project properties from vsstudio after opening the project, it is available this time) . &lt;WindowsTargetPlatformVersion&gt;10.0.17763.0&lt;/WindowsTargetPlatformVersion&gt; &lt;PlatformToolset&gt;v141&lt;/PlatformToolset&gt; . And this appears to compile. . netCDF . Again may be interesting to compile from source, but not essential. netcdf windows binaries document. Appears to be compiled with vcpp 2017. . Downloading netCDF4.9.0-NC4-64.exe . It appears to run on top of the v140 platform tools.That’s OK since this is a C API. Try. May see if we can compile from source with v141 target later. . xcopy C: tmp netcdf bin *.dll %dest_lib_dir% %COPYOPTIONS% xcopy C: tmp netcdf lib *.lib %dest_lib_dir% %COPYOPTIONS% mkdir %dest_header_dir% netcdf xcopy C: tmp netcdf include *.h %dest_header_dir% netcdf %COPYOPTIONS% . jsoncpp . I have a fork of jsoncpp and a branch with a customised .vcxproj file. Modifying with: . &lt;PlatformToolset&gt;v141&lt;/PlatformToolset&gt; &lt;WindowsTargetPlatformVersion&gt;10.0.17763.0&lt;/WindowsTargetPlatformVersion&gt; . And this compiles fine. Note that without WindowsTargetPlatformVersion in the project file, this failed to compile. . Now, how do I automate the building of these? TODO perhaps streamline the powershell script to deal with jsoncpp too. Even if likely very infrequent. . set jsoncpp_srcdir=C: src jsoncpp set jsoncpp_builddir=%jsoncpp_srcdir% makefiles custom x64 xcopy %jsoncpp_builddir% Release jsoncpp.dll %dest_lib_dir% %COPYOPTIONS% xcopy %jsoncpp_builddir% Release jsoncpp.lib %dest_lib_dir% %COPYOPTIONS% :: Debug also needed, likely. Know of very odd crashes when mixing debug/nondebug in the past. May not be the case anymore. mkdir %dest_dbg_lib_dir% xcopy %jsoncpp_builddir% Debug jsoncpp.dll %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %jsoncpp_builddir% Debug jsoncpp.lib %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %jsoncpp_builddir% Debug jsoncpp.pdb %dest_dbg_lib_dir% %COPYOPTIONS% set robocopy_opt=/MIR /MT:1 /R:2 /NJS /NJH /NFL /NDL /XX robocopy %jsoncpp_srcdir% include json %dest_header_dir% json %robocopy_opt% . yaml-cpp . set yamlcpp_srcdir=C: src yaml-cpp set yamlcpp_builddir=%yamlcpp_srcdir% vsproj x64 xcopy %yamlcpp_builddir% Release yaml-cpp.dll %dest_lib_dir% %COPYOPTIONS% xcopy %yamlcpp_builddir% Release yaml-cpp.lib %dest_lib_dir% %COPYOPTIONS% :: Debug also needed, likely. Know of very odd crashes when mixing debug/nondebug in the past. May not be the case anymore. xcopy %yamlcpp_builddir% Debug yaml-cpp.dll %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %yamlcpp_builddir% Debug yaml-cpp.lib %dest_dbg_lib_dir% %COPYOPTIONS% xcopy %yamlcpp_builddir% Debug yaml-cpp.pdb %dest_dbg_lib_dir% %COPYOPTIONS% robocopy %yamlcpp_srcdir% include yaml-cpp %dest_header_dir% yaml-cpp %robocopy_opt% . uchronia / datatypes . Uchronia - time series handling for ensembles simulations and forecasts in C++ is the bedrock for data handling in our stack. . At this point I do need to copy the files for the catch unit testing framework, from config-utils, which the unit tests rely on. I notice also a bit of a minor issue, because the unit tests compile against the deployed headers, not the ones from source. This may be a gotcha to watch for, because my vcpp_settings.props file is not set up for development mode compilation. Beware when setting a build pipeline. . robocopy C: src config-utils catch include catch %dest_header_dir% catch %robocopy_opt% . The compilation and deploymebnt of this is relatively streamlined by using a high level powershell script. . swift . swift uses wila and its multithreading optimnisers. We need to install a complementary threadpool tool. . robocopy C: src threadpool boost %dest_header_dir% boost %robocopy_opt% . Also uses in-house numerical header-only files: . robocopy C: src numerical-sl-cpp algorithm include sfsl %dest_header_dir% sfsl %robocopy_opt% robocopy C: src numerical-sl-cpp math include sfsl %dest_header_dir% sfsl %robocopy_opt% . Error C2039 &#39;tuple&#39;: is not a member of &#39;boost::math&#39; (compiling source file channel_muskingum_nl.cpp) libswift c: src swift libswift include swift channel_muskingum_nl.h 74 . Seems I just an explicit additional #include &lt;boost/math/tools/tuple.hpp&gt; now with a more recent version of boost. . robocopy C: local include_old tclap %dest_header_dir% tclap %robocopy_opt% :: &lt;!-- and for fogss later on: --&gt; robocopy C: local include_old eigen3 %dest_header_dir% eigen3 %robocopy_opt% . Error C2079 &#39;outfile&#39; uses undefined class &#39;std::basic_ofstream&lt;char,std::char_traits&lt;char&gt;&gt;&#39; swiftcl c: src swift applications swiftcl run_calibration.cpp 120 . #include &lt;fstream&gt; is now needed. . After that, unit tests are mostly as expected, a few watchpoint that may be red herring. . Conclusion, and next . While it took a bit longer than ideal, so far, it looks rather straightforward and successful. Nothing was particularly risky a priori in this migration, but one is usually taken by a few surprises. . I am feeling the need for a more fully automated CI/CD. To be fair to myself, a fair bit of streamline already to build on. A challenge with the CI/CD, aside from some of the logistical cost of setting it up, probably on Azure Devops, is to define the artefacts it produces. Some artefacts the way they are currently would be better served as conda packages, but this is a larger scope. .",
            "url": "https://jmp75.github.io/work-blog/recipes/vcpp/c++/2022/06/26/vcpp-compilation-upgrade.html",
            "relUrl": "/recipes/vcpp/c++/2022/06/26/vcpp-compilation-upgrade.html",
            "date": " • Jun 26, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Setting up a private conda channel - part 1",
            "content": "Background . I have a conda package for a compiled native library (moirai). While it may be in a position to be submitted to conda-forge, I will also be interested at some point in setting up “my” own conda channel, with a view to distribute packages on my enterprise intranet. I may as well use that library moirai to test setting up a custom conda channel. . This post will be a review of the possible ways to do this. . Resources . conda documentation . The main conda documentation has sections on managing channels and creating custom channels. . fast.ai . Jeremy Howard has written the post “fastchan, a new conda mini-distribution”. It is a good read giving the rationale for setting up this channel. The repo of the conda channel ‘fastconda’ has some elements of pipelines but may be a good starting point for channels hosted by Anaconda. I am not sure I can repurpose this for a private channel. . Reusing conda-forge? . Since I intuit that a process similar to that of conda-forge could be what is emulated internally, an option may be to upfront borrow from the conda-forge documentation . The workhorse of conda-forge appears to be using conda-smithy to manage your CI, though that section actually rather confused me at the first read. The github readme of conda-smithy may be a better starting point. . https://github.com/conda-forge/astra-toolbox-feedstock is a recently accepted recipe. Likely a good template to study for the swift stack. . Commercial offerings . At an enterprise level it may be better to use Anaconda Server rather than cook up something. I probably cannot afford the time to trial standing one up, but my IM&amp;T business unit may look at it. In particular, if there are use cases for sophisticated user based access controls, “free” solutions may not be up to scratch nor to scale. | Anaconda cloud thingy | Channel on anaconda.org | . Other . How To: Set up a local Conda channel for installing the ArcGIS Python API | Building a Private Conda Channel | Setting up a feedstock from scratch | The github repo Private Conda Repository may be a good first step. | . By the way one misleading thing if you google “How do I set up a conda channel?”: the erronously titled video How to create new channel in Anaconda (python) is actually demonstrating the creation of a conda environment. Skip. . Stocktake . The first impression is that there is not a clear, single path to standing up your own conda channel over HTTP(S), no turn-key solution. The closest may be the github repo Private Conda Repository. . A first plan for the next steps, presumably next posts, are: . First, set up a file based channel following the conda documentation for creating custom channels. | Second, try to use PCR: Private Conda Repository. | .",
            "url": "https://jmp75.github.io/work-blog/conda/conda-channel/2022/06/18/conda-channels-prep-work.html",
            "relUrl": "/conda/conda-channel/2022/06/18/conda-channels-prep-work.html",
            "date": " • Jun 18, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Lithology classification using Hugging Face, part 2",
            "content": "About . This is a continuation of Lithology classification using Hugging Face, part 1. . We saw in the previous post that the Namoi lithology logs data had their primary (major) lithology mostly completed. A substantial proportion had the label None nevertheless, despite descriptions that looked like they would obviously lead to a categorisation. There were many labels, with a long-tailed frequency histogram. . The aim of this post is (was) to get a classification training happening. . Spoiler alert: it won&#39;t. Almost. . Rather than write a post after the fact pretending it was a totally smooth journey, the following walktrough deliberately keeps and highlights issues, albeit succinctly. Don&#39;t jump to the conclusion that we will not get there eventually, or that Hugging Face is not good. When you adapt prior work to your own use case, you will likely stumble, so this post will make you feel in good company. . Kernel installation . The previous post was about data exploration and used mostly facilities such as pandas, not any deep learning related material. This post will, so we need to install Hugging Face. I did bump into a couple of issues while trying to get an environment going. I will not give the full grubby details, but highlight upfront a couple of things: . Do create a new dedicated conda environment for your work with Hugging Face, even if you already have an environment with e.g. pytorch you&#39;d like to reuse. | The version 4.11.3 of HF transformers on the conda channel huggingface, at the time of writing, has a bug. You should install the packages from the conda-forge channel. | . In a nutshell, for Linux: . myenv=hf mamba create -n $myenv python=3.9 -c conda-forge mamba install -n $myenv --yes ipykernel matplotlib sentencepiece scikit-learn -c conda-forge mamba install -n $myenv --yes pytorch=1.11 -c pytorch -c nvidia -c conda-forge mamba install -n $myenv --yes torchvision torchaudio -c pytorch -c nvidia -c conda-forge mamba install -n $myenv --yes -c conda-forge datasets transformers conda activate $myenv python -m ipykernel install --user --name $myenv --display-name &quot;Hugging Face&quot; . and in Windows: . set myenv=hf mamba create -n %myenv% python=3.9 -c conda-forge mamba install -n %myenv% --yes ipykernel matplotlib sentencepiece scikit-learn -c conda-forge mamba install -n %myenv% --yes pytorch=1.11 -c pytorch -c nvidia -c conda-forge mamba install -n %myenv% --yes torchvision torchaudio -c pytorch -c nvidia -c conda-forge mamba install -n %myenv% --yes -c conda-forge datasets transformers conda activate %myenv% python -m ipykernel install --user --name %myenv% --display-name &quot;Hugging Face&quot; . Walkthrough . Let&#39;s get on with all the imports upfront (not obvious, mind you, but after the fact...) . import numpy as np import pandas as pd import torch from datasets import Dataset from transformers import AutoModelForSequenceClassification, AutoTokenizer from pathlib import Path from datasets import ClassLabel from transformers import TrainingArguments, Trainer from sklearn.metrics import f1_score from collections import Counter # Some column string identifiers MAJOR_CODE = &quot;MajorLithCode&quot; MAJOR_CODE_INT = &quot;MajorLithoCodeInt&quot; # We will create a numeric representation of labels, which is (I think?) required by HF. MINOR_CODE = &quot;MinorLithCode&quot; DESC = &quot;Description&quot; . /home/per202/miniconda/envs/hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm . fn = Path(&quot;~&quot;).expanduser() / &quot;data/ela/shp_namoi_river/NGIS_LithologyLog.csv&quot; litho_logs = pd.read_csv( fn, dtype={&quot;FromDepth&quot;: str, &quot;ToDepth&quot;: str, MAJOR_CODE: str, MINOR_CODE: str} ) # To avoid importing from the ela package, copy a couple of functions: # from ela.textproc import token_freq, plot_freq def token_freq(tokens, n_most_common=50): list_most_common = Counter(tokens).most_common(n_most_common) return pd.DataFrame(list_most_common, columns=[&quot;token&quot;, &quot;frequency&quot;]) def plot_freq(dataframe, y_log=False, x=&quot;token&quot;, figsize=(15, 10), fontsize=14): &quot;&quot;&quot;Plot a sorted histogram of work frequencies Args: dataframe (pandas dataframe): frequency of tokens, typically with colnames [&quot;token&quot;,&quot;frequency&quot;] y_log (bool): should there be a log scale on the y axis x (str): name of the columns with the tokens (i.e. words) figsize (tuple): fontsize (int): Returns: barplot: plot &quot;&quot;&quot; p = dataframe.plot.bar(x=x, figsize=figsize, fontsize=fontsize) if y_log: p.set_yscale(&quot;log&quot;, nonposy=&quot;clip&quot;) return p litho_classes = litho_logs[MAJOR_CODE].values df_most_common = token_freq(litho_classes, 50) plot_freq(df_most_common) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . Imbalanced data sets . From the histogram above, it is pretty clear that labels are also not uniform an we have a class imbalance. Remember to skim Lithology classification using Hugging Face, part 1 for the initial data exploration if you have not done so already. . For the sake of the exercise in this post, I will reduce arbitrarily the number of labels used in this post, by just &quot;forgetting&quot; the less represented classes. . There are many resources about class imbalances. One of them is 8 Tactics to combat imbalanced classes in your machine learning dataset . Let&#39;s see what labels we may want to keep for this post: . def sample_desc_for_code(major_code, n=50, seed=None): is_code = litho_logs[MAJOR_CODE] == major_code coded = litho_logs.loc[is_code][DESC] if seed is not None: np.random.seed(seed) return coded.sample(n=50) . sample_desc_for_code(&quot;UNKN&quot;, seed=123) . 134145 (UNKNOWN), NO SAMPLE COLLECTED DUE TO WATER LOSS 134715 (UNKNOWN); COULD NOT BE LOGGED BECAUSE NO CUTT... 122303 GREY SHALEY 133856 NOMINAL 134378 None 133542 DRILLER 122258 WATER BEARING 127916 WATER SUPPLY 133676 DRILLER 134399 DRILLER 134052 DRILLER 128031 VERY SANDY STONES SOME LARGE 134140 SAMPLE MISSING 122282 REDDISH YELLOW VOLCANIC 133623 WHITE CRYSTALLINE 134505 MISSING 133694 DRILLER 133585 DRILLER 134201 MISSING 134627 NO DATA 133816 DRILLER 133893 DRILLER 134232 DRILLER 133687 DRILLER 133871 DRILLER 133698 DRILLER 134752 MISSING 128077 WATER BEARING WATER SUPPLY 122253 WATER SUPPLY 133607 DRILLER 133617 DRILLER 133643 HARD 134526 (UNKNOWN) CORE LOSS 133709 SANDY STREAKS 123254 NOMINAL WATER SUPPLY 122219 WATER SUPPLY 133525 DRILLER 127799 WATER SUPPLY 133940 DRILLER 124775 (UNKNOWN) WATER BEARING 126814 (UNKNOWN); WATER BEARING 133965 DRILLER 134074 DRILLER 134395 DRILLER 133970 DRILLER 134262 DRILLER 122407 WATER SUPPLY 144370 S/S LT BR 125023 (UNKNOWN); WATER BEARING 133675 DRILLER Name: Description, dtype: object . The &quot;unknown&quot; category is rather interesting in fact, and worth keeping as a valid class. . Subsetting . Let&#39;s keep &quot;only&quot; the main labels, for the sake of this exercise. We will remove None however, despite its potential interest. We will (hopefully) revisit this in another post. . labels_kept = df_most_common[&quot;token&quot;][:17].values # 17 first classes somewhat arbitraty labels_kept = labels_kept[labels_kept != &quot;None&quot;] labels_kept . array([&#39;CLAY&#39;, &#39;GRVL&#39;, &#39;SAND&#39;, &#39;SHLE&#39;, &#39;SDSN&#39;, &#39;BSLT&#39;, &#39;TPSL&#39;, &#39;SOIL&#39;, &#39;ROCK&#39;, &#39;GRNT&#39;, &#39;SDCY&#39;, &#39;SLSN&#39;, &#39;CGLM&#39;, &#39;MDSN&#39;, &#39;UNKN&#39;, &#39;COAL&#39;], dtype=object) . kept = [x in labels_kept for x in litho_classes] litho_logs_kept = litho_logs[kept].copy() # avoid warning messages down the track. litho_logs_kept.sample(10) . OBJECTID BoreID HydroCode RefElev RefElevDesc FromDepth ToDepth TopElev BottomElev MajorLithCode MinorLithCode Description Source LogType OgcFidTemp . 70655 526412 | 10072593 | GW031851.1.1 | None | UNK | 53.94 | 59.13 | None | None | CLAY | NaN | CLAY SANDY | UNK | 1 | 9308381 | . 7173 64072 | 10043001 | GW001815.1.1 | None | UNK | 31.39 | 44.5 | None | None | SHLE | NaN | SHALE | UNK | 1 | 8732384 | . 30076 197788 | 10152523 | GW099036.1.1 | None | UNK | 181.0 | 228.0 | None | None | SHLE | NaN | SHALE: GREY, FINE | UNK | 1 | 8870150 | . 93967 701859 | 10105392 | GW031140.1.1 | None | UNK | 0.0 | 8.84 | None | None | SOIL | NaN | SOIL CLAY | UNK | 1 | 9327759 | . 115538 803595 | 10099300 | GW970770.1.1 | None | UNK | 36.6 | 38.1 | None | None | SAND | NaN | SAND; FINE TO COARSE, BROWN | UNK | 1 | 9435886 | . 107173 762000 | 10122945 | GW018629.1.1 | None | UNK | 72.54 | 74.37 | None | None | SDSN | NaN | SANDSTONE YELLOW HARD | UNK | 1 | 9389679 | . 106769 760370 | 10111007 | GW026576.1.1 | None | UNK | 65.23 | 71.32 | None | None | SDSN | NaN | SANDSTONE WATER SUPPLY | UNK | 1 | 9388007 | . 13553 114744 | 10116235 | GW022175.1.1 | None | UNK | 37.8 | 39.01 | None | None | GRVL | NaN | GRAVEL FINE-COARSE | UNK | 1 | 8784472 | . 142398 971715 | 10074454 | GW901230.1.1 | None | UNK | 20.0 | 24.0 | None | None | GRVL | NaN | GRAVEL | UNK | 1 | 9567221 | . 9664 85061 | 10043586 | GW011521.1.1 | None | UNK | 12.19 | 20.73 | None | None | CLAY | NaN | CLAY YELLOW GRAVEL | UNK | 1 | 8753973 | . labels = ClassLabel(names=labels_kept) int_labels = np.array([ labels.str2int(x) for x in litho_logs_kept[MAJOR_CODE].values ]) int_labels = int_labels.astype(np.int8) # to mimick chapter3 HF so far as I can see . litho_logs_kept[MAJOR_CODE_INT] = int_labels . Class imbalance . Even our subset of 16 classes is rather imbalanced; the number of &quot;clay&quot; labels is looking more than 30 times that of &quot;coal&quot; just by eyeballing. . The post by Jason Brownlee 8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset, outlines several approaches. One of them is to resample from labels, perhaps with replacement, to equalise classes. It is a relatively easy approach to implement, but there are issues, growing with the level of imbalance. Notably, if too many rows from underrepresented classes are repeated, there is an increased tendency to overfitting at training. . The video Simple Training with the 🤗 Transformers Trainer (at 669 seconds) also explains the issues with imbalances and crude resampling. It offers instead a solution with class weighting that is more robust. That approach is evoked in Jason&#39;s post, but the video has a &quot;Hugging Face style&quot; implementation ready to repurpose. . Resample with replacement . Just for information, what we&#39;d do with a relatively crude resampling may be: . def sample_major_lithocode(dframe, code, n=10000, seed=None): x = dframe[dframe[MAJOR_CODE] == code] replace = n &gt; len(x) return x.sample(n=n, replace=replace, random_state=seed) . sample_major_lithocode(litho_logs_kept, &quot;CLAY&quot;, n=10, seed=0) . OBJECTID BoreID HydroCode RefElev RefElevDesc FromDepth ToDepth TopElev BottomElev MajorLithCode MinorLithCode Description Source LogType OgcFidTemp MajorLithoCodeInt . 106742 760246 | 10144429 | GW030307.1.1 | 279.5 | NGS | 54.3 | 72.2 | 225.2 | 207.3 | CLAY | NaN | CLAY LIGHT BROWN GRAVEL | UNK | 1 | 9387877 | 0 | . 138850 950521 | 10147004 | GW036015.2.2 | 236.0 | NGS | 73.15 | 74.676 | 162.85 | 161.324 | CLAY | NaN | CLAY; AS ABOVE, MORE MICACEOUS &amp; FINE GRAVEL (... | ?? - WC&amp;IC | 2 | 9543085 | 0 | . 30006 197243 | 10049338 | GW062392.1.1 | None | UNK | 63.0 | 64.0 | None | None | CLAY | NaN | CLAY SANDY | UNK | 1 | 8869540 | 0 | . 3225 29304 | 10142901 | GW014623.1.1 | None | UNK | 22.86 | 23.47 | None | None | CLAY | NaN | CLAY SANDY | UNK | 1 | 8696556 | 0 | . 9795 86262 | 10121680 | GW009977.1.1 | None | UNK | 39.01 | 42.67 | None | None | CLAY | NaN | CLAY YELLOW PUGGY | UNK | 1 | 8755205 | 0 | . 49588 427460 | 10067562 | GW964964.1.1 | None | UNK | 11.0 | 14.0 | None | None | CLAY | NaN | CLAY | UNK | 1 | 9199868 | 0 | . 136116 943202 | 10055892 | GW971627.1.1 | None | UNK | 14.0 | 20.0 | None | None | CLAY | NaN | GREY WET CLAY | UNK | 1 | 9534634 | 0 | . 5723 50788 | 10049974 | GW010017.1.1 | None | UNK | 14.02 | 24.38 | None | None | CLAY | NaN | CLAY RED SANDY | UNK | 1 | 8718677 | 0 | . 94938 706287 | 10018922 | GW022845.1.1 | None | UNK | 1.22 | 11.58 | None | None | CLAY | NaN | CLAY | UNK | 1 | 9332267 | 0 | . 38277 287347 | 10132392 | GW042735.1.1 | None | UNK | 0.75 | 6.0 | None | None | CLAY | NaN | CLAY | UNK | 1 | 8942094 | 0 | . balanced_litho_logs = [ sample_major_lithocode(litho_logs_kept, code, n=10000, seed=0) for code in labels_kept ] balanced_litho_logs = pd.concat(balanced_litho_logs) balanced_litho_logs.head() . OBJECTID BoreID HydroCode RefElev RefElevDesc FromDepth ToDepth TopElev BottomElev MajorLithCode MinorLithCode Description Source LogType OgcFidTemp MajorLithoCodeInt . 106742 760246 | 10144429 | GW030307.1.1 | 279.5 | NGS | 54.3 | 72.2 | 225.2 | 207.3 | CLAY | NaN | CLAY LIGHT BROWN GRAVEL | UNK | 1 | 9387877 | 0 | . 138850 950521 | 10147004 | GW036015.2.2 | 236.0 | NGS | 73.15 | 74.676 | 162.85 | 161.324 | CLAY | NaN | CLAY; AS ABOVE, MORE MICACEOUS &amp; FINE GRAVEL (... | ?? - WC&amp;IC | 2 | 9543085 | 0 | . 30006 197243 | 10049338 | GW062392.1.1 | None | UNK | 63.0 | 64.0 | None | None | CLAY | NaN | CLAY SANDY | UNK | 1 | 8869540 | 0 | . 3225 29304 | 10142901 | GW014623.1.1 | None | UNK | 22.86 | 23.47 | None | None | CLAY | NaN | CLAY SANDY | UNK | 1 | 8696556 | 0 | . 9795 86262 | 10121680 | GW009977.1.1 | None | UNK | 39.01 | 42.67 | None | None | CLAY | NaN | CLAY YELLOW PUGGY | UNK | 1 | 8755205 | 0 | . plot_freq(token_freq(balanced_litho_logs[MAJOR_CODE].values, 50)) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . Dealing with imbalanced classes with weights . Instead of the resampling above, we adapt the approach creating weights for the Trainer we will run. . sorted_counts = litho_logs_kept[MAJOR_CODE].value_counts() sorted_counts . CLAY 43526 GRVL 15824 SAND 15317 SHLE 10158 SDSN 9199 BSLT 7894 TPSL 5300 SOIL 4347 ROCK 2549 GRNT 1852 SDCY 1643 SLSN 1443 CGLM 1233 MDSN 1207 UNKN 1125 COAL 1040 Name: MajorLithCode, dtype: int64 . sorted_counts / sorted_counts.sum() . CLAY 0.351990 GRVL 0.127967 SAND 0.123867 SHLE 0.082147 SDSN 0.074391 BSLT 0.063838 TPSL 0.042860 SOIL 0.035154 ROCK 0.020613 GRNT 0.014977 SDCY 0.013287 SLSN 0.011669 CGLM 0.009971 MDSN 0.009761 UNKN 0.009098 COAL 0.008410 Name: MajorLithCode, dtype: float64 . class_weights = (1 - sorted_counts / sorted_counts.sum()).values class_weights . array([0.64801022, 0.87203312, 0.87613317, 0.91785342, 0.92560874, 0.93616213, 0.95713951, 0.96484631, 0.97938653, 0.98502309, 0.98671325, 0.98833062, 0.99002887, 0.99023913, 0.99090225, 0.99158964]) . We check that cuda is available (of course optional) . assert torch.cuda.is_available() . On Linux if you have a DELL laptop with an NVIDIA card, but nvidia-smi returns: NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running, you may need to change your kernel specification file $HOME/.local/share/jupyter/kernels/hf/kernel.json. This behavior seems to depend on the version of Linux kernel you have. It certainly changed out of the blue for me from yesterday, despite no change that I can tell. . optirun nvidia-smi returning a proper graphic card report should be a telltale sign you have to update your kernel.json like so: . { &quot;argv&quot;: [ &quot;optirun&quot;, &quot;/home/your_ident/miniconda/envs/hf/bin/python&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;Hugging Face&quot;, &quot;language&quot;: &quot;python&quot;, &quot;metadata&quot;: { &quot;debugger&quot;: true } } . You may need to restart jupyter-lab, or visual studio code, etc., for change to take effect. Restarting the kernel may not be enough, conter-intuitively. . Background details about optirun architecture at [Bumblebee Debian]https://wiki.debian.org/Bumblebee . class_weights = torch.from_numpy(class_weights).float().to(&quot;cuda&quot;) class_weights . tensor([0.6480, 0.8720, 0.8761, 0.9179, 0.9256, 0.9362, 0.9571, 0.9648, 0.9794, 0.9850, 0.9867, 0.9883, 0.9900, 0.9902, 0.9909, 0.9916], device=&#39;cuda:0&#39;) . model_nm = &quot;microsoft/deberta-v3-small&quot; . Tokenisation . Bump on the road; download operations taking too long . At this point I spent more hours than I wish I had on an issue, perhaps very unusual. . The operation tokz = AutoTokenizer.from_pretrained(model_nm) was taking an awful long time to complete: . CPU times: user 504 ms, sys: 57.9 ms, total: 562 ms Wall time: 14min 13s . To cut a long story short, I managed to figure out what was going on. It is documented on the Hugging Face forum at: Some HF operations take an excessively long time to complete. If you have issues where HF operations take a long time, read it. . Now back to the tokenisation story. Note that the local caching may be superflous if you do not encounter the issue just mentioned. . max_length = 128 . p = Path(&quot;./tokz_pretrained&quot;) pretrained_model_name_or_path = p if p.exists() else model_nm # https://discuss.huggingface.co/t/sentence-transformers-paraphrase-minilm-fine-tuning-error/9612/4 tokz = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=True, max_length=max_length, model_max_length=max_length) if not p.exists(): tokz.save_pretrained(&quot;./tokz_pretrained&quot;) . Let&#39;s see what this does on a typical lithology description . tokz.tokenize(&quot;CLAY, VERY SANDY&quot;) . [&#39;▁C&#39;, &#39;LAY&#39;, &#39;,&#39;, &#39;▁VERY&#39;, &#39;▁S&#39;, &#39;ANDY&#39;] . Well, the vocabulary is probably case sensitive and all the descriptions being uppercase in the source data are likely problematic. Let&#39;s check what happens on lowercase descriptions: . tokz.tokenize(&quot;clay, very sandy&quot;) . [&#39;▁clay&#39;, &#39;,&#39;, &#39;▁very&#39;, &#39;▁sandy&#39;] . This looks better. So let&#39;s change the descriptions to lowercase; we are not loosing any relevent information in this case, I think. . litho_logs_kept[DESC] = litho_logs_kept[DESC].str.lower() . litho_logs_kept_mini = litho_logs_kept[[MAJOR_CODE_INT, DESC]] litho_logs_kept_mini.sample(n=10) . MajorLithoCodeInt Description . 8256 5 | basalt | . 96820 4 | sandstone | . 36776 2 | sand | . 110231 0 | clay; light brown, very silty | . 80270 1 | gravel &amp; large stones | . 17592 1 | gravel water supply | . 74437 0 | clay | . 22904 5 | basalt stones | . 71578 1 | gravel very clayey water supply | . 73030 3 | shale | . Create dataset and tokenisation . We want to create a dataset such that tokenised data is of uniform shape (better for running on GPU) Applying the technique in this segment of the HF course video. Cheating a bit on guessing the length (I know from offline checks that max is 90 tokens) . ds = Dataset.from_pandas(litho_logs_kept_mini) def tok_func(x): return tokz( x[DESC], padding=&quot;max_length&quot;, truncation=True, max_length=max_length, return_tensors=&quot;pt&quot;, ) . The Youtube video above suggests to use tok_ds = ds.map(tok_func, batched=True) for a faster execution; however I ended up with the foollowing error: . TypeError: Provided `function` which is applied to all elements of table returns a `dict` of types [&lt;class &#39;torch.Tensor&#39;&gt;, &lt;class &#39;torch.Tensor&#39;&gt;, &lt;class &#39;torch.Tensor&#39;&gt;]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(&lt;class &#39;list&#39;&gt;, &lt;class &#39;numpy.ndarray&#39;&gt;)`. . The following non-batched option works in a reasonable time: . tok_ds = ds.map(tok_func) . Parameter &#39;function&#39;=&lt;function tok_func at 0x7f0d047695e0&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn&#39;t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won&#39;t be showed. 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 123657/123657 [00:24&lt;00:00, 4962.06ex/s] . tok_ds_tmp = tok_ds[:5] tok_ds_tmp.keys() . dict_keys([&#39;MajorLithoCodeInt&#39;, &#39;Description&#39;, &#39;__index_level_0__&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;]) . len(tok_ds_tmp[&quot;input_ids&quot;][0][0]) . 128 . num_labels = len(labels_kept) . p = Path(&quot;./model_pretrained&quot;) model_name = p if p.exists() else model_nm model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, max_length=max_length) # label2id=label2id, id2label=id2label).to(device) if not p.exists(): model.save_pretrained(p) . print(type(model)) . &lt;class &#39;transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification&#39;&gt; . # litho_desc_list = [x for x in litho_logs_kept_mini[DESC].values] # input_descriptions = tokz(litho_desc_list, padding=True, truncation=True, max_length=256, return_tensors=&#39;pt&#39;) # input_descriptions[&#39;input_ids&#39;].shape # model(input_descriptions[&#39;input_ids&#39;][:5,:], attention_mask=input_descriptions[&#39;attention_mask&#39;][:5,:]).logits . tok_ds . Dataset({ features: [&#39;MajorLithoCodeInt&#39;, &#39;Description&#39;, &#39;__index_level_0__&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 123657 }) . Transformers always assumes that your labels has the column name &quot;labels&quot;. Odd, but at least this fosters a consistent system, so why not: . tok_ds = tok_ds.rename_columns({MAJOR_CODE_INT: &quot;labels&quot;}) . tok_ds = tok_ds.remove_columns([&#39;Description&#39;, &#39;__index_level_0__&#39;]) . # Note that HF is supposed to take care of movind data to the GPU if available, so you should not ahve to manually copy the data to the GPU device tok_ds.set_format(&quot;torch&quot;) . # evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, # num_train_epochs=epochs, weight_decay=0.01, report_to=&#39;none&#39;) . dds = tok_ds.train_test_split(0.25, seed=42) . dds.keys() . dict_keys([&#39;train&#39;, &#39;test&#39;]) . tok_ds.features[&#39;labels&#39;] = labels . tok_ds.features # TODO: # This differs from chapter3 of HF course https://huggingface.co/course/chapter3/4?fw=pt # {&#39;attention_mask&#39;: Sequence(feature=Value(dtype=&#39;int8&#39;, id=None), length=-1, id=None), # &#39;input_ids&#39;: Sequence(feature=Value(dtype=&#39;int32&#39;, id=None), length=-1, id=None), # &#39;labels&#39;: ClassLabel(num_classes=2, names=[&#39;not_equivalent&#39;, &#39;equivalent&#39;], id=None), # &#39;token_type_ids&#39;: Sequence(feature=Value(dtype=&#39;int8&#39;, id=None), length=-1, id=None)} . {&#39;labels&#39;: ClassLabel(num_classes=16, names=array([&#39;CLAY&#39;, &#39;GRVL&#39;, &#39;SAND&#39;, &#39;SHLE&#39;, &#39;SDSN&#39;, &#39;BSLT&#39;, &#39;TPSL&#39;, &#39;SOIL&#39;, &#39;ROCK&#39;, &#39;GRNT&#39;, &#39;SDCY&#39;, &#39;SLSN&#39;, &#39;CGLM&#39;, &#39;MDSN&#39;, &#39;UNKN&#39;, &#39;COAL&#39;], dtype=object), id=None), &#39;input_ids&#39;: Sequence(feature=Sequence(feature=Value(dtype=&#39;int32&#39;, id=None), length=-1, id=None), length=-1, id=None), &#39;token_type_ids&#39;: Sequence(feature=Sequence(feature=Value(dtype=&#39;int8&#39;, id=None), length=-1, id=None), length=-1, id=None), &#39;attention_mask&#39;: Sequence(feature=Sequence(feature=Value(dtype=&#39;int8&#39;, id=None), length=-1, id=None), length=-1, id=None)} . tok_ds[&#39;input_ids&#39;][0] . [tensor([ 1, 3592, 14432, 8076, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])] . . # def compute_metrics(eval_pred): # logits, labels = eval_pred # predictions = np.argmax(logits, axis=-1) # return metric.compute(predictions=predictions, references=labels) . class WeightedLossTrainer(Trainer): def compute_loss(self, model, inputs, return_outputs=False): # Feed inputs to model and extract logits outputs = model(**inputs) logits = outputs.get(&quot;logits&quot;) # Extract Labels labels = inputs.get(&quot;labels&quot;) # Define loss function with class weights loss_func = torch.nn.CrossEntropyLoss(weight=class_weights) # Compute loss loss = loss_func(logits, labels) return (loss, outputs) if return_outputs else loss . def compute_metrics(eval_pred): labels = eval_pred.label_ids predictions = eval_pred.predictions.argmax(-1) f1 = f1_score(labels, predictions, average=&quot;weighted&quot;) return {&quot;f1&quot;: f1} . output_dir = &quot;./hf_training&quot; batch_size = 64 # 128 epochs = 5 lr = 8e-5 . training_args = TrainingArguments( output_dir=output_dir, num_train_epochs=epochs, learning_rate=lr, lr_scheduler_type=&quot;cosine&quot;, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size * 2, weight_decay=0.01, evaluation_strategy=&quot;epoch&quot;, logging_steps=len(dds[&quot;train&quot;]), fp16=True, push_to_hub=False, report_to=&quot;none&quot;, ) . model = model.to(&quot;cuda:0&quot;) . The above nay not be strictly necessary, depending on your version of transformers. I bumped into the following issue, which was probably the transformers 4.11.3 bug: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select) . trainer = Trainer( model=model, args=training_args, train_dataset=dds[&quot;train&quot;], eval_dataset=dds[&quot;test&quot;], tokenizer=tokz, compute_metrics=compute_metrics, ) . Using amp half precision backend . Training? . You did read the introduction and its spoiler alert, right? . trainer.train() . /home/per202/miniconda/envs/hf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 92742 Num Epochs = 5 Instantaneous batch size per device = 64 Total train batch size (w. parallel, distributed &amp; accumulation) = 64 Gradient Accumulation steps = 1 Total optimization steps = 7250 . RuntimeError Traceback (most recent call last) Input In [51], in &lt;cell line: 1&gt;() -&gt; 1 trainer.train() File ~/miniconda/envs/hf/lib/python3.9/site-packages/transformers/trainer.py:1317, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs) 1312 self.model_wrapped = self.model 1314 inner_training_loop = find_executable_batch_size( 1315 self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size 1316 ) -&gt; 1317 return inner_training_loop( 1318 args=args, 1319 resume_from_checkpoint=resume_from_checkpoint, 1320 trial=trial, 1321 ignore_keys_for_eval=ignore_keys_for_eval, 1322 ) File ~/miniconda/envs/hf/lib/python3.9/site-packages/transformers/trainer.py:1554, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval) 1552 tr_loss_step = self.training_step(model, inputs) 1553 else: -&gt; 1554 tr_loss_step = self.training_step(model, inputs) 1556 if ( 1557 args.logging_nan_inf_filter 1558 and not is_torch_tpu_available() 1559 and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step)) 1560 ): 1561 # if loss is nan or inf simply add the average of previous logged losses 1562 tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged) File ~/miniconda/envs/hf/lib/python3.9/site-packages/transformers/trainer.py:2183, in Trainer.training_step(self, model, inputs) 2180 return loss_mb.reduce_mean().detach().to(self.args.device) 2182 with self.autocast_smart_context_manager(): -&gt; 2183 loss = self.compute_loss(model, inputs) 2185 if self.args.n_gpu &gt; 1: 2186 loss = loss.mean() # mean() to average on multi-gpu parallel training File ~/miniconda/envs/hf/lib/python3.9/site-packages/transformers/trainer.py:2215, in Trainer.compute_loss(self, model, inputs, return_outputs) 2213 else: 2214 labels = None -&gt; 2215 outputs = model(**inputs) 2216 # Save past state if it exists 2217 # TODO: this needs to be fixed and made cleaner later. 2218 if self.args.past_index &gt;= 0: File ~/miniconda/envs/hf/lib/python3.9/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs) 1106 # If we don&#39;t have any hooks, we want to skip the rest of the logic in 1107 # this function, and just call forward. 1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1109 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1110 return forward_call(*input, **kwargs) 1111 # Do not call functions when jit is used 1112 full_backward_hooks, non_full_backward_hooks = [], [] File ~/miniconda/envs/hf/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1279, in DebertaV2ForSequenceClassification.forward(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict) 1271 r&#34;&#34;&#34; 1272 labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*): 1273 Labels for computing the sequence classification/regression loss. Indices should be in `[0, ..., 1274 config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If 1275 `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy). 1276 &#34;&#34;&#34; 1277 return_dict = return_dict if return_dict is not None else self.config.use_return_dict -&gt; 1279 outputs = self.deberta( 1280 input_ids, 1281 token_type_ids=token_type_ids, 1282 attention_mask=attention_mask, 1283 position_ids=position_ids, 1284 inputs_embeds=inputs_embeds, 1285 output_attentions=output_attentions, 1286 output_hidden_states=output_hidden_states, 1287 return_dict=return_dict, 1288 ) 1290 encoder_layer = outputs[0] 1291 pooled_output = self.pooler(encoder_layer) File ~/miniconda/envs/hf/lib/python3.9/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs) 1106 # If we don&#39;t have any hooks, we want to skip the rest of the logic in 1107 # this function, and just call forward. 1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1109 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1110 return forward_call(*input, **kwargs) 1111 # Do not call functions when jit is used 1112 full_backward_hooks, non_full_backward_hooks = [], [] File ~/miniconda/envs/hf/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1042, in DebertaV2Model.forward(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict) 1039 if token_type_ids is None: 1040 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device) -&gt; 1042 embedding_output = self.embeddings( 1043 input_ids=input_ids, 1044 token_type_ids=token_type_ids, 1045 position_ids=position_ids, 1046 mask=attention_mask, 1047 inputs_embeds=inputs_embeds, 1048 ) 1050 encoder_outputs = self.encoder( 1051 embedding_output, 1052 attention_mask, (...) 1055 return_dict=return_dict, 1056 ) 1057 encoded_layers = encoder_outputs[1] File ~/miniconda/envs/hf/lib/python3.9/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs) 1106 # If we don&#39;t have any hooks, we want to skip the rest of the logic in 1107 # this function, and just call forward. 1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1109 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1110 return forward_call(*input, **kwargs) 1111 # Do not call functions when jit is used 1112 full_backward_hooks, non_full_backward_hooks = [], [] File ~/miniconda/envs/hf/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:875, in DebertaV2Embeddings.forward(self, input_ids, token_type_ids, position_ids, mask, inputs_embeds) 872 mask = mask.unsqueeze(2) 873 mask = mask.to(embeddings.dtype) --&gt; 875 embeddings = embeddings * mask 877 embeddings = self.dropout(embeddings) 878 return embeddings RuntimeError: The size of tensor a (768) must match the size of tensor b (128) at non-singleton dimension 3 . Stocktake and conclusion . So, as announced at the start of this post, we hit a pothole in our journey. . RuntimeError: The size of tensor a (768) must match the size of tensor b (128) at non-singleton dimension 3 . Where the number (768) comes from is a bit of a mystery. I gather from Googling that this may have to do with the embedding of the Deberta model we are trying to fine tune, but I may be off the mark. . It is probably something at which an experience NLP practitioner will roll their eyes. . That&#39;s OK, We&#39;ll get there. .",
            "url": "https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html",
            "relUrl": "/hugging-face/nlp/lithology/2022/06/13/lithology-classification-hugging-face-2.html",
            "date": " • Jun 13, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Conda package for compiled native libraries",
            "content": "Background . I recently wrote about submitting your first conda package to conda-forge. You’ll find background on the “bigger picture” endeavour in that previous post. . This post is a follow-on with, perhaps, a second submission to conda-forge, this time not of a python package but a C++ library. . I’ll try to package a relatively small C++ codebase MOIRAI: Manage C++ objects lifetime when exposed through a C API. While dealing with very similar needs as refcount (reference counting and memory management), there is no explicit dependency between them. . Market review . moirai grew out of specific projects almost a decade ago, but its inception did not occur without looking first at third party options. There were surprisingly few I could identify, and of the ones I saw licensing or design made it difficult to adopt as they were. Still, in 2022 is there, on conda-forge or not, something making moirai possibly redundant? . It can be tricky to find relevant work without a time consuming research. A cursory scan comes up: . Cppy seems to have intersects, but this is solely Python-centric. | Loki-lib is an (underappreciated) library with reference counting features, but not on conda-forge. I believe Loki-lib was largely written by Andrei Alexandrescu, author of Modern C++ Design, one of the most impressive computer science book I read. | . Maybe there is a place for moirai. Plus the name is not taken… . Walkthrough . I already forked and cloned staged-recipes from the previous post. . cd ~/src/staged-recipes git checkout main git branch moirai . Starting point . grayskull is userful to generate meta.yaml stubs out of python packages, and not applicable in this case. . I learned the hard way that installing conda-build, grayskull and shyaml and conda update condain my conda base environment landed me in a broken mamba and incompatible package versions. So, this time create a new dedicate environment: . conda create -n cf python=3.9 mamba -c conda-forge conda activate cf mamba install -c conda-forge conda-build grayskull # grayskull not for this post, but other future submissions . Is there an example I can start from and work by inference and similarity rather than first principles? . libnetcdf-feedstock may be an appropriate case to start from, even if more sophisticated than my case. Rather than strip down this libnetcdf recipe though, I work from the example in the staged-recipe and add, staying closer to contributing packages . I did bump into a couple of issues, but I got less issues than I thought to get a package compiling . The end result should be something like: . {% set name = &quot;moirai&quot; %} {% set version = &quot;1.1&quot; %} package: name: {{ name|lower }} version: {{ version }} source: # url: https://github.com/moirai/moirai/releases/download/{{ version }}/moirai-{{ version }}.tar.gz # and otherwise fall back to archive: url: https://github.com/csiro-hydroinformatics/moirai/archive/refs/tags/{{ version }}.tar.gz sha256: b329353aee261ec42ddd57b7bb4ca5462186b2d132cdb2c9dacc9325899b85f3 build: number: 0 requirements: build: - cmake - make # [not win] # - pkg-config # [not win] # - gnuconfig # [unix] - {{ compiler(&#39;c&#39;) }} - {{ compiler(&#39;cxx&#39;) }} run: - curl # [win] - libgcc # [unix] test: files: - CMakeLists.txt commands: - ls about: home: https://github.com/csiro-hydroinformatics/moirai summary: &#39;Manage C++ objects lifetime when exposed through a C API&#39; description: | This C++ library is designed to help handling C++ objects from so called opaque pointers, via a C API, featuring: * counting references via the C API to C++ domain objects * handle C++ class inheritance even via opaque pointers * mechanism for resilience to incorrect type casts * thread-safe design license: BSD-3-Clause license_family: BSD license_file: LICENSE.txt doc_url: https://github.com/csiro-hydroinformatics/moirai/blob/master/doc/Walkthrough.md dev_url: https://github.com/csiro-hydroinformatics/moirai extra: recipe-maintainers: - jmp75 . Note that you do need make as a build requirement besides cmake, otherwise you’d end up with : . CMake Error: CMake was unable to find a build program corresponding to &quot;Unix Makefiles&quot;. CMAKE_MAKE_PROGRAM is not set. You probably need to select a different build tool. CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage . build.sh: . #!/bin/bash # Build moirai. mkdir -v -p build cd build # May be needed for unit tests down the track? export TERM= export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PREFIX/lib # cmake -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON .. cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${PREFIX} -DBUILD_SHARED_LIBS=ON ../ make -j 2 install # rm -rf ../build/ # not if we want to run the unit tests . Building locally on linux64 . Note: although it may have changed by the time you read this, at the time I write I may have to export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 to force that image being used. See previous post. . conda activate cf export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 cd ~/src/staged-recipes python ./build-locally.py linux64 . and the build seems OK… . + touch /home/conda/staged-recipes/build_artifacts/conda-forge-build-done . Building locally on win64 . Trying to run build-locally.py with the option win64 returns: ValueError: only Linux/macOS configs currently supported, got win64 . You’ll want to read or re-read the conda-forge documentation Using cmake and particularities on windows . Trying for bld.bat: . REM Build moirai. @REM Credits: some material in this file are courtesy of Kavid Kent (ex Australian Bureau of Meteorology) mkdir build cd build @REM following may be unncessary set PATH=&quot;%PREFIX% bin&quot;;%PATH% :: Configure using the CMakeFiles cmake -G &quot;%CMAKE_GENERATOR%&quot; ^ -DCMAKE_INSTALL_PREFIX:PATH=&quot;%LIBRARY_PREFIX%&quot; ^ -DCMAKE_PREFIX_PATH:PATH=&quot;%LIBRARY_PREFIX%&quot; ^ -DCMAKE_BUILD_TYPE:STRING=Release ^ .. if %errorlevel% neq 0 exit 1 msbuild /p:Configuration=Release /v:q /clp:/v:q &quot;INSTALL.vcxproj&quot; if %errorlevel% neq 0 exit 1 @REM del *.* . Since the local build script cannot emulate a win64 build, I am trying to set up on a Windows box and see what a conda build gives. . I first tried on my Windows desktop where I do have vs 2019 and 2022 installed. Not sure whether they can be used. This section of the conda-forge doc seem to suggest Microsoft Build Tools for Visual Studio 2017 is required, but the link to the Python wiki page on Windows compilers is covering many more versions, so this is rather confusing. See the Appendix for more details on what happens in this case. . Trying again on a windows machine, but installing visual studio build tools 2017. . After creating the conda environment cf, similar to the above on Linux: . cd c: src staged-recipes recipes conda build moirai . “mostly” works. for a minute or two it seems to freeze at a “number of files” line: . Packaging moirai INFO:conda_build.build:Packaging moirai Packaging moirai-1.1-h82bb817_0 INFO:conda_build.build:Packaging moirai-1.1-h82bb817_0 number of files: 11 . then: . PGO: UNKNOWN is not implemented yet! PGO: UNKNOWN is not implemented yet! Unknown format Unknown format Unknown format Unknown format INFO: sysroot: &#39;C:/Windows/&#39; files: &#39;[&#39;zh-CN/winhlp32.exe.mui&#39;, &#39;zh-CN/twain_32.dll.mui&#39;, &#39;zh-CN/regedit.exe.mui&#39;, &#39;zh-CN/notepad.exe.mui&#39;]&#39; &lt;edit: snip&gt; Importing conda-verify failed. Please be sure to test your packages. conda install conda-verify to make this message go away. . conda search -c conda-forge conda-verify indeed returns something. Noted… . &lt;edit: snip&gt; ## Package Plan ## environment location: C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 _test_env The following NEW packages will be INSTALLED: ca-certificates: 2022.4.26-haa95532_0 curl: 7.82.0-h2bbff1b_0 libcurl: 7.82.0-h86230a5_0 libssh2: 1.10.0-hcd4344a_0 moirai: 1.1-h82bb817_0 local openssl: 1.1.1o-h2bbff1b_0 vc: 14.2-h21ff451_1 vs2015_runtime: 14.27.29016-h5e58377_2 zlib: 1.2.12-h8cc25b3_2 . &lt;edit: snip&gt; set PREFIX=C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 _test_env set SRC_DIR=C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654825059872 test_tmp (cf) %SRC_DIR%&gt;call &quot;%SRC_DIR% conda_test_env_vars.bat&quot; (cf) %SRC_DIR%&gt;set &quot;CONDA_SHLVL=&quot; &amp;&amp; (cf) %SRC_DIR%&gt;conda activate &quot;%PREFIX%&quot; (%PREFIX%) %SRC_DIR%&gt;IF 0 NEQ 0 exit /B 1 (%PREFIX%) %SRC_DIR%&gt;call &quot;%SRC_DIR% run_test.bat&quot; (%PREFIX%) %SRC_DIR%&gt;ls (%PREFIX%) %SRC_DIR%&gt;IF -1073741511 NEQ 0 exit /B 1 . The latter fails because the command line ls borks with the error message (windows box error message) ls.exe - Entry Point not found. where ls returns C: Users xxxyyy Miniconda3 envs cf Library usr bin ls.exe so this is something that came from conda. . Taking a look at the resulting moirai-1.1-h82bb817_0.tar.bz2 file, the content looks sensible (include header files, bin/moirai.dll) . info/hash_input.json info/index.json info/files info/paths.json info/about.json info/git info/recipe/build.sh info/recipe/meta.yaml.template info/licenses/LICENSE.txt Library/include/moirai/extern_c_api_as_opaque.h Library/include/moirai/extern_c_api_as_transparent.h Library/include/moirai/setup_modifiers.h Library/include/moirai/reference_handle_map_export.h Library/include/moirai/error_reporting.h Library/include/moirai/reference_handle.h info/recipe/conda_build_config.yaml info/recipe/meta.yaml info/test/run_test.bat info/recipe/bld.bat Library/bin/moirai.dll Library/include/moirai/reference_handle_test_helper.hpp Library/include/moirai/opaque_pointers.hpp Library/include/moirai/reference_type_converters.hpp Library/include/moirai/reference_handle.hpp . Recapitulation and summary . In some respects I had less issues with this conda package than the “pure python” one, partly because of prior experience, but not entirely. . I may be in a decent place to submit this to the conda-forge/staged-recipe repository. I may hold off a bit though. First, the unit tests in moirai are not exercised by the meta.yaml file (or build scripts). Second, I may next look at setting up a conda channel to test managing conda dependencies, even if I see moirai as belonging to conda-forge rather than a private channel. Third, there are probably other things I need to tidy up. . To recapitulate on the essentials out of post: . conda create -n cf python=3.9 mamba -c conda-forge conda activate cf # grayskull not for this post, but other future submissions mamba install -c conda-forge conda-build conda-verify grayskull . The draft conda recipe for moirai at this stage is available there . Acknowledgements . Some of the conda recipe trialled was influenced by prior work by Kavid Kent (ex Australian Bureau of Meteorology). . Appendices . Appendix: if missing ms build tools 2017 . If trying without having installed ms build tools 2017: . %SRC_DIR%&gt;CALL %BUILD_PREFIX% etc conda activate.d vs2017_get_vsinstall_dir.bat Did not find VSINSTALLDIR Windows SDK version found as: &quot;10.0.19041.0&quot; The system cannot find the path specified. Did not find VSINSTALLDIR CMake Error at CMakeLists.txt:16 (PROJECT): Generator Visual Studio 15 2017 Win64 could not find any instance of Visual Studio. . (cf) %SRC_DIR%&gt;set &quot;SCRIPTS=%PREFIX% Scripts&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_PREFIX=%PREFIX% Library&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_BIN=%PREFIX% Library bin&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_INC=%PREFIX% Library include&quot; (cf) %SRC_DIR%&gt;set &quot;LIBRARY_LIB=%PREFIX% Library lib&quot; (cf) %SRC_DIR%&gt;set &quot;c_compiler=vs2017&quot; (cf) %SRC_DIR%&gt;set &quot;fortran_compiler=gfortran&quot; (cf) %SRC_DIR%&gt;set &quot;vc=14&quot; (cf) %SRC_DIR%&gt;set &quot;cxx_compiler=vs2017&quot; . call C: Users xxxyyy Miniconda3 Scripts activate.bat . Note: . base * C: Users xxxyyy Miniconda3 envs cf C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654760072022 _build_env C: Users xxxyyy Miniconda3 envs cf conda-bld moirai_1654760072022 _h_env . If I try to start with Visual Studio 2022 Developer Command Prompt v17.1.5. Then conda environment activation, still: . %SRC_DIR%&gt;CALL %BUILD_PREFIX% etc conda activate.d vs2017_get_vsinstall_dir.bat Did not find VSINSTALLDIR Windows SDK version found as: &quot;10.0.19041.0&quot; ********************************************************************** ** Visual Studio 2022 Developer Command Prompt v17.1.5 ** Copyright (c) 2022 Microsoft Corporation ********************************************************************** [ERROR:vcvars.bat] Toolset directory for version &#39;14.16&#39; was not found. [ERROR:VsDevCmd.bat] *** VsDevCmd.bat encountered errors. Environment may be incomplete and/or incorrect. *** [ERROR:VsDevCmd.bat] In an uninitialized command prompt, please &#39;set VSCMD_DEBUG=[value]&#39; and then re-run [ERROR:VsDevCmd.bat] vsdevcmd.bat [args] for additional details. [ERROR:VsDevCmd.bat] Where [value] is: [ERROR:VsDevCmd.bat] 1 : basic debug logging [ERROR:VsDevCmd.bat] 2 : detailed debug logging [ERROR:VsDevCmd.bat] 3 : trace level logging. Redirection of output to a file when using this level is recommended. [ERROR:VsDevCmd.bat] Example: set VSCMD_DEBUG=3 [ERROR:VsDevCmd.bat] vsdevcmd.bat &gt; vsdevcmd.trace.txt 2&gt;&amp;1 Did not find VSINSTALLDIR CMake Error at CMakeLists.txt:16 (PROJECT): Generator Visual Studio 15 2017 Win64 could not find any instance of Visual Studio. -- Configuring incomplete, errors occurred! . Resources . Reference Counting in Library Design – Optionally and with Union-Find Optimization | .",
            "url": "https://jmp75.github.io/work-blog/recipes/conda/conda-forge/c++/2022/06/10/conda-packages-conda-forge-2.html",
            "relUrl": "/recipes/conda/conda-forge/c++/2022/06/10/conda-packages-conda-forge-2.html",
            "date": " • Jun 10, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Submitting your first conda package to conda-forge",
            "content": "Background . For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. It is a stack with a core in C++, but accessible via a C API by users from R, Matlab, Python and so on. A whole article could be written about the design rationale, successes and shortcomings of this stack, and the interplay of people, organisations and technologies in using these tools and how. But this will not be this post. . Focussing on the Python side of things, these streamflow forecasting tools are used mostly on Windows and Linux, the core is deployed as dynamic libraries (.dll or .so) on disk, and python packages access these using cffi for interoperability. The python packages contain solely python code; there is no cython or straight C. . I’ve come to appreciate (mostly) conda environments for managing software stacks for various projects. This post is a start to test packaging some of “my” software with conda, in the hope this reduces the surprisingly strong impedance, technical but not only, towards usage by a broader audience. . A bit picture end point would be a corporate equivalent to a conda-forge channel, with the full software stack available for any employee. . Baby steps . I had a look a few weeks ago at how I’d package a substantial but relatively small C++ code MOIRAI: Manage C++ Objects’s lifetime when exposed through a C API. This proved a bit premature for reasons I won’t detail. . So, let’s (re)start with a python-only library, as it happens in the same vein, refcount. Astonishingly, there is still no strict equivalent that I can find in conda-forge dedicated to reference counting external pointers. So, can I claim the spot? . Resources . I started this post thinking first about conda packaging rather than submission per se to conda-forge. Some resources I initially looked at as promising, but from which I backed away (for now): . Building conda packages from scratch | python packaging tutorials Scipy 2018 Tutorial: The Joy of Packaging - conda packages | Activision Game: A tutorial (+ build recipes) describing how to use conda for C, C++, and python package management outlines well the rationale for packaging in conda, and appears didactic. It appears not to have recent commit, although it is not necessarily a problem. | . You’ll see in the walkthrough below (next section) that I reoriented towards an upfront submission to conda-forge: . 3 simple stept to build a python package for conda-forge. This post really got me on a better path. | conda-forge documentation: Contributing packages | . Walkthrough . First trial: conda build locally? . Of course python is necessary and a conda environment a given. I do . ~/config/baseconda because I never have conda activated by default from .bashrc. . I am actually not sure from the conda-build tutorial in which environment I should install conda-build. Let’s try the base environment and see whether we get stuck or not. . mamba install -c conda-forge conda-build (installing from conda-forge is an idiosyncrasy. I strongly recommend mamba) . You should at least skim through the concepts. This is rather dry to read throughly upfront. . The tutorial Building conda packages from scratch quickly confused me; I was trying to transpose it to refcount but this does not look like the right template to start from. The section editing-the-meta-yaml-file appears out of sync with the “correct” meta.yaml file. Baffling. . Preparing a PR to conda-forge/staged-recipes . Enter two new resources: 3 simple stept to build a python package for conda-forge and conda-forge documentation: Contributing packages. From these it becomes clear I should use grayskull to get a starting point as a meta.yaml file . cd ~/src git clone --depth 1 git@github.com:jmp75/staged-recipes.git cd ~/src/staged-recipes/recipes git remote add upstream git@github.com:conda-forge/staged-recipes.git mamba install -c conda-forge grayskull grayskull pypi refcount . #### Initializing recipe for refcount (pypi) #### Recovering metadata from pypi... Starting the download of the sdist package refcount refcount 100% Time: 0:00:00 15.3 MiB/s|###############################################################################################################################################################################################################################################| Recovering information from setup.py Executing injected distutils... Recovering metadata from setup.cfg No data was recovered from setup.py. Forcing to execute the setup.py as script Recovering metadata from setup.cfg Checking &gt;&gt; cffi 100% |##########################################################################################################################################################################################################################################|[Elapsed Time: 0:00:00] Matching license file with database from Grayskull... Match percentage of the license is 59%. Low match percentage could mean that the license was modified. License type: BSD-3-Clause License file: LICENSE.txt Host requirements: - pip - python Run requirements: - cffi - python RED: Missing packages GREEN: Packages available on conda-forge Maintainers: - j-m #### Recipe generated on /home/per202/src/staged-recipes/recipes for refcount #### . The output meta.yaml (which is actually a ninja template file), is a good start, however you should revise it a bit rather than accept wholesale . Mostly fine, however this did not pick up a requirement cffi &gt;=1.11.5, and second guessing from reading this gallon.me post, a minimum python version is necessary to get accepted. . requirements: host: - pip - python &gt;=3.6 run: - cffi &gt;=1.11.5 - python &gt;=3.6 . Perhaps optional, remove a hard-coded package string “refcount “in the source url section. . It is instructive to look at the existing pull requests on staged-recipes. Notably I realise that the github ID extracted by grayskull is not the correct one; I am not the ID j-m, unfortunately (could have been judging by history length). . extra: recipe-maintainers: - j-m . extra: recipe-maintainers: - jmp75 . The end result should be something like: . {% set name = &quot;refcount&quot; %} {% set version = &quot;0.9.3&quot; %} package: name: {{ name|lower }} version: {{ version }} source: url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.zip sha256: bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442 build: noarch: python script: {{ PYTHON }} -m pip install . -vv number: 0 requirements: host: - pip - python &gt;=3.6 run: - cffi &gt;=1.11.5 - python &gt;=3.6 test: imports: - refcount commands: - pip check requires: - pip about: home: https://github.com/csiro-hydroinformatics/pyrefcount summary: A Python package for reference counting and interop with native pointers description: | This package helps you achieve reliable management of memory allocated in native libraries, written for instance in C++. While it boils down to &quot;simply&quot; maintaining a set of counters, it is deceptively complicated to do so properly and not end up with memory leaks or crashes. &lt;https://pyrefcount.readthedocs.io/en/latest/&gt;. dev_url: https://github.com/csiro-hydroinformatics/pyrefcount license: BSD-3-Clause license_family: BSD license_file: LICENSE.txt extra: recipe-maintainers: - jmp75 . So; ready to submit a pull request? Wait, wait. . Building locally . test has a section on Running unit tests. Note that the default conda recipe above has a “pip check”, but nothing more. refcount unit tests use pytest, and has unit tests; tick that. refcount is a pure python package, but it is a package for (mostly) interoperability with native code via a C API. Unit tests do contain some c/c++ code. . Should the recipe run fine upon submission, including unit tests? Before submitting a pull request that may trigger a failed check, let’s experiment with staging tests locally. . so: . cd ~/src/staged-recipes python ./build-locally.py linux64 . File &quot;/home/per202/miniconda/lib/python3.9/subprocess.py&quot;, line 373, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command &#39;[&#39;.scripts/run_docker_build.sh&#39;]&#39; returned non-zero exit status 1. . I tried to conda install -c conda-forge shyaml which seems to be used by the scripts, but this did not alleviate the issue. . That took me some time to find a workaround to this one. The “exit status 1” is actually very misleading. The root cause is a docker run -it that exited with an error code 139. I seem to not be the only one to have bumped into this issue still open. I may have pointed to the workaround in the conda-forge FAQ. . I needed to override the default docker image build-locally.py falls back to with: . export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 python build-locally.py linux64 . the build script works this time, but at some point: . Processing $SRC_DIR Added file://$SRC_DIR to build tracker &#39;/tmp/pip-build-tracker-bkn7ckr9&#39; Running setup.py (path:$SRC_DIR/setup.py) egg_info for package from file://$SRC_DIR Created temporary directory: /tmp/pip-pip-egg-info-68li1y6t Preparing metadata (setup.py): started Running command python setup.py egg_info Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt; File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt; File &quot;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/setup.py&quot;, line 41, in &lt;module&gt; with open(os.path.join(here, &#39;README.md&#39;), encoding=&#39;utf-8&#39;) as f: File &quot;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pl/lib/python3.10/codecs.py&quot;, line 905, in open file = builtins.open(filename, mode, buffering) FileNotFoundError: [Errno 2] No such file or directory: &#39;/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/README.md&#39; error: subprocess-exited-with-error × python setup.py egg_info did not run successfully. │ exit code: 1 ╰─&gt; See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. . This is an issue that may be in my control. . cd ~/src/staged-recipes/build_artifacts/refcount_1654312313810/work ls ## build_env_setup.sh conda_build.sh LICENSE.txt MANIFEST.in metadata_conda_debug.yaml PKG-INFO README.rst refcount refcount.egg-info setup.cfg setup.py . refcount has both a README.md and README.rst, the latter being an export from the former because pypi requires (or used to require) a README.rst to display correctly. The zip archive of the source code on pypi indeed does not have the README.md file included. . I’ve inherited the practice to use in the packages setup.py the following, to limit redundances. . with open(os.path.join(here, &#39;README.md&#39;), encoding=&#39;utf-8&#39;) as f: long_description = f.read() long_description_content_type=&#39;text/markdown&#39; . Previously I needed to convert on the fly to restructured Text, but Markdown is more supported. Still there is a lot of inertia with restructuredText. . I may try to just nuke the README.rst. The only fly on the ointment is: is pypi ok with rendering markdown correctly these days? Probably; the packaging documentation is using README.md by default. . So, build and submit to pypi the updated refcount 0.9.4 with no README.rst. Looks fine, including the zip source archive. . RuntimeError: SHA256 mismatch: &#39;21567918cb1bb30bf8116ce3483d3f431de202618eabbc6887b4814b40a3b94a&#39; != &#39;bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442&#39; Traceback (most recent call last): . Right, I forgot to change the checksum in the meta.yaml file. . And… it seems to complete. . import: &#39;refcount&#39; + pip check No broken requirements found. + exit 0 Resource usage statistics from testing refcount: Process count: 1 CPU time: Sys=0:00:00.0, User=- Memory: 3.0M Disk usage: 28B Time elapsed: 0:00:02.1 TEST END: /home/conda/staged-recipes/build_artifacts/noarch/refcount-0.9.4-pyhd8ed1ab_0.tar.bz2 . Submit the pull request, and pleasantly: . . Conclusion . While there were a couple of bumps along the way, this should end up with a positive outcome. If not with refcount on conda-forge, I’ve a better understanding to tackle conda packaging on the rest of the software stack. . Building a conda package “from scratch” may not be the easiest learning path. Even if you indent to build a conda package not for conda-forge, going through the staged-recipes process may be a most | Some of the reference documentation may need a spruice up. Building conda packages from scratch confused me. First, packaging a pypi package is not starting “from scratch” for most users. Second, inconsistencies in the documentation. I am sure I’ll get back to that resource, but I wish there were more “water tight”, step-by-step tutorials for conda packaging. | .",
            "url": "https://jmp75.github.io/work-blog/recipes/conda/conda-forge/2022/06/04/conda-packages-conda-forge.html",
            "relUrl": "/recipes/conda/conda-forge/2022/06/04/conda-packages-conda-forge.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Lithology classification using Hugging Face, part 1",
            "content": "About . This is (perhaps) the start of a series of posts on using natural language processing for lithology classification. I hope to explore multi-label classification in subsequent posts. . . Background . I&#39;ve been attending virtually the latest Deep Learning course run by Jeremy Howard (See the fast.ai forum for pointers to past courses). Part of the experience can be to find &quot;homeworks&quot;. I traditionally work with point time series data, and it would have been a Pavlov reflex for me to get a use case with this type of data. . However in one of the lessons Jeremy used Hugging Face Transformers applied to the Kaggle competition U.S. Patent Phrase to Phrase Matching. At some point he made a comment about how much NLP progressed over the recent years, and how much potential value creation there was in this. . I do not follow closely NLP research, and am not knowledgeable enough to agree or not, but got an inkling of the potential value a few years back when working on a python package for exploratory lithology analysis, for groundwater characterisation. Lithology is &quot;the study of the general physical characteristics of rocks&quot;. Drilling core soil samples is not cheap; existing records are valuable. . Sections of a core sample drill can be described with sentences such as: . topsoil | shale, slippery back, green seams | sandstone alluvium water bearing | gravel red very clayey water supply | sandstone, red/pale brown, fine-coarse, white clay bands, brownish yellow bands, pebbles (16.9-16.98m) | fill; orange-brown, dry, loose, pebbles to 2.5cm, heterogeneous | clay; light brown, strongly cohesive, contains silicate &amp; carbonate clasts to coarse sand size, no clay smell. | . You would have an inkling that the prose of the writer (the &quot;driller&quot;) can vary in style and detail. One typical use case is to determine the primary and (optionally) secondary lithologies of a record, as it strongly influences how fast water can flow underground. &quot;gravel red very clayey&quot; is on the easier side of the spectrum of difficulty: &quot;gravel&quot; as a primary lithology and &quot;clay&quot; as a secondary lithology (assuming these are valid classes for the context). It can get much trickier to classify of course, and certainly expensive if done very manually. Automating this classification process opens the possibility of an iterative process towards a sound lithology classification fit for purpose. . I co-authored a conference presentation &quot;Comparing regular expression and deep learning methods for the classification of descriptive lithology&quot; (Page 161 of the MODSIM 2019 book of abstracts if you are curious). With all the caveats of a study made on very limited resources, I was surprised by how well NLP performed overall to train and test on a human-labeled dataset. . I am not working on this domain during paid hours these days, but feel like revisiting this for a bit over the course. . Getting acquainted with data . Arbitrarily, I downloaded data for the Namoi catchment from the Australian Groundwater Explorer. While I am not totally new to the domain, I genuinely do need to explore this data from scratch. This will probably be at least the rest of this post. . import pandas as pd from pathlib import Path . fn = Path(&#39;~&#39;).expanduser() / &quot;data/ela/shp_namoi_river/NGIS_LithologyLog.csv&quot; . The types of a few columns lead to warnings: let&#39;s force them to &#39;str&#39; here. I do not anticipate using the depth records for now. . litho_logs = pd.read_csv(fn, dtype={&#39;FromDepth&#39;: str, &#39;ToDepth&#39;: str, &#39;MajorLithCode&#39;: str, &#39;MinorLithCode&#39;: str}) . So how does this data look like? . litho_logs.head() . OBJECTID BoreID HydroCode RefElev RefElevDesc FromDepth ToDepth TopElev BottomElev MajorLithCode MinorLithCode Description Source LogType OgcFidTemp . 0 14 | 10045588 | GW003048.1.1 | None | UNK | 0.0 | 1.22 | None | None | SOIL | NaN | SOIL SANDY CLAY | UNK | 1 | 8666163 | . 1 15 | 10045588 | GW003048.1.1 | None | UNK | 19.51 | 28.65 | None | None | BSLT | NaN | BASALT WATER BEARING | UNK | 1 | 8666164 | . 2 28 | 10060509 | GW006968.1.1 | None | UNK | 0.0 | 3.66 | None | None | TPSL | NaN | TOPSOIL | UNK | 1 | 8666177 | . 3 29 | 10060509 | GW006968.1.1 | None | UNK | 33.53 | 34.75 | None | None | GRVL | NaN | GRAVEL WATER BEARING | UNK | 1 | 8666178 | . 4 30 | 10060509 | GW006968.1.1 | None | UNK | 48.77 | 51.82 | None | None | SHLE | NaN | SLIPPERY BACK | UNK | 1 | 8666179 | . A MajorLithoCode column appears to be populated, so this may be suitable for training a classifier. I have no idea (or forgot) how these lithology codes have been derived and what the corpus of labels is. . len(litho_logs) . 144518 . For the sake of conciseness I will point-blank reuse the text processing utilities in the ela package, without any explanation of the setup (ela comes with many dependencies from NLP to 3d vis that can be tricky to install). . from ela.textproc import token_freq, plot_freq . MAJOR_CODE=&#39;MajorLithCode&#39; MINOR_CODE=&#39;MinorLithCode&#39; DESC=&#39;Description&#39; . Major (primary) lithology codes . litho_classes=litho_logs[MAJOR_CODE].values df_most_common= token_freq(litho_classes, 50) . plot_freq(df_most_common) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . This is a long-tailed distribution; quite a few labels. The limit to 4 characters labels suggest a form of controlled vocabulary. The numbers 20, 19, 23, 8, 1 look odd. &quot;None&quot; is an artefact though not misleading, quite a few records are such that no major lithology could be attributed. . Minor (secondary) lithology codes . litho_classes=litho_logs[MINOR_CODE].values df_most_common= token_freq(litho_classes, 50) . plot_freq(df_most_common) . &lt;AxesSubplot:xlabel=&#39;token&#39;&gt; . Well, no minor lithology codes, so this data set may not be a good start for multi-label classification. Still, I&#39;ll persist with this data set, and reassess later on . Back to major lithology codes . What are the flavour of descriptions leading to the most frequent classes (CLAY, GRVL, etc.), as well as &quot;None&quot; . is_clay = litho_logs[MAJOR_CODE] == &#39;CLAY&#39; . clay_coded = litho_logs.loc[is_clay][DESC] . import numpy as np np.random.seed(123) clay_coded.sample(n=50) . 144320 CLAY 117387 CLAY 18630 CLAY 35565 CLAY SOME SANDY 61997 CLAY 13533 CLAY SANDSTONE BANDS 117529 YELLOW CLAY 82290 CLAY 73280 CLAY GREY SANDY 106748 CLAY STONEY 26265 CLAY YELLOW 16456 CLAY 25032 CLAY 9911 CLAY 135852 CLAY, SANDY GRAVELLY, ORANGE BROWN, MEDIUM PLA... 30596 CLAY 140698 CLAY, SANDY FAT; RED, DRY-MOIST, MEDIUM CONSIS... 70267 CLAY 6534 CLAY YELLOW 69927 CLAY GRAVEL 76476 SANDY CLAY 7329 CLAY GRITTY 51685 CLAY; 40% BROWN, GRAVEL &amp; SAND 60%, MOST 1-3MM 137385 CLAY 35816 CLAY 73972 CLAY 102333 CLAY SOME STONES 29218 CLAY 92266 RED CLAY 26151 CLAY SANDY 107456 CLAY GREY 52550 CLAY - LIGHT GREY, EXTREMELY SANDY (FINE TO CO... 24225 CLAY 120696 CLAY; LIGHT GREY, TRACE OF WHITE 9030 CLAY 14889 CLAY 25834 CLAY LIGHT BROWN GREY SILTY 44078 CLAY 114869 CLAY 109302 CLAY 38107 CLAY PATCHY 30801 CLAY 82362 CLAY/BROWN ORANGE 141490 CLAY; BROWN, PINK, SOME BLUE 23075 CLAY SANDY GRAVEL 42110 CLAY 50493 CLAY, WHITE SEAM, RED 90209 CLAY - DARK BROWN 104019 CLAY 12690 CLAY Name: Description, dtype: object . Nothing too surprising in this, very intuitive, though the tail suggests there may be a few outliers: . clay_coded.tail(10) . 144386 CLAY; LIGHT BROWN, SOME LIGHT GREY, SILTY 144388 CLAY; BROWN 144391 CLAY; LIGHT BROWN 144393 CLAY - BROWN 144394 CLAY - LIGHT BROWN, EXTREMELY SANDY (COARSE) 144398 CONGLOMERATE - WEATHERED BUT STILL HOLDS TOGET... 144401 CLAY; BROWN 144402 CLAY - GREY 144488 CLAY 144503 None Name: Description, dtype: object . Looking at the &quot;sand&quot; code: . def sample_desc_for_code(major_code, n=50, seed=None): is_code = litho_logs[MAJOR_CODE] == major_code coded = litho_logs.loc[is_code][DESC] if seed is not None: np.random.seed(seed) return coded.sample(n=50) . sample_desc_for_code(&#39;SAND&#39;, seed=123) . 72256 SAND GRAVEL FINE WATER SUPPLY 109345 SAND CLAYEY GRAVEL 66816 SAND WATER SUPPLY 85097 SAND - 70% UP TO 2MM, GRAVEL 10% 2-5MM, CLAY 2... 137476 SAND; LIGHT BROWN, MEDIUM TO COARSE, FINE GRAV... 29946 SAND GRAVEL DIRTY 37012 COARSE SAND AND GRAVEL 135985 SAND 74402 SAND WHITE GREY WELL SORTED WATER SUPPLY 42422 SAND GRAVEL 41225 SAND CLAYEY GRAVEL 50700 SAND 33813 BROWN SAND (VERY FINE TO MEDIUM) 38452 SAND GRAVEL MEDIUM WATER SUPPLY 107707 SAND FINE-COARSE 51759 SAND; 60%, SILTY, FINE 10%, MEDIUM 20% &amp; COARS... 102234 SAND GRAVEL WATER SUPPLY 74640 SAND GRAVEL WATER SUPPLY 13123 SAND GRAVEL WATER SUPPLY 23110 SAND 11373 SAND GRAVEL WATER SUPPLY 35508 SAND SILTY GRAVEL WATER SUPPLY 32744 SAND CLAYEY 67310 SAND CLAY 41813 SAND GRAVEL WATER SUPPLY 106457 SAND GRAVEL WATER SUPPLY 85591 SAND AND GRAVEL - CLAYEY AND SILTY 112156 SAND, WHITE (VERY VERY FINE) 111256 SAND/GRAVEL; FINE TO COARSE, BROWN 55808 SAND; POORLY GRADED, COARSE, SUBROUNDED, LIGHT... 91458 SAND, MEDIUM-COARSE; SLIGHTLY SILTY, WET, LOOS... 61608 SAND, GRAVEL &amp; LIMESTONE 96382 SAND CLAYEY 37119 MEDIUM BROWN SAND &amp; GRAVEL SOME LARGE GRAVEL 118473 SAND; 40% 1/20-1/2MM, 40% 1/2-1MM, 20% 1-3MM, ... 70703 SAND GRAVEL WATER SUPPLY 92920 BROWN SAND 80551 SAND, SILTY; AS ABOVE, TRACE CLAY, ORANGE, LOW... 25175 COARSE SAND &amp; FINE GRAVEL WITH ANGULAR FRAGMEN... 38905 SAND FINE WATER BEARING 44248 SAND GRAVEL WATER SUPPLY 32261 SAND DRIFT 34510 SAND CLAYEY COARSE GRAVEL 77431 SAND AND GRAVEL FINE TO COARSE GREY 138362 SAND BROWN 103758 SAND GRAVEL STONES SOME CLAY BANDS 69238 SAND GRAVEL 143566 SAND &amp; GRAVEL; SAND 50% (FINE 30%, MEDIUM 20%,... 135131 SAND 30361 SAND CLAYEY WELL SORTED Name: Description, dtype: object . Rather straighforward, consistent and and intuitive . Major lithology code &quot;None&quot; . This one is likely to be more curly and surprising. Let&#39;s see . sample_desc_for_code(&#39;None&#39;, seed=123) . 131075 W.B. BROWN SHALE 127170 COARSE SAND AND GRAVEL 126145 BLUE CLAY 126889 GREY AND BROWN CLAY 132179 GREY BROWN CLAY 130033 BROWN SANDY CLAY 123436 GRAVEL 128794 CLAY 128242 SAND AND FINE GRAVEL 134156 None 129636 W.B. BASALT 122884 LIGHT ORANGE SILTY CLAY 124580 SHALES 128684 W.B. SHALE 131120 SAND AND GRAVEL 122690 RED RIDGE CLAY 132319 BLACK SOIL 128528 BLACK CLAY 123646 MUDSTONE 129757 SAND &amp; CLAY 124353 SOFT BROWN CLAY 131921 CLAY 133208 BROWN CLAYS 132945 COARSE SAND 122956 SAND AND GRAVEL 123723 CALY AND GRAVEL 122102 TOP SOIL 126455 HARD SANDY BROWN &amp; GREY CLAY WITH STONES 125637 SANDY BLUE AND GREY CLAY 131154 BROWN SANDY CLAY 129931 RIDGE CLAY 123557 RIDGE CLAY 124964 DARK BROWN CLAY 129391 GREY SHALE 132891 WATER 0.37 LS 132006 BROWN CLAY 134673 SEP 123692 BROWN CLAY 126034 REDDISH BROWN CLAY 129690 SOFT GINGER/BROWN CLAY &amp; STONES 125560 FINE SANDY CLAYS 126238 SHALE &amp; (WATER) 123009 SOIL 124236 SOIL 125814 BLUE SHALE 130474 RED &amp; GREY CLAY 127005 BROWN CLAY 134212 BROWN AND 125640 SOIL 127355 SHALE Name: Description, dtype: object . Well, it does not require a fully trained geologist to think there should be obvious primary lithology codes for many of these, so why is there &quot;None&quot; for these descriptions? . Not shown in the 50-odd sample above are descriptions which should indeed be unclassified (e.g. &quot;no strata description&quot;) . This is also an occasion to note the less obvious information and complications in descriptive logs, compared to earlier categories: . CALY as a typo for CLAY | Slang terms and acronyms, e.g. &quot;W.B.&quot; perhaps for &quot;Weathered, broken&quot; | Quantitative and qualitative attributes such as SAND; 60%, SILTY, FINE 10%, MEDIUM 20% &amp; COARSE, which may be valuable for some types of classification | . sample_desc_for_code(&#39;SDSN&#39;, seed=123) . 9423 SANDSTONE SOFT 117911 SANDSTONE, SILTY; AS ABOVE, QUARTZ (+60%) 6613 SAND ROCK GREY 20345 SANDSTONE GREY HARD 106885 SANDSTONE 139088 SANDSTONE; DARK GREY, COARSE GRAINED, HARD 5101 SANDSTONE ROTTEN 26561 SANDSTONE YELLOW 137304 SANDSTONE; COARSE 56627 SANDSTONE; GREY, FINE-MEDIUM GRAINED, MOD WEAK... 98974 SANDSTONE WHITE WATER SUPPLY 87187 SANDSTONE 45141 SANDSTONE, VERY PALE BROWN, HIGH CLAY 70015 SANDSTONE YELLOW 29492 SANDSTONE WATER SUPPLY 92918 SANDSTONE 83555 SANDSTONE; LIGHT GREY, VERY FINE, STRONG 136401 SANDSTONE 48201 SANDSTONE 34060 SANDSTONE, YELLOW, MEDIUM GRAINED, HIGH CLAY M... 63752 SANDSTONE; WHITISH GREY, VERY FINE, ANGULAR, C... 51878 SANDSTONE, MEDIUM, WHITISH YELLOW 59346 SANDSTONE, SOFT 62968 SANDSTONE; OFF-WHITE GREY, VERY FINE, STRONG 140930 SANDSTONE, LIGHT GREY, SOME FRACTURES 6394 SAND ROCK 113546 SANDSTONE - HARD - LIGHT GREY 49539 SANDSTONE; RED GREY, FINE-MEDIUM GRAINED, FRIA... 60198 SANDSTONE; LIGHT GREY, FINE GRAINED 1366 SANDSTONE 35923 SANDSTONE WEATHERED COARSE WATER SUPPLY 141556 SANDSTONE, WHITE, IMPREGNATED WITH WATER WASHE... 10100 SANDSTONE WATER SUPPLY 89138 SANDSTONE; OFF-WHITE GREY, MEDIUM-COARSE GRAIN... 86046 QUARTZ SANDSTONE; MEDIUM GRAINED 76366 SANDSTONE, PALE BROWN, FINE-COARSE, LOW CLAY, ... 46179 SANDSTONE, LIGHTYELLOW/RED BROWN MOTTLED, FINE... 8613 SANDSTONE 4537 SANDSTONE YELLOW 60506 SANDSTONE 8917 SANDSTONE 1372 SANDSTONE WATER SUPPLY 82975 SANDSTONE; LIGHT GREY, FINE GRAINED, MOD STRON... 60166 SANDSTONE, VERY SOFT 31674 SANDSTONE SHALE 8397 SANDSTONE 113310 SANDSTONE; OFF-WHITE, FINE GRAINED, MOD STRONG 27980 SANDSTONE 79933 SANDSTONE WHITE BROKEN 33654 SANDSTONE YELLOW CLAYEY Name: Description, dtype: object . Now, what&#39;s with the weird numbers as lithology codes? . sample_desc_for_code(&#39;20&#39;, seed=123) . 130369 SANDY CLAY; BROWN (COARSE) 122444 CLAY SANDY 127761 CLAY SANDY 125771 SANDY CLAY 127783 CLAY SANDY 133014 SANDY CLAY AND CLAYEY SAND AND GRAVEL 124007 BROWN &amp; GREY SANDY CLAY 123445 CLAY LT BROWN GREY SANDY, MED-COARSE BROWN BANDS 122180 CLAY SANDY 127440 SANDY CLAY 131498 SANDY CLAY 127935 CLAY RED SANDY 129304 SANDY CLAY 130614 SANDY SHALE 125543 SANDY CLAY 125149 SANDY CLAY 124971 SANDY CLAY/GREY BROWN 123939 SANDY CLAY, BROWN 122392 CLAY SANDY 123999 BROWN &amp; GREY SANDY CLAY 129203 SANDY LOAM 127933 CLAY RED SANDY 134474 SANDY CLAYEY MED. BROWN 126071 SANDY CLAY 127927 CLAY SANDY 123587 CLAY SANDY 126559 SANDY CLAY 127928 CLAY GREY SANDY 124643 SANDY CALY/BROWN 122555 CLAY SANDY 132628 SANDY CLAY 129853 SANDY CLAY 130187 SANDY CLAY 129057 SANDY CLAY 133300 SANDY CLAY 122535 CLAY SANDY 122732 CLAY SANDY 127977 CLAY YELLOW SANDY 126568 SANDY CLAY 122339 CLAY SANDY 128386 SANDY CLAY, BROWN 128198 SANDY CLAY, RIDGE 125530 SANDY CLAY 126798 SANDY CLAY 122452 CLAY SANDY 123237 SANDY BROWN CLAYS 126510 SANDY CLAY; 40% LIGHT GREY, EXTREMELY SANDY (C... 132331 SANDY GRAVEL 131807 SANDY CLAY 132895 SANDY CLAY Name: Description, dtype: object . Interesting. There is a clear pattern. I know from my prior exposure that &quot;Clayey sands&quot; and &quot;sandy clays&quot; are not that uncommon (and gradations of mixes of sand and clay matter a great deal to estimate hydraulic conductivity). . Next . This was the initial EDA. Next I&#39;ll probably train a classifier on the major lithology code (or a subset thereof). I am keen to explore multi-label classification, but will have to decide whether to populate the secondary lithology code using regexp classification, or switch to a fully labelled dataset at some point. . This first post illustrated the need to have a look at data. This data was already collated and curated, and I have no doubt many people went through a lot of work to get there. But this may not be a fully labelled dataset amenable to be used for training a classifier. At least, not without further data preparation. .",
            "url": "https://jmp75.github.io/work-blog/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html",
            "relUrl": "/hugging-face/nlp/lithology/2022/06/01/lithology-classification-hugging-face.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Victorian Water Measurement Information System",
            "content": "About . Trying to retrieve time series or streamflow measurements via a REST API. Or anything that works for that matter. . This builds on work done by my colleague Andrew Freebairn, which really gave me a leg up. . Web API: which one? . I am not a regular user of online data. Whenever I really needed to fetch data from an online service, this tended to be an experience where reverting back to using a Web GUI with a &quot;download data&quot; button the path of least pain. . Before writing this section I got confused, trying to use kiwis-pie to retrieve data from the VIC data web site. Which is not suitable. I thought all Kisters products would be accessible using a unified web API. But Hydstra has its own separate from WIKSI, so far as I understand. See https://kisters.com.au/webservices.html . Curiously what seems to be the documentation for the Json API to Hydstra is in a page named HYDLLP – Flat DLL Interface to Hydstra. The examples are given in Perl, which is, well, not all that mainstream anymore. . There are visibly data codes to identify the types of data, at least to retrieve time series data. Query parameters such as varfrom and varto are kind of odd in an API though, and would deserve an explanation. It is only after manually downloading a time series from the VIC water data GUI that there is an explanation note about these variable codes. . The VIC data web site offers a way to directly link to URLs for particular data, but this seems not to work when used: https://data.water.vic.gov.au?ppbm=405288|B_405_GOULBURN|WEB_SW&amp;rs&amp;1&amp;rsdf_org . rdmw.qld.gov.au providing documentation for how to build json queries. . import requests import json import urllib import pandas as pd from datetime import date from IPython.core.display import HTML from IPython.display import JSON . def vic_url_builder(json_p, query): endpoint = &#39;http://data.water.vic.gov.au/cgi/webservice.exe&#39; return f&quot;{endpoint}?{json.dumps(json_p)}&amp;{urllib.parse.urlencode(query)}&quot; . The site identifiers of interest are: . site_id_405284 = &#39;405284&#39; # SUNDAY CREEK @ MT DISAPPOINTMENT site_id_405288 = &#39;405288&#39; # SUNDAY CREEK @ MT DISAPPOINTMENT (US SUNDAY CK. STORAGE site_id_405287 = &#39;405287&#39; # WESTCOTT CREEK @ MT DISAPPOINTMENT US SUNDAY CK STORAGE site_id_405239 = &#39;405239&#39; # SUNDAY CREEK @ CLONBINANE . # This took me a bit of time to figure out how to extract cumecs. One has to figure out that the &#39;varfrom&#39; in the query parameter needs to bhe the river level in metres. var_code_level_metres = &#39;100&#39; # var_code_cumecs = &#39;140&#39; # cumecs var_code_mlpd = &#39;141&#39; # ML/Day ver = { &#39;ver&#39;: &#39;2.0&#39; } #station_list = &#39;405288&#39; # don&#39;t set as 1-element list though: fails... station_list = &#39;,&#39;.join([ site_id_405284, site_id_405288, site_id_405287, site_id_405239, ]) def dt_str(x): return x.strftime(&#39;%Y%m%d%H%M%S&#39;) get_ts = { &#39;function&#39;:&#39;get_ts_traces&#39;, &#39;version&#39;:&#39;2&#39;, &#39;params&#39;:{ &#39;site_list&#39;:station_list, &#39;datasource&#39;:&#39;A&#39;, # THere are other codes, but this one is the only that returned something so far I can see. &#39;varfrom&#39;:var_code_level_metres, &#39;varto&#39;:var_code_cumecs, &#39;start_time&#39;:&#39;20220101000000&#39;, &#39;end_time&#39;:&#39;20220529000000&#39;, &#39;interval&#39;:&#39;day&#39;, &#39;multiplier&#39;:&#39;1&#39;, &#39;data_type&#39;:&#39;mean&#39;, } } query = vic_url_builder(get_ts, ver) . response = requests.get(query) . if response.status_code == requests.codes.ok: print(&#39;Return Json&#39;) print(response.headers[&#39;content-type&#39;]) response_json = response.json() # for visualisation in a notebook (not sure about rendered in a blog?) JSON(response_json) . Return Json text/html . &lt;IPython.core.display.JSON object&gt; . type(response_json) . dict . traces = response_json[&#39;return&#39;][&#39;traces&#39;] len(traces) . 2 . We got only trace time series for 2 of the 4 sites. This is because these sites have no recent data, at least not for the preiod of interest. Fair enough. For instance 405284 SUNDAY CREEK @ MT DISAPPOINTMENT functioned from from 23/06/1981 to 05/07/1984 . traces[0].keys() . dict_keys([&#39;error_num&#39;, &#39;compressed&#39;, &#39;site_details&#39;, &#39;quality_codes&#39;, &#39;trace&#39;, &#39;varfrom_details&#39;, &#39;site&#39;, &#39;varto_details&#39;]) . traces[0][&#39;site&#39;] . &#39;405288&#39; . tr = traces[0] tr[&#39;quality_codes&#39;] . {&#39;255&#39;: &#39;&#39;, &#39;100&#39;: &#39;Irregular data, use with caution.&#39;, &#39;2&#39;: &#39;Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction&#39;} . traces[1][&#39;quality_codes&#39;] . {&#39;255&#39;: &#39;&#39;, &#39;150&#39;: &#39;Rating extrapolated above 1.5x maximum flow gauged.&#39;, &#39;149&#39;: &#39;Rating extrapolated 1.5 times the maximum flow gauged.&#39;, &#39;2&#39;: &#39;Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction&#39;} . So this has good quality data, and some missing data (255). I am a bit puzzled by the codes for ratings beyond the max flow gauge, but the value appears to be sometimes zero flows in the data. Odd. . After a bit of iterative discovery, a way to turn json data to pandas series: . import numpy as np def to_dt64(x): return pd.Timestamp(x).to_datetime64() def make_daily_time_series(tr): site_name = tr[&#39;site&#39;] y = pd.DataFrame(tr[&#39;trace&#39;]) y.v = pd.to_numeric(y.v) v = y.v.values q = y.q.values y.t = y.t.apply(str) v[q == 255] = np.nan index = y.t.apply(to_dt64).values ts = pd.Series(v, index=index, name=site_name) return ts . ts = make_daily_time_series(traces[0]) ts.plot(ylabel=&quot;m3/s&quot;, xlabel=&quot;Time&quot;, title = ts.name) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;405288&#39;}, xlabel=&#39;Time&#39;, ylabel=&#39;m3/s&#39;&gt; . ts = make_daily_time_series(traces[1]) ts.plot(ylabel=&quot;m3/s&quot;, xlabel=&quot;Time&quot;, title = ts.name) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;405287&#39;}, xlabel=&#39;Time&#39;, ylabel=&#39;m3/s&#39;&gt; . Related resources . Andrew Freebairn&#39;s python package &#39;bomwater&#39; .",
            "url": "https://jmp75.github.io/work-blog/jupyter/hydstra/2022/05/28/hydstra-rest-data.html",
            "relUrl": "/jupyter/hydstra/2022/05/28/hydstra-rest-data.html",
            "date": " • May 28, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My profile page at the CSIRO | Github profile | . I live on Ngunnawal country . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmp75.github.io/work-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmp75.github.io/work-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}