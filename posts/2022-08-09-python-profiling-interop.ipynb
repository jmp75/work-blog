{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /C++/Python/performance/runtime/2022/08/09/python-profiling-interop\n",
    "author: J-M\n",
    "badges: true\n",
    "categories:\n",
    "- C++\n",
    "- Python\n",
    "- performance\n",
    "- runtime\n",
    "date: '2022-08-09'\n",
    "description: Finding a runtime hotspot in a software stack with Python and, potentially,\n",
    "  c++\n",
    "draft: false\n",
    "output-file: 2022-08-09-python-profiling-interop.html\n",
    "title: Runtime profiling of python bindings for a C/C++ library\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-roller",
   "metadata": {},
   "source": [
    "![Profiling python code with cProfile](images/cProfile_20220809.png \"Profiling python code with cProfile\")\n",
    "\n",
    "# Background\n",
    "\n",
    "A [software stack for streamflow forecasting](https://github.com/csiro-hydroinformatics/streamflow-forecasting-tools-onboard/) consists of a C++ modelling engine, accessible via a C API from various interactive language bindings including python. I have not done some benchmark performance profiling for a while on this stack. I expect the C++ core to be pretty much as good as it was 8 years ago, but as I bed down some of the python bindings, I notice some lags when interacting. This is not a surprise, and I expect these to be either in python code, or perhaps at the boundary with the C API. This intuition needs to be confirmed by objective measurements. \n",
    "\n",
    "It is one of these things that are rarely a blocker for quite a while, but a bit of instrumentation and performance tuning improves the user experience, a little bit every day. Also, some use cases where substantial stochastic data is generated in Python and passed to the C++ core would be very penalised by inefficiencies in Python or Python/C interoperability code.\n",
    "\n",
    "**Foreword**: in the case below, we do not need to profile C++ code _per se_ after all, so if this is what you are after specifically. Read on for the Python side of the story.\n",
    "\n",
    "# Python profilers\n",
    "\n",
    "[9 fine libraries for profiling python code](https://www.infoworld.com/article/3600993/9-fine-libraries-for-profiling-python-code.html) is a recent article, as I write. Of these, Palanteer is interesting in its mixed mode profiling capabilities (Python and C++). I'll have to explore it, perhaps not just yet though. A priori the `cProfiler` coming with the Python base library is all I need for this immediate use case. \n",
    "\n",
    "# Runtime profiling\n",
    "\n",
    "I will skip on the details of the imported libraries here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa7d6f4-527b-4f46-b449-298f3fb32cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from swift2.simulation import create_subarea_simulation, create_catchment\n",
    "# etc. etc, other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881ebe12-58ed-4a38-84f1-15cdf94c80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from swift2.utils import mk_full_data_id\n",
    "from swift2.parameteriser import create_parameteriser, create_parameter_sampler, create_sce_termination_wila, get_default_sce_parameters\n",
    "from swift2.doc_helper import get_free_params, sample_series\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from typing import TYPE_CHECKING\n",
    "if TYPE_CHECKING:\n",
    "    from swift2.classes import Simulation\n",
    "\n",
    "from cinterop.cffi.marshal import TIME_DIMNAME, slice_xr_time_series, pd_series_to_xr_series, as_timestamp\n",
    "\n",
    "from swift2.system import runoff_model_ids, runoff_model_var_ids\n",
    "from swift2.utils import paste_2, vpaste\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-sensitivity",
   "metadata": {},
   "source": [
    "## Creating synthethic hydrologic model\n",
    "\n",
    "The overall purpose of the exercise will be to measure performance under various conditions: model structure, size, time steps, etc. We will not do all that in this post; suffice to say we define a set of functions to create synthetic model setups. We do not show the full definitions. To give a flavour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4442e93d-4f0e-48fe-9e8b-7fbf199895fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cinterop.timeseries import mk_hourly_xarray_series\n",
    "\n",
    "def create_pulses(nTimeSteps, start) : return mk_hourly_xarray_series( createPulse(nTimeSteps, 5.0, 48), start)\n",
    "\n",
    "def create_uniform(nTimeSteps, start) : return mk_hourly_xarray_series( createUniform(nTimeSteps, 1.0), start)\n",
    "\n",
    "def set_test_input(ms:'Simulation', subAreaName:'str', rainVarName, petVarName, rain_ts, pet_ts):\n",
    "    p_id = mk_full_data_id('subarea', subAreaName, rainVarName)\n",
    "    e_id = mk_full_data_id('subarea', subAreaName, petVarName)\n",
    "    ms.play_input( rain_ts, p_id)\n",
    "    ms.play_input( pet_ts, e_id)\n",
    "    \n",
    "# def create_line_system(n_time_steps, n_links, rain_varname = \"P\", pet_varname = \"E\", area_km2 = 1):\n",
    "# et caetera    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76fa0d5-9614-43c8-8a43-69d5c37939b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def create_line_system(n_time_steps, n_links, rain_varname = \"P\", pet_varname = \"E\", area_km2 = 1):\n",
    "    node_ids = [str(x) for x in range(1,n_links+2)]\n",
    "    node_names = [\"node_\" + str(x) for x in node_ids]\n",
    "    link_ids = [str(x) for x in range(1,n_links+1)]\n",
    "    link_names = ['lnk_' + str(x) for x in link_ids]\n",
    "    link_from_node = node_ids[:-1]\n",
    "    link_to_node = node_ids[1:] \n",
    "\n",
    "    ms = create_catchment(node_ids, node_names, link_ids, link_names, link_from_node, link_to_node)\n",
    "\n",
    "    sa_ids = ms.get_subarea_ids()\n",
    "\n",
    "    setTestTimeSpan(n_time_steps, ms)\n",
    "    start = ms.get_simulation_span()['start']\n",
    "    rain_ts = create_pulses(n_time_step, start)\n",
    "    pet_ts = create_uniform(n_time_step, start)\n",
    "\n",
    "    for sa in sa_ids:\n",
    "        set_test_input(ms, sa, rain_varname, pet_varname, rain_ts, pet_ts)\n",
    "    return ms\n",
    "\n",
    "def setTestTimeSpan(nTimeSteps, ms:'Simulation'):\n",
    "    startDate = pd.Timestamp(year=1989, month=1, day=1)\n",
    "    endDate = startDate + pd.DateOffset(hours=(nTimeSteps-1))\n",
    "    ms.set_simulation_span(startDate, endDate)\n",
    "    ms.set_simulation_time_step(\"hourly\")\n",
    "\n",
    "\n",
    "def _moduval(n, value):\n",
    "    def f(i):\n",
    "        return value if i % n == 0 else 0\n",
    "    return f\n",
    "\n",
    "def createPulse(nTimeSteps, value, pulsePeriod):\n",
    "    f = _moduval(pulsePeriod, value)\n",
    "    x = [f(i) for i in range(nTimeSteps)]\n",
    "    return np.array(x).astype(float)\n",
    "\n",
    "def createUniform(nTimeSteps, value):\n",
    "    x = np.empty(shape=(nTimeSteps,))\n",
    "    x[:] = value\n",
    "    return x.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7e84d-1fff-4cce-b788-71433ee7fff5",
   "metadata": {},
   "source": [
    "We are now ready to create our catchment simulation. Before we plunge into `cProfiler` let's use a simpler way to assess the runtime from notebooks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f10881-8459-491f-bfa6-315a3d88c774",
   "metadata": {},
   "source": [
    "## Using notebook's `%%time`\n",
    "\n",
    "Let's create a setup with 15 subareas, hourly time step over 10 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c2529d-fbc9-4a27-b1c1-66c053bd370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 888 ms, sys: 8.94 ms, total: 897 ms\n",
      "Wall time: 897 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "n_time_step = 10 * 365 * 24\n",
    "ms = create_line_system(n_time_step, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139058d-3d15-4e41-be5b-ad3e452a14b3",
   "metadata": {},
   "source": [
    "Well, this was only the create of the baseline model, not even execution, and this already takes close to a second. Granted, there are a fair few hours in a decade. Still, a whole second!\n",
    "\n",
    "What about the simulation runtime? Let's parameterise minimally to avoid possible artefacts, and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5439ec7-99dc-4d0e-a400-e02f8bbf02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swift2.doc_helper import configure_hourly_gr4j, get_free_params\n",
    "configure_hourly_gr4j(ms)\n",
    "p = create_parameteriser('Generic subarea', get_free_params(\"GR4J\"))\n",
    "p.apply_sys_config(ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0e0f1-4ca4-49cd-88e6-c9f6950aa786",
   "metadata": {},
   "source": [
    "Double check we are indeed running hourly over 10 years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657a0a42-4510-452e-9577-ee7964883dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': datetime.datetime(1989, 1, 1, 0, 0),\n",
       " 'end': datetime.datetime(1998, 12, 29, 23, 0),\n",
       " 'time step': 'hourly'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.get_simulation_span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8bcaaf-2e89-4edb-9904-f3d9539cb4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 314 ms, sys: 691 µs, total: 315 ms\n",
      "Wall time: 314 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ms.exec_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b439aaf-3b95-4fd7-a7d3-5191a9276bcf",
   "metadata": {},
   "source": [
    "This is actually quite good, and \"unexpectedly\" less than the model creation itself. This is actually not all that surprising. All of the model execution happens in C++ land. The model setup involves much more operations in python.\n",
    "\n",
    "Let's look at an operation exchanging data from the C++ engine for display in Python. The model simulation has some of its states receiving input time series:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9962cddb-dbd1-483c-a49e-856f2a3a92a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subarea.1.E',\n",
       " 'subarea.1.P',\n",
       " 'subarea.10.E',\n",
       " 'subarea.10.P',\n",
       " 'subarea.11.E',\n",
       " 'subarea.11.P']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.get_played_varnames()[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2c661-ceb7-4ac4-8658-e49c0e7eca20",
   "metadata": {},
   "source": [
    "Let's see what happens in the retrieval of one of these input time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecda5294-432f-412e-a000-d9171dc11bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 441 ms, sys: 106 µs, total: 441 ms\n",
      "Wall time: 440 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts = ms.get_played(\"subarea.1.E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f664a-8b32-44f2-905c-b344987a187f",
   "metadata": {},
   "source": [
    "This is substantial; more than the native execution over a catchment with 15 subareas. So:\n",
    "\n",
    "* Can we identify the hotspot(s)?\n",
    "* Can we do something to improve it.\n",
    "\n",
    "## Profiling\n",
    "\n",
    "Enter `cProfile`, as we will stick with this in this post. Adapting some of the sample code shown in the [Python documentation on profilers](https://docs.python.org/3/library/profile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ac3ae9-8f52-45f5-ae66-cbacd97e241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f4bb25-b9c4-42b6-ab23-7a123fa8ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats, io\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "ts = ms.get_played(\"subarea.1.E\")\n",
    "pr.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76f59a17-65b3-4737-aeda-f09a3289e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = io.StringIO()\n",
    "sortby = pstats.SortKey.CUMULATIVE\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe02d1b-fee8-427c-a10c-25ac327fc8db",
   "metadata": {},
   "source": [
    "We will print only the top 5 % of the list of function calls, and see if we can spot the likely hotspot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5e84da-05d4-4540-9e5a-401bb7dfb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         6137 function calls (5964 primitive calls) in 0.445 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 588 to 29 due to restriction <0.05>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.445    0.222 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3361(run_code)\n",
      "        2    0.000    0.000    0.445    0.222 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.445    0.445 /tmp/ipykernel_34182/2688031072.py:4(<cell line: 4>)\n",
      "        1    0.000    0.000    0.445    0.445 /home/per202/src/swift/bindings/python/swift2/swift2/classes.py:471(get_played)\n",
      "        1    0.000    0.000    0.445    0.445 /home/per202/src/swift/bindings/python/swift2/swift2/play_record.py:190(get_played)\n",
      "        1    0.000    0.000    0.444    0.444 /home/per202/src/swift/bindings/python/swift2/swift2/internal.py:94(internal_get_played_tts)\n",
      "        1    0.000    0.000    0.444    0.444 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/data_set.py:10(get_multiple_time_series_from_provider)\n",
      "        1    0.000    0.000    0.444    0.444 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:69(internal_get_multiple_time_series)\n",
      "        1    0.000    0.000    0.441    0.441 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:76(<listcomp>)\n",
      "        1    0.000    0.000    0.441    0.441 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:74(f)\n",
      "        1    0.000    0.000    0.441    0.441 /home/per202/src/datatypes/bindings/python/uchronia/uchronia/internals.py:79(internal_get_single_model_time_series)\n",
      "        1    0.002    0.002    0.441    0.441 /home/per202/src/swift/bindings/python/swift2/swift2/wrap/swift_wrap_custom.py:216(get_played_pkg)\n",
      "        1    0.000    0.000    0.438    0.438 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:386(geom_to_xarray_time_series)\n",
      "        1    0.000    0.000    0.368    0.368 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:367(ts_geom_to_even_time_index)\n",
      "        1    0.000    0.000    0.368    0.368 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py:22(create_even_time_index)\n",
      "        1    0.368    0.368    0.368    0.368 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py:25(<listcomp>)\n",
      "       16    0.000    0.000    0.071    0.004 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/dataarray.py:365(__init__)\n",
      "        1    0.000    0.000    0.070    0.070 /home/per202/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:356(create_ensemble_series)\n",
      "        9    0.000    0.000    0.070    0.008 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:74(as_variable)\n",
      "        1    0.000    0.000    0.070    0.070 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/dataarray.py:90(_infer_coords_and_dims)\n",
      "       36    0.000    0.000    0.070    0.002 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:181(as_compatible_data)\n",
      "    35/31    0.060    0.002    0.060    0.002 {built-in method numpy.asarray}\n",
      "        1    0.000    0.000    0.009    0.009 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:172(_possibly_convert_objects)\n",
      "        1    0.000    0.000    0.009    0.009 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/series.py:323(__init__)\n",
      "        2    0.000    0.000    0.009    0.005 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/construction.py:470(sanitize_array)\n",
      "        2    0.000    0.000    0.009    0.005 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/construction.py:695(_try_cast)\n",
      "        1    0.000    0.000    0.009    0.009 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1466(maybe_infer_to_datetimelike)\n",
      "        2    0.000    0.000    0.006    0.003 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py:1994(_sequence_to_dt64ns)\n",
      "        1    0.000    0.000    0.006    0.006 /home/per202/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1499(try_datetime)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps.print_stats(.05)\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e3d7c-eaa7-42e3-bdbe-38ab76394cff",
   "metadata": {},
   "source": [
    "in a file rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py the function `create_even_time_index` appears to be where a lengthy operation occurs. More precisely, when this function does a list comprehension (`listcomp`). Note that I infer this because in the `cumtime` (cumulative time) column there is a drop from 0.396 ms for `listcomp` to 0.071 for the rest of the operations under this function. I think this is the right way to interpret it in this case, but it may not be the case in other profiling context.\n",
    "\n",
    "The code for `create_even_time_index` is at [this permalink](https://github.com/csiro-hydroinformatics/rcpp-interop-commons/blob/f7dff4b83d8f01a8ae71e16fb903b80fa4f23f5d/bindings/python/cinterop/cinterop/timeseries.py#L22)\n",
    "\n",
    "```python\n",
    "def create_even_time_index(start:ConvertibleToTimestamp, time_step_seconds:int, n:int) -> List:\n",
    "    start = as_timestamp(start)\n",
    "    delta_t = np.timedelta64(time_step_seconds, 's')\n",
    "    return [start + delta_t * i for i in range(n)]\n",
    "```\n",
    "\n",
    "`[start + delta_t * i for i in range(n)]` is the bulk of the time, .396 out of a total 0.477 ms. \n",
    "\n",
    "This list is created as the basis for a time index for the creation of the `xarray` object returned by the overall `get_played` function. So, is there a faster way to create this time series index?\n",
    "\n",
    "## Performance tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b05162-9c22-4328-9802-a7d7a6f92d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp(year=2000, month=1, day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "011ccb52-2695-4697-b273-277ae908bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 24*365*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45276d7-564d-4be4-adb0-a3e8bcf6b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_index_creation(start, n:int) -> List:\n",
    "    start = as_timestamp(start)\n",
    "    time_step_seconds = 3600\n",
    "    delta_t = np.timedelta64(time_step_seconds, 's')\n",
    "    return [start + delta_t * i for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d5c04f-3d7a-4ea8-a283-a375d6a5319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 352 ms, sys: 562 µs, total: 353 ms\n",
      "Wall time: 351 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = test_index_creation(start, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957df77-175e-4071-9855-03add460feaf",
   "metadata": {},
   "source": [
    "`start` is a pandas `Timestamp`, and we add to it an object of type `np.timedelta64` 87600 times. I doubt this is the main issue, but let's operate in numpy types as much as we can by converting the pd.Timestamp once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d925e550-39b1-4017-8f03-d4f49e7ed608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2000-01-01 00:00:00')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7975c82-2cb1-481d-ad2d-b8875c4b410f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2000-01-01T00:00:00.000000000')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start.to_datetime64()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10092da1-5029-4181-b829-ad141eb6d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_index_creation(start, n:int) -> List:\n",
    "    start = as_timestamp(start).to_datetime64()\n",
    "    time_step_seconds = 3600\n",
    "    delta_t = np.timedelta64(time_step_seconds, 's')\n",
    "    return [start + delta_t * i for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26ec9f72-8933-4855-bf88-f5990570b301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 293 ms, sys: 8.48 ms, total: 301 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = test_index_creation(start, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a1364-7722-483f-bb95-fe6bc7113275",
   "metadata": {},
   "source": [
    "This is actually more of an improvement than I anticipated. OK. What else can we do?\n",
    "\n",
    "Pandas has the helpful [Time series / date functionality](https://pandas.pydata.org/docs/user_guide/timeseries.html) page in its documentation. The function from which we started is generic, but for important cases such as hourly and daily time steps, there are options to use the `freq` argument to the `date_range` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90dc70c0-f0c7-462a-8470-8149bf440bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 242 µs, sys: 686 µs, total: 928 µs\n",
      "Wall time: 520 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 01:00:00',\n",
       "               '2000-01-01 02:00:00', '2000-01-01 03:00:00',\n",
       "               '2000-01-01 04:00:00', '2000-01-01 05:00:00',\n",
       "               '2000-01-01 06:00:00', '2000-01-01 07:00:00',\n",
       "               '2000-01-01 08:00:00', '2000-01-01 09:00:00',\n",
       "               ...\n",
       "               '2009-12-28 14:00:00', '2009-12-28 15:00:00',\n",
       "               '2009-12-28 16:00:00', '2009-12-28 17:00:00',\n",
       "               '2009-12-28 18:00:00', '2009-12-28 19:00:00',\n",
       "               '2009-12-28 20:00:00', '2009-12-28 21:00:00',\n",
       "               '2009-12-28 22:00:00', '2009-12-28 23:00:00'],\n",
       "              dtype='datetime64[ns]', length=87600, freq='H')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pd.date_range(start, periods=n, freq=\"H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1121459-2f4c-4f5f-9f42-f1d95ded43a0",
   "metadata": {},
   "source": [
    "It is two order of magnitude faster... Definitely worth a re-engineering of the features in the `timeseries.py` we started from. \n",
    "\n",
    "This probably does not solve the issue for many other cases (irregular time steps, e.g. monthly), but there are many cases where we could benefit. The [date_range documentation](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html?highlight=date_range) specifies that an arbitrary [DateOffset](https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.DateOffset.html) to its `freq` argument (`freq: str or DateOffset, default ‘D’`). How efficient is this operation on our 87600 data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3fca2ac-d459-4be8-96c9-f1a2decf5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_offset = pd.tseries.offsets.DateOffset(minutes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86e077c8-a051-49c9-bd25-0a64030e2524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2000-01-01 00:15:00')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start + d_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45c64772-6546-4ad9-8f3b-af59f3fbe7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 1.32 ms, total: 1.5 s\n",
      "Wall time: 1.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 00:15:00',\n",
       "               '2000-01-01 00:30:00', '2000-01-01 00:45:00',\n",
       "               '2000-01-01 01:00:00', '2000-01-01 01:15:00',\n",
       "               '2000-01-01 01:30:00', '2000-01-01 01:45:00',\n",
       "               '2000-01-01 02:00:00', '2000-01-01 02:15:00',\n",
       "               ...\n",
       "               '2002-07-01 09:30:00', '2002-07-01 09:45:00',\n",
       "               '2002-07-01 10:00:00', '2002-07-01 10:15:00',\n",
       "               '2002-07-01 10:30:00', '2002-07-01 10:45:00',\n",
       "               '2002-07-01 11:00:00', '2002-07-01 11:15:00',\n",
       "               '2002-07-01 11:30:00', '2002-07-01 11:45:00'],\n",
       "              dtype='datetime64[ns]', length=87600, freq='<DateOffset: minutes=15>')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pd.date_range(start, periods=n, freq=d_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e62bb-a575-4577-b4bc-c67ef5d3e176",
   "metadata": {},
   "source": [
    "It looks like in this case this is actually a fair bit slower than my original implementation. Interesting. And using `start.to_datetime64()` makes no difference too.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebca2d-8d3e-4ee8-ad3a-f9ef52b220f7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This demonstrated a relatively simple, but real case where `cProfiler` helps alleviate usually small but pervasive runtime inefficiencies. In this case, so far we have not needed to look close to the Python/C interoperability layer. The key bottleneck was pure python. I envisage I may post something later on looking at trickier situation.\n",
    "\n",
    "To be honest, I did second guess a few things upfront. But the `cProfiler` and simpler `%%time` brought at least confirmation and sometimes useful, conter-intuitive insight. \"You cannot manage what you cannot measure\" as the saying goes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c3b401-9349-4a98-b3cb-167646070165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "HFC",
   "language": "python",
   "name": "hydrofc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
