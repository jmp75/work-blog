[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "J-M's 'lab notebook'",
    "section": "",
    "text": "blogging about the waa tool port from fortran.\nquarto installed thanks to apt version 1.1.165\nquarto create-project --type website:blog .\nquarto install extension quarto-ext/video\n\ncp -r $HOME/src/work-blog/_notebooks/* posts\ncp -r $HOME/src/work-blog/_posts/* posts\n\ncp $HOME/src/work-blog/images/* posts\ncp -r $HOME/src/work-blog/images/copied_from_nb/* posts/\n\nnbdev_migrate --path posts\nusage: nbdev_migrate [-h] [--fname FNAME] [--disp] [--stdin] [--no_skip]\nnbdev_migrate: error: unrecognized arguments: -v\n(bm) abcdef@keywest-bm:~/src/blog2$ conda list | grep nbdev\nnbdev                     2.2.8                      py_0    fastai\nmamba update -c fastai -c conda-forge nbdev\n  - nbdev       2.2.8  py_0            fastai                       \n  + nbdev       2.3.6  py_0            fastai/noarch            63kB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "J-M’s ‘lab notebook’",
    "section": "",
    "text": "Windows installer with WiX 4 - part 1\n\n\n\n\n\n\n\nWiX\n\n\ninstallers\n\n\n\n\n\n\n\nBuilding a Windows installer with WiX 4 - part 1\n\n\n\n\n\n\nMar 16, 2023\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nC++ static code analysis\n\n\n\n\n\n\n\nC++\n\n\nmemory\n\n\nsafety\n\n\n\n\nC++ static code analysis\n\n\n\n\n\n\nFeb 23, 2023\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nRunning a Windows VM with Vagrant on Linux\n\n\n\n\n\n\n\nkvm\n\n\nvagrant\n\n\nqemu\n\n\n\n\nRunning a Windows VM with Vagrant on Linux, trying via Qemu and VirtualBox\n\n\n\n\n\n\nNov 22, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nElectron+python multi-os installers\n\n\n\n\n\n\n\nAzure\n\n\nElectron\n\n\npackaging\n\n\n\n\nBuilding multiplatform installers for an electron application with an embedded python server\n\n\n\n\n\n\nNov 16, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nCustom rules for gitleaks\n\n\n\n\n\n\n\nSecurity\n\n\ngit\n\n\n\n\nAdding rules to a gitleaks configuration to scan git repo for low-medium sensitivity information\n\n\n\n\n\n\nNov 11, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nUsing nbdev on Windows - part 2\n\n\n\n\n\n\n\nPython\n\n\nNotebook\n\n\nIDE\n\n\nnbdev\n\n\n\n\nGetting through a tutorial to test the operation of nbdev on Windows native\n\n\n\n\n\n\nOct 8, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nUsing nbdev on Windows\n\n\n\n\n\n\n\nPython\n\n\nNotebook\n\n\nIDE\n\n\n\n\nTesting setting up and using nbdev on Windows\n\n\n\n\n\n\nOct 7, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nCompiling C++ libraries to WebAssembly - Part 1\n\n\n\n\n\n\n\nC++\n\n\nWASM\n\n\nWebAssembly\n\n\nemscripten\n\n\nWASI\n\n\n\n\nFirst attempt at compiling a streamflow forecasting software stack to WebAssembly\n\n\n\n\n\n\nSep 11, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nGenerate programming language bindings to a C API\n\n\n\n\n\n\n\ncode generation\n\n\ninterop\n\n\nC++\n\n\nPython\n\n\n\n\nGenerate language binging for Python, R and other wrappers on top of a C API\n\n\n\n\n\n\nSep 3, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nPresenting doxygen C++ API documentation via MkDocs with doxybook2\n\n\n\n\n\n\n\ndocumentation\n\n\nC++\n\n\nPython\n\n\nMkDocs\n\n\n\n\nConvert doxygen C++ API documentation to Markdown with doxybook2, to generate a gorgeous site with MkDocs\n\n\n\n\n\n\nAug 22, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nRuntime profiling of python bindings for a C/C++ library\n\n\n\n\n\n\n\nC++\n\n\nPython\n\n\nperformance\n\n\nruntime\n\n\n\n\nFinding a runtime hotspot in a software stack with Python and, potentially, c++\n\n\n\n\n\n\nAug 9, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nAzure devops CI pipeline for hydrologic forecasting software\n\n\n\n\n\n\n\nrecipes\n\n\nVCPP\n\n\nC++\n\n\nDebian\n\n\nWindows\n\n\nPython packaging\n\n\nR packaging\n\n\n\n\nAzure devops pipeline for a software stack with c++, Python and R packages across multiple repositories.\n\n\n\n\n\n\nJul 31, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nA whinge about Azure Devops\n\n\n\n\n\n\n\nAzure\n\n\ndevops\n\n\nMicrosoft\n\n\n\n\nA bumpy ride trying to do a relatively simple thing, publish packages from a build pipeline.\n\n\n\n\n\n\nJul 27, 2022\n\n\nJ-M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLithology classification using Hugging Face, part 3\n\n\n\n\n\n\n\nhugging-face\n\n\nNLP\n\n\nlithology\n\n\n\n\nFirst working lithology classification training on the Namoi data\n\n\n\n\n\n\nJul 3, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nUpgrading a C++ streamflow compilation toolchain\n\n\n\n\n\n\n\nrecipes\n\n\nVCPP\n\n\nC++\n\n\n\n\nMoving a legacy Visual C++ 13 toolchain to Build Tools 2017\n\n\n\n\n\n\nJun 26, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nSetting up a private conda channel - part 1\n\n\n\n\n\n\n\nconda\n\n\nconda-channel\n\n\n\n\nResearch paths to set up an enterprise-wide private conda channel\n\n\n\n\n\n\nJun 18, 2022\n\n\nJ-M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLithology classification using Hugging Face, part 2\n\n\n\n\n\n\n\nhugging-face\n\n\nNLP\n\n\nlithology\n\n\n\n\nExploring the use of NLP to exploit geologic information. Trying to classify lithologies.\n\n\n\n\n\n\nJun 13, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nConda package for compiled native libraries\n\n\n\n\n\n\n\nrecipes\n\n\nconda\n\n\nconda-forge\n\n\nC++\n\n\n\n\nJourney in building a conda package for C++ libraries\n\n\n\n\n\n\nJun 10, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nSubmitting your first conda package to conda-forge\n\n\n\n\n\n\n\nrecipes\n\n\nconda\n\n\nconda-forge\n\n\n\n\nJourney in building my first conda package and ending up submitting to conda-forge\n\n\n\n\n\n\nJun 4, 2022\n\n\nJ-M\n\n\n\n\n\n\n  \n\n\n\n\nLithology classification using Hugging Face, part 1\n\n\n\n\n\n\n\nhugging-face\n\n\nNLP\n\n\nlithology\n\n\n\n\nExploring the use of NLP to exploit geologic information. Get to know your data.\n\n\n\n\n\n\nJun 1, 2022\n\n\nJ-M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVictorian Water Measurement Information System\n\n\n\n\n\n\n\njupyter\n\n\nhydstra\n\n\n\n\nTrying to simply get access via REST to data in https://data.water.vic.gov.au.\n\n\n\n\n\n\nMay 28, 2022\n\n\nJ-M & Andrew Freebairn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20230223-cpp-static-analysis/index.html",
    "href": "posts/20230223-cpp-static-analysis/index.html",
    "title": "C++ static code analysis",
    "section": "",
    "text": "“C++ logo”"
  },
  {
    "objectID": "posts/20230223-cpp-static-analysis/index.html#moirai",
    "href": "posts/20230223-cpp-static-analysis/index.html#moirai",
    "title": "C++ static code analysis",
    "section": "Moirai",
    "text": "Moirai\nMoirai is a library is handling object lifetime of C++ objects used by external languages.\n\ncppcheck\nScaning the library with the command:\ncppcheck -I include/ src/ 2> err.txt\nflags no error. cppcheck is known for minimising false positives.\n\n\ncoverity scan\nMoving on to scan with coverity via coverity scan:\n\nButton “Register my github project”\nYou need a manual refresh of your repo list with “Please load the repository list to select from your GitHub projects”\n\nRegister Your Open Source GitHub Project on Coverity Scan\n“Once submitted, your project will be available when analysis is completed. It can take up to 48 hours for the first analysis to complete” (in practice, much less delay)\n“Upload a Project Build”\nOne needs to download the Coverity Scan Build Tool: C/C++ and follow instructions. cov-analysis-linux64-2022.06.tar.gz for linux64 in my case. This is a 1.1GB download, so it takes some time.\nCheck the download with md5sum cov-analysis-linux64-2022.06.tar.gz\ntar zxpvf cov-analysis-linux64-2022.06.tar.gz \nls cov-analysis-linux64-2022.6.0\nmv cov-analysis-linux64-2022.6.0 ~/bin/\nUpdate the PATH env var to add ~/bin/cov-analysis-linux64-2022.6.0/bin as per the instructions. Check with which cov-build.\nThen moving to build the project:\ncd ~/src/moirai/build\nmake clean\ncov-build --dir cov-int make -j 4 \ntail cov-int/build-log.txt\n# Emitted 10 C/C++ compilation units (100%) successfully\ntar caf moirai.bz2 cov-int\nThere is a warning “Linux users on kernel version 4.8.x and newer will need to apply a sysctl parameter to support our binaries”. I am running on a Linux xxxxxx 5.10.0-21-amd64, and this seems to have worked fine.\nversion 1.2.0\nPressing “Upload Tar File”\nLast Build Status: In-queue. Your build is in the queue to be analyzed. There are 3 builds ahead of it.\nAn email notifies:\n    Analysis Summary:\n       Defects found: 19\n\nThe badge says the code passes the scan; a priori good. The defect density of 0.23, which is below the 0.35 for projects less than 100K LOC (as of 2013 though).\nCWE-676: Use of Potentially Dangerous Function. “Likelihood Of Exploit” is High.\n\n\n\n\n\nCWE-676\n\n\nUsing the “View Defects” button opens a dashboard with more details of the various potential issues. Most appear to arise in the third party codebase, the header-only unit test framework used, Catch. This happens to be an old version as well, so a newer version may not have these issues. But this is a low concern at any rate, as unit tests are not compiled into the library itself.\nIt is not entirely clear which issue is flagged “CWE-676” actually, perhaps:\n\n\n\n\n\nUse of 32-bit time_t\n\n\nThere are a couple of issues mentioned that may impact the library itself, but nothing that warrants particular concern and more suggestions for improvements (e.g. “A move contructor may improve speed”)"
  },
  {
    "objectID": "posts/20230223-cpp-static-analysis/index.html#uchronia-time-series",
    "href": "posts/20230223-cpp-static-analysis/index.html#uchronia-time-series",
    "title": "C++ static code analysis",
    "section": "uchronia time series",
    "text": "uchronia time series\n\ncppcheck\nUsing cppcheck on Uchronia reports some “double memory freeing” errors, which I think are more a software stability than security concern.\ninclude/datatypes/time_series_io.hpp:1037:15: error: Memory pointed to by 'values' is freed twice. [doubleFree]\n     delete[] values;\nUpon inspection this actually appears to be a false flag, where the memory is freed once before an exception is thrown:\n  if (code != NC_NOERR) {\n    delete[] values;\n    ExceptionUtilities::ThrowInvalidOperation(\"SetVariableDimOne failed for variable ID \" + std::to_string(varId));\n  }\nMaybe this is a suggestion that the body should be refactored with a try / finally syntax, but this is a low priority task.\ninclude/datatypes/time_series_strategies.hpp:477:12: error: Array index -1 is out of bounds. [negativeContainerIndex]\n    return buffer[-1];\nThis is indeed odd at first sight, but upon inspection this is again a line that cannot be reached because of an exception being thrown beforehand, and added to mote a compiler complaint. Still, this probably warrants something less quirky.\nMoving on to using coverity\n\n\ncoverity scan\n“No top 25 CWE defects were found.”\nDefect density of 0.10 for 185,219 LOC. Most of the lines of course would result from third party work including Boost libraries, so the low density may well be mostly because of third party work.\nThere are a couple of suggestions of a type that would improve performance:\n\n\n\n\n\n“auto_causes_copy”\n\n\nOther types of errors deal with uninitialised object members. Whether these are false positives, or very unlikely to occur given typical practices, these should be followed up.\n\n\n\n\n\n“uninit in ctor”"
  },
  {
    "objectID": "posts/2022-09-11-cpp-code-wasm.html",
    "href": "posts/2022-09-11-cpp-code-wasm.html",
    "title": "Compiling C++ libraries to WebAssembly - Part 1",
    "section": "",
    "text": "“web-assembly-logo.png 1”"
  },
  {
    "objectID": "posts/2022-09-11-cpp-code-wasm.html#trying-on-one-of-my-c-codebase",
    "href": "posts/2022-09-11-cpp-code-wasm.html#trying-on-one-of-my-c-codebase",
    "title": "Compiling C++ libraries to WebAssembly - Part 1",
    "section": "Trying on one of my c++ codebase",
    "text": "Trying on one of my c++ codebase\nThe library MOIRAI: Manage C++ Objects’s lifetime when exposed through a C API may be a natural starting point, as it is a workhorse for the C API of many larger libraries. It is small in size, but non-trivial, and the unit tests already feature techniques like C API with opaque pointers and template programming. It can be compiled using cmake.\nAdapting emscripten instructions for projects to my case (where emcmake $CM below is emcmake cmake -DBLAH_LOTS_OF_OPTIONS .., we’ll get back to this)\ncd ${GITHUB_REPOS}/moirai\nmkdir -p embuild ; cd embuild\nemcmake $CM\nI get:\nCMake Error at /home/xxxyyy/src/emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake:136 (message):\n  System LLVM compiler cannot be used to build with Emscripten! Check\n  Emscripten's LLVM toolchain location in .emscripten configuration file, and\n  make sure to point CMAKE_C_COMPILER to where emcc is located.  (was\n  pointing to \"gcc\")\nGoogling for “System LLVM compiler cannot be used to build with Emscripten” was rather confusing me. My full expanded, failing command line looks like:\nemcmake cmake -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON ..\nMaybe removing most arguments?\nemcmake cmake -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON ..\nNope, still not, same message. Well, how about trying to “point CMAKE_C_COMPILER to where emcc is located”?\nemcmake cmake -DCMAKE_CXX_COMPILER=/home/xxxyyy/src/emsdk/upstream/emscripten/em++ -DCMAKE_C_COMPILER=/home/xxxyyy/src/emsdk/upstream/emscripten/emcc -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON ..\nSurprise, it works. Albeit with a warning.\nCMake Warning (dev) at CMakeLists.txt:145 (ADD_LIBRARY):\n  ADD_LIBRARY called with SHARED option but the target platform does not\n  support dynamic linking.  Building a STATIC library instead.  This may lead\n  to problems.\nBring it on. Battle scarred. Let’s plodder and see.\nemmake make\nresults in:\nScanning dependencies of target moirai\n[  6%] Building CXX object CMakeFiles/moirai.dir/src/reference_handle.cpp.o\n[ 12%] Building CXX object CMakeFiles/moirai.dir/src/reference_handle_map_export.cpp.o\n[ 18%] Linking CXX static library libmoirai.a\n[ 18%] Built target moirai\n# etc. etc.\nScanning dependencies of target moirai_test_api\n[ 81%] Building CXX object CMakeFiles/moirai_test_api.dir/tests/moirai_test_api/main.cpp.o\n[ 87%] Linking CXX executable moirai_test_api.js\nwasm-ld: error: duplicate symbol: free_string\n>>> defined in libmoirai_test_lib_a.a(c_interop_api.cpp.o)\n>>> defined in libmoirai_test_lib.a(c_interop_api.cpp.o)\nOk, not bad, really, compiling all the object files succeeds already. It is at linking stage that this falters, and given the above warning about shared/static libraries, fair enough."
  },
  {
    "objectID": "posts/2022-09-11-cpp-code-wasm.html#appendix-other-resources-for-next-steps",
    "href": "posts/2022-09-11-cpp-code-wasm.html#appendix-other-resources-for-next-steps",
    "title": "Compiling C++ libraries to WebAssembly - Part 1",
    "section": "Appendix: other resources for next steps",
    "text": "Appendix: other resources for next steps\n\nVery interested in Porting libffi to pure WebAssembly, and also the company profile of the author.\nminimal-cmake-emscripten-project\nCompiling an Existing C Module to WebAssembly\nThe Rust community seems to be quite actively interested in WASM\nBytecode Alliance\n\nUpdates\n\nhttps://tpiros.dev/blog/bring-your-cplusplus-application-to-the-web-with-web-assembly/\nhttps://medium.com/@tdeniffel/pragmatic-compiling-from-c-to-webassembly-a-guide-a496cc5954b8\nhttps://github.com/WebAssembly/tool-conventions/blob/main/DynamicLinking.md\nhttps://github.com/emscripten-core/emscripten/wiki/WebAssembly-Standalone#create-a-dynamic-library\nhttps://www.infoworld.com/article/3619608/13-hot-language-projects-riding-webassembly.html\nhttps://leaningtech.com/cheerp-2-7-compile-c-to-webassembly-plus-javascript/\nAdd a simple example about how to build your own C++ Pyodide package\nopenssl in pyodide may be informative as to interop.\n\nMore 2022-09-17:\n\nhttps://emscripten.org/docs/compiling/Dynamic-Linking.html. Basically, the question is how I use this in a cmakelists. Any starting point?\nMay not address shared modules, but well written: https://peterdn.com/post/2020/12/31/building-c-and-cpp-libraries-for-webassembly/\nsmall but didactic: https://medium.com/@arora.aashish/webassembly-dynamic-linking-1644c9f40c8c\nhttps://docs.wasmtime.dev/tutorial-create-hello-world.html\nhttps://madewithwebassembly.com/\nhttps://github.com/emscripten-core/emscripten/issues/17245\n\nThis issue from Thorsten Beier, QuantStack crowd I believe.\nFurther experimentations:\nemcmake cmake \\\n  -DCMAKE_CXX_COMPILER=/home/xxxyyy/src/emsdk/upstream/emscripten/em++ \\\n  -DCMAKE_C_COMPILER=/home/xxxyyy/src/emsdk/upstream/emscripten/emcc \\\n  -DCMAKE_PREFIX_PATH=/usr/local \\\n  -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules \\\n  -DCMAKE_INSTALL_PREFIX=/home/xxxyyy/tmp \\\n  -DCMAKE_PROJECT_INCLUDE=moirai_emscripten.cmake \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DBUILD_SHARED_LIBS=ON .. \n\n\nemmake make VERBOSE=yes\nerror: undefined symbol: clone_handle_b_in_domain_a (referenced by top-level compiled C/C++ code)\nwarning: _clone_handle_b_in_domain_a may need to be added to EXPORTED_FUNCTIONS if it arrives from a system library\nmoirai_emscripten.cmake has, borrowed from something in pyodide (geos)\nset_property(GLOBAL PROPERTY TARGET_SUPPORTS_SHARED_LIBS TRUE)\nset(CMAKE_SHARED_LIBRARY_CREATE_C_FLAGS \"-s EXPORT_ALL=1 -s SIDE_MODULE=1\")\nset(CMAKE_SHARED_LIBRARY_CREATE_CXX_FLAGS \"-s EXPORT_ALL=1 -s SIDE_MODULE=1\")\nset(CMAKE_STRIP FALSE)  # used by default in pybind11 on .so modules\nbut any of the .so files e.g. emnm libmoirai_test_lib_a.so returns libmoirai_test_lib_a.so: no symbols\nIssue CMake SHARED library with Emscripten toolchain is not supported may hold clues regarding multi-target cmakes.\nemcmake: SIDE_MODULE can’t find another SIDE_MODULE when link"
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#building-the-python-server",
    "href": "posts/20221116-waa-installers/index.html#building-the-python-server",
    "title": "Electron+python multi-os installers",
    "section": "Building the python server",
    "text": "Building the python server\nThe docker image cdrx/pyinstaller-windows which uses Wine to emulate Windows.\npyinstaller --clean -y --dist ./dist/windows --workpath /tmp -F --additional-hooks-dir=hooks --add-data \"waasrv/server/templates;templates\" --add-data \"waasrv/server/static;static\" run_app.py"
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#building-the-application-with-electron",
    "href": "posts/20221116-waa-installers/index.html#building-the-application-with-electron",
    "title": "Electron+python multi-os installers",
    "section": "Building the application with Electron",
    "text": "Building the application with Electron\nThe overall application is build using the docker image electronuserland/builder:wine-mono. I am not sure why the wine-mono tag is used, but I’ll look at it if I have to.\ncd app\ncp package.json package.tmp\n\necho entrypoint:VERSION_NUMBER:${VERSION_NUMBER}\necho entrypoint:CURRENT_UID:${CURRENT_UID}\nyarn version --no-git-tag-version --no-commit-hooks --new-version ${VERSION_NUMBER}\ncd ..\nyarn\necho yarn package-win-publish...\nyarn package-win-publish\n\ncd app\nrm package.json\nmv package.tmp package.json\nyarn package-win-publish a bit curious since it seems to be to publish to GitHub releases, which we are not using."
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#windows-32-bits",
    "href": "posts/20221116-waa-installers/index.html#windows-32-bits",
    "title": "Electron+python multi-os installers",
    "section": "Windows 32 bits",
    "text": "Windows 32 bits\nI am told that some of the potential users may run older versions of Windows, perhaps 32 bits.\npyinstaller’s page supporting multiple operating systems states that:\n\nIf you need to distribute your application for more than one OS, for example both Windows and macOS, you must install PyInstaller on each platform and bundle your app separately on each.\n\nThe docker image at cdrx/pyinstaller-windows includes a 32 bit image as well, with the tag python3-32bit: cdrx/pyinstaller-windows:python3-32bit, so by similarity this should be easy to swap the docker image and get a 32 bits python app installer for the server side.\nIf I get the doc of electron-build right, I should prepare package.json with new commands, moving from:\n    \"scripts\": {\n        ## things\n        \"package-win\": \"yarn build && electron-builder build --win --x64\",\n        \"package-win-publish\": \"yarn build && electron-builder build --win --x64 --publish always\",\n        ## things\n    }\nto:\n    \"scripts\": {\n        ## things\n        \"package-win64\": \"yarn build && electron-builder build --win --x64\",\n        \"package-win64-publish\": \"yarn build && electron-builder build --win --x64 --publish always\",\n        \"package-win32\": \"yarn build && electron-builder build --win --ia32\",\n        \"package-win32-publish\": \"yarn build && electron-builder build --win --ia32 --publish always\",\n        ## things\n    }"
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#macos",
    "href": "posts/20221116-waa-installers/index.html#macos",
    "title": "Electron+python multi-os installers",
    "section": "MacOS",
    "text": "MacOS\nA priori, need to build on the macos image. There is no docker image such as a cdrx/pyinstaller-macos that I know of, so I doubt we can minimally repurpose the existing pipeline as we did for win 32 and 64."
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#first-increment",
    "href": "posts/20221116-waa-installers/index.html#first-increment",
    "title": "Electron+python multi-os installers",
    "section": "First increment",
    "text": "First increment\nWe’ll start from scratch at the top level of the workflow. One thing I am unsure of is how to gather multiple artifacts (installers) over several different jobs or stages. Let’s explore.\nIf I use the possibly naive:\n# A simplified pipeline to test building installers for multiple platforms \n\ntrigger:\n- main\n\nresources:\n- repo: self\n\nvariables:\n  tag: '$(Build.BuildId)'\n\n# to get a custom '$(Build.BuildNumber)', and 'r' is a counter reset to 1 every change of the major/minor versions\nname: '0.1.$(Rev:r)'\n\nstages:\n- stage: WindowsInstallers\n  displayName: WindowsInstallers\n  jobs:\n  - job: BuildJob\n    strategy:\n      matrix:\n        'Windows64':\n          arch_tag: 'win64'\n          pyinstaller_tag: 'python3'\n        'Windows32':\n          arch_tag: 'win32'\n          pyinstaller_tag: 'python3-32bit'\n    displayName: Build Tool windows installer\n    pool:\n      vmImage: ubuntu-latest\n    steps:\n    - checkout: self\n      submodules: 'true'\n    - task: Bash@3\n      displayName: 'Build $(arch_tag) pyinstaller'\n      inputs:\n        targetType: 'inline'\n        script: |\n          mkdir -p $(Build.ArtifactStagingDirectory)/$(arch_tag)\n          cd $(Build.ArtifactStagingDirectory)/$(arch_tag)\n          echo $(arch_tag) > installer.txt\n      env: \n        VERSION_NUMBER: '$(Build.BuildNumber)'\n    - task: Bash@3\n      displayName: 'listing task $(arch_tag)'\n      inputs:\n        targetType: 'inline'\n        script: |\n          ls -l $(Build.ArtifactStagingDirectory)\n\n- stage: CheckContents\n  dependsOn: WindowsInstallers\n  displayName: CheckContents\n  jobs:\n  - job: BuildJob\n    displayName: List pipeline content artifacts\n    pool:\n      vmImage: ubuntu-latest\n    steps:\n    - checkout: self\n      submodules: 'true'\n    - task: Bash@3\n      displayName: 'listing task'\n      inputs:\n        targetType: 'inline'\n        script: |\n          ls -l $(Build.ArtifactStagingDirectory)\n      env: \n        VERSION_NUMBER: '$(Build.BuildNumber)'\nThe first listing task “listing task win64” returns drwxr-xr-x 2 vsts docker 4096 Nov 16 10:04 win64\nThe first listing task “listing task win32” returns drwxr-xr-x 2 vsts docker 4096 Nov 16 10:05 win32: the folder win64 is not present anymore. This may be obvious to seasoned devops, but this was not to me. So, something is missing to persist the artifacts between jobs, and a fortiori between stages."
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#second-increment-pipeline-artifacts",
    "href": "posts/20221116-waa-installers/index.html#second-increment-pipeline-artifacts",
    "title": "Electron+python multi-os installers",
    "section": "Second increment: pipeline artifacts",
    "text": "Second increment: pipeline artifacts\nSee Publish and download pipeline Artifacts and adapt. Although there is something I find quite confusing in that doc.\nTo the stage: WindowsInstallers let’s add:\n    - publish: '$(Build.ArtifactStagingDirectory)/$(arch_tag)'\n      displayName: 'Publish $(arch_tag)'\n      artifact: arti_$(arch_tag)\nThe MS documentation states that:\n\nBy default, artifacts are downloaded to $(Pipeline.Workspace)\n\nso let’s add to the stage CheckContents:\n    - download: current\n      artifact: arti_win32\n    - download: current\n      artifact: arti_win64\n    - task: Bash@3\n      displayName: 'listing task'\n      inputs:\n        targetType: 'inline'\n        script: |\n          echo ls -l $(Build.ArtifactStagingDirectory)\n          ls -l $(Build.ArtifactStagingDirectory)\n          echo ls -l $(Pipeline.Workspace)\n          ls -l $(Pipeline.Workspace)\nls -l $(Pipeline.Workspace) indeed then returns:\ntotal 24\ndrwxr-xr-x 2 vsts docker 4096 Nov 16 10:27 TestResults\ndrwxr-xr-x 2 vsts docker 4096 Nov 16 10:27 a\ndrwxr-xr-x 2 vsts docker 4096 Nov 16 10:27 arti_win32\ndrwxr-xr-x 2 vsts docker 4096 Nov 16 10:27 arti_win64\ndrwxr-xr-x 2 vsts docker 4096 Nov 16 10:27 b\ndrwxr-xr-x 3 vsts docker 4096 Nov 16 10:27 s\nI think we now have a way to persist installers between stages, to then collate them in a bigger output artifact (not a pipeline artifact, but a project artifact)"
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#third-increment",
    "href": "posts/20221116-waa-installers/index.html#third-increment",
    "title": "Electron+python multi-os installers",
    "section": "Third increment",
    "text": "Third increment\nWe change the “real” pipeline from its single architecture 64 to packaging for win 32 and 64. Leave MacOS out for now.\nWe add the matrix strategy for the build job:\n    strategy:\n      matrix:\n        'Windows64':\n          arch_tag: 'win64'\n          pyinstaller_tag: 'python3'\n        'Windows32':\n          arch_tag: 'win32'\n          pyinstaller_tag: 'python3-32bit'\n\npackaging the python server as an app.exe\nThe server Docker file now should have an argument PYINSTALLER_TAG:\nARG PYINSTALLER_TAG=python3\nFROM cdrx/pyinstaller-windows:${PYINSTALLER_TAG}\nwhich is passed from the main startup shell script with:\nPYINSTALLER_TAG=$1\ndocker build --no-cache --force-rm \\\n    --build-arg CURRENT_UID_ARG=${CURRENT_UID} \\\n    --build-arg PYINSTALLER_TAG=${PYINSTALLER_TAG} -t ${IMAGE_NAME} .\nwhich is launched from the pipeline by:\n    - task: Bash@3\n      displayName: 'Build python-based server for Windows with pyinstaller $(arch_tag)'\n      inputs:\n        targetType: 'inline'\n        script: |\n          cd $(Build.SourcesDirectory)/server\n          ./build-server.sh $(pyinstaller_tag)\n\n\nElectron app packaging\nWe start from a build that was implicitely for 64 bits only:\n    - task: Bash@3\n      displayName: 'Build the UI, and package with Electron'\n      inputs:\n        targetType: 'inline'\n        script: |\n          cd $(Build.SourcesDirectory)/docker\n          ./build-electron.sh\n      env: \n        VERSION_NUMBER: '$(Build.BuildNumber)'\nStarting from the inside out, the electron-builder command that starts the packaging is in the docker container entrypoint.sh, which now needs an ARCH variable\necho yarn package-${ARCH}-publish...\nyarn package-${ARCH}-publish\nWe can pass it as an environment variable set in the docker file:\nFROM electronuserland/builder:wine-mono\n\nARG CURRENT_UID_ARG\nARG VERSION_NUMBER_ARG\nARG ARCH_ARG\n\nENV CURRENT_UID=${CURRENT_UID_ARG}\nENV VERSION_NUMBER=${VERSION_NUMBER_ARG}\nENV ARCH=${ARCH_ARG}\n\nWORKDIR /src\n\nCMD [\"./docker/entrypoint.sh\"]\nWhich is passed from the docker build script with:\nARCH=$1\ndocker build --no-cache --force-rm \\\n    --build-arg CURRENT_UID_ARG=${CURRENT_UID} \\\n    --build-arg VERSION_NUMBER_ARG=${VERSION_NUMBER} \\\n    --build-arg ARCH_ARG=${ARCH} \\\n    -t ${IMAGE_NAME} .\nand the argument is passed from the pipeline with:\n    - task: Bash@3\n      displayName: 'Build the UI, and package with Electron for $(arch_tag)'\n      inputs:\n        targetType: 'inline'\n        script: |\n          cd $(Build.SourcesDirectory)/docker\n          ./build-electron.sh $(arch_tag)\n      env: \n        VERSION_NUMBER: '$(Build.BuildNumber)'\n\n\nAdding the pipeline artifacts between jobs\nWe need to publish the built installer, so that we can retrieve it in a subsequent job, or stage.\n    - task: CopyFiles@2\n      displayName: 'Copy build output to the artifact staging directory'\n      inputs:\n        sourceFolder: '$(Build.SourcesDirectory)/release'\n        contents: '*.*'\n        targetFolder: '$(Build.ArtifactStagingDirectory)/release/$(arch_tag)'\n    - publish: '$(Build.ArtifactStagingDirectory)/release/$(arch_tag)'\n      displayName: 'Publish $(arch_tag)'\n      artifact: arti_$(arch_tag)"
  },
  {
    "objectID": "posts/20221116-waa-installers/index.html#fourth-increment-add-macos.",
    "href": "posts/20221116-waa-installers/index.html#fourth-increment-add-macos.",
    "title": "Electron+python multi-os installers",
    "section": "Fourth increment: Add MacOS.",
    "text": "Fourth increment: Add MacOS.\nFor MacOS, we probably cannot have the same approach. The docker image used for electron-builder can package applications for each of the targets. However pyinstaller needs to run on an OSX host to package the server side for it, and we cannot do this easily and/or legally in a docker container.\nOther questions:\n\ntarget the architecture x86_64 (MacOS on Intel) or arm64 (MacOS on Apple M1 and M2 chips)\n\nI may tackle this in another post."
  },
  {
    "objectID": "posts/2022-05-28-hydstra-rest-data.html",
    "href": "posts/2022-05-28-hydstra-rest-data.html",
    "title": "Victorian Water Measurement Information System",
    "section": "",
    "text": "About\nTrying to retrieve time series or streamflow measurements via a REST API. Or anything that works for that matter.\nThis builds on work done by my colleague Andrew Freebairn, which really gave me a leg up.\n\n\nWeb API: which one?\nI am not a regular user of online data. Whenever I really needed to fetch data from an online service, this tended to be an experience where reverting back to using a Web GUI with a “download data” button the path of least pain.\nBefore writing this section I got confused, trying to use kiwis-pie to retrieve data from the VIC data web site. Which is not suitable. I thought all Kisters products would be accessible using a unified web API. But Hydstra has its own separate from WIKSI, so far as I understand. See https://kisters.com.au/webservices.html\nCuriously what seems to be the documentation for the Json API to Hydstra is in a page named HYDLLP – Flat DLL Interface to Hydstra. The examples are given in Perl, which is, well, not all that mainstream anymore.\nThere are visibly data codes to identify the types of data, at least to retrieve time series data. Query parameters such as varfrom and varto are kind of odd in an API though, and would deserve an explanation. It is only after manually downloading a time series from the VIC water data GUI that there is an explanation note about these variable codes.\nThe VIC data web site offers a way to directly link to URLs for particular data, but this seems not to work when used: https://data.water.vic.gov.au?ppbm=405288|B_405_GOULBURN|WEB_SW&rs&1&rsdf_org\nrdmw.qld.gov.au providing documentation for how to build json queries.\n\nimport requests\nimport json\nimport urllib\nimport pandas as pd\nfrom datetime import date\nfrom IPython.core.display import HTML\nfrom IPython.display import JSON\n\n\ndef vic_url_builder(json_p, query):\n    endpoint = 'http://data.water.vic.gov.au/cgi/webservice.exe'\n    return f\"{endpoint}?{json.dumps(json_p)}&{urllib.parse.urlencode(query)}\"\n\nThe site identifiers of interest are:\n\nsite_id_405284 = '405284' # SUNDAY CREEK @ MT DISAPPOINTMENT\nsite_id_405288 = '405288' # SUNDAY CREEK @ MT DISAPPOINTMENT (US SUNDAY CK. STORAGE\nsite_id_405287 = '405287' # WESTCOTT CREEK @ MT DISAPPOINTMENT US SUNDAY CK STORAGE\nsite_id_405239 = '405239' # SUNDAY CREEK @ CLONBINANE\n\n\n# This took me a bit of time to figure out how to extract cumecs. One has to figure out that the 'varfrom' in the query parameter needs to bhe the river level in metres. \nvar_code_level_metres = '100' # \nvar_code_cumecs = '140' # cumecs\nvar_code_mlpd = '141' # ML/Day\nver = {\n  'ver': '2.0'\n}\n\n#station_list = '405288' # don't set as 1-element list though: fails...\n\nstation_list = ','.join([\nsite_id_405284,\nsite_id_405288,\nsite_id_405287,\nsite_id_405239,\n])\n\ndef dt_str(x): return x.strftime('%Y%m%d%H%M%S')\n\nget_ts = {\n    'function':'get_ts_traces',\n    'version':'2',\n    'params':{\n        'site_list':station_list,\n        'datasource':'A', # THere are other codes, but this one is the only that returned something so far I can see.\n        'varfrom':var_code_level_metres,\n        'varto':var_code_cumecs,\n        'start_time':'20220101000000',\n        'end_time':'20220529000000',\n        'interval':'day',\n        'multiplier':'1',\n        'data_type':'mean',\n    }\n}\nquery = vic_url_builder(get_ts, ver)\n\n\nresponse = requests.get(query)\n\n\nif response.status_code == requests.codes.ok:\n    print('Return Json')\n    print(response.headers['content-type'])\n    response_json = response.json()\n\n# for visualisation in a notebook (not sure about rendered in a blog?)\nJSON(response_json)\n\nReturn Json\ntext/html\n\n\n<IPython.core.display.JSON object>\n\n\n\ntype(response_json)\n\ndict\n\n\n\ntraces = response_json['return']['traces']\nlen(traces)\n\n2\n\n\nWe got only trace time series for 2 of the 4 sites. This is because these sites have no recent data, at least not for the preiod of interest. Fair enough. For instance 405284 SUNDAY CREEK @ MT DISAPPOINTMENT functioned from from 23/06/1981 to 05/07/1984\n\ntraces[0].keys()\n\ndict_keys(['error_num', 'compressed', 'site_details', 'quality_codes', 'trace', 'varfrom_details', 'site', 'varto_details'])\n\n\n\ntraces[0]['site']\n\n'405288'\n\n\n\ntr = traces[0]\ntr['quality_codes']\n\n{'255': '',\n '100': 'Irregular data, use with caution.',\n '2': 'Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction'}\n\n\n\ntraces[1]['quality_codes']\n\n{'255': '',\n '150': 'Rating extrapolated above 1.5x maximum flow gauged.',\n '149': 'Rating extrapolated 1.5 times the maximum flow gauged.',\n '2': 'Good quality data - minimal editing required. +/- 0mm - 10mm Drift correction'}\n\n\nSo this has good quality data, and some missing data (255). I am a bit puzzled by the codes for ratings beyond the max flow gauge, but the value appears to be sometimes zero flows in the data. Odd.\nAfter a bit of iterative discovery, a way to turn json data to pandas series:\n\nimport numpy as np\n\ndef to_dt64(x):\n    return pd.Timestamp(x).to_datetime64()\n\ndef make_daily_time_series(tr):\n    site_name = tr['site']\n    y = pd.DataFrame(tr['trace'])\n    y.v = pd.to_numeric(y.v)\n    v = y.v.values\n    q = y.q.values\n    y.t = y.t.apply(str)\n    v[q == 255] = np.nan\n    index = y.t.apply(to_dt64).values\n    ts = pd.Series(v, index=index, name=site_name)\n    return ts\n\n\nts = make_daily_time_series(traces[0])\nts.plot(ylabel=\"m3/s\", xlabel=\"Time\", title = ts.name)\n\n<AxesSubplot:title={'center':'405288'}, xlabel='Time', ylabel='m3/s'>\n\n\n\n\n\n\nts = make_daily_time_series(traces[1])\nts.plot(ylabel=\"m3/s\", xlabel=\"Time\", title = ts.name)\n\n<AxesSubplot:title={'center':'405287'}, xlabel='Time', ylabel='m3/s'>\n\n\n\n\n\n\n\nRelated resources\nAndrew Freebairn’s python package ‘bomwater’"
  },
  {
    "objectID": "posts/20221111-gitleaks-config/index.html",
    "href": "posts/20221111-gitleaks-config/index.html",
    "title": "Custom rules for gitleaks",
    "section": "",
    "text": "The Three S’es, from comedy cult Black Books S1E5 ‘The Big Lock-Out’"
  },
  {
    "objectID": "posts/20221111-gitleaks-config/index.html#installation",
    "href": "posts/20221111-gitleaks-config/index.html#installation",
    "title": "Custom rules for gitleaks",
    "section": "Installation",
    "text": "Installation\n\n\n\n\n\n\nTip\n\n\n\nIn general I’d prefer to build tools from source if I can. for gitleaks I figured I needed sudo apt golang to then try make build, but bumped into:\nconfig/config.go:4:2: package embed is not in GOROOT (/usr/lib/go-1.15/src/embed)\ndetect/detect.go:8:2: package io/fs is not in GOROOT (/usr/lib/go-1.15/src/io/fs)\nwhich is a bit of a bump for folks not familiar with the go ecosystem. Binary releases are readily available; your choice to install. See this issue."
  },
  {
    "objectID": "posts/20221111-gitleaks-config/index.html#trial-basic-use",
    "href": "posts/20221111-gitleaks-config/index.html#trial-basic-use",
    "title": "Custom rules for gitleaks",
    "section": "Trial basic use",
    "text": "Trial basic use\nPick a random repo to test, and see what happen.\ngitleaks detect -r report.json:\n6:22PM INF 284 commits scanned.\n6:22PM INF scan completed in 260ms\n6:22PM WRN leaks found: 2\nHuh oh… What? Thankfully this is a false positive: some generated R code uses arbitrary tokens, with no security implications. It does pick the commit at which this information was added.\n {\n  \"Description\": \"Generic API Key\",\n  \"StartLine\": 2,\n  \"EndLine\": 3,\n  \"StartColumn\": 14,\n  \"EndColumn\": 1,\n  \"Match\": \"token: 10BE3573-1514-4C36-9D1C-5A225CD40393\",\n  \"Secret\": \"10BE3573-1514-4C36-9D1C-5A225CD40393\",\n  \"File\": \"path/to/some/R/RcppExports.R\",\n  \"SymlinkFile\": \"\",\n  \"Commit\": \"21324567890oiuasheroiuhawoiruhaoiuwrh\",\n  \"Entropy\": 3.6943858,\n  \"Author\": \"J-M\",\n  \"Email\": \"some.guy@example.com\",\n  \"Date\": \"2017-04-05T06:20:11Z\",\n  \"Message\": \"Add R package 'blah' wrapping for mylibrary.dll\",\n  \"Tags\": [],\n  \"RuleID\": \"generic-api-key\",\n  \"Fingerprint\": \"21324567890oiuasheroiuhawoiruhaoiuwrh:path/to/some/R/RcppExports.R:generic-api-key:2\"\n },"
  },
  {
    "objectID": "posts/20221111-gitleaks-config/index.html#custom-rules",
    "href": "posts/20221111-gitleaks-config/index.html#custom-rules",
    "title": "Custom rules for gitleaks",
    "section": "Custom rules",
    "text": "Custom rules\nOK, I get the gist of the tool, and this is appealing.\nCreating a test repository with a readme, to further test default and upcoming custom rules.\nStarting with a segment of the readme like the following, which I expect violates the rul “Generic API Key”\nusername: abcdef\npassword: password\nNope.\nI suspect this is because there is an entropy threshold to consider a password is indeed a secret, rather than a dummy documentation artifact. So, let’s complicate the password:\npassword: ;oiaspgoih-1514-4C36-9D1C-poainrpoiunpwe4i\nNope.\nOn a complete hunch: because starting character is not a letter or number?\npassword: oiaspgoih-1514-4C36-9D1C-poainrpoiunpwe4i\nBingo!, that was it. The underlying regex rules are quite complicated (to me at least), so not quite sure what is going on, but not flagging the line password: ;oiaspgoih-1514-4C36-9D1C-poainrpoiunpwe4i may be a small flaw in the tool.\nI note that we have a high entropy in the reported match:\n  \"Match\": \"password: oiaspgoih-1514-4C36-9D1C-poainrpoiunpwe4i\",\n  \"Secret\": \"password: oiaspgoih-1514-4C36-9D1C-poainrpoiunpwe4i\",\n  \"File\": \"README.md\",\n  \"SymlinkFile\": \"\",\n  \"Commit\": \"19fbedaa6e0cf9907d8779583aa391f49bb09a03\",\n    \"Entropy\": 4.3028474,"
  },
  {
    "objectID": "posts/20221111-gitleaks-config/index.html#custom-rule-with-a-low-entropy-threshold",
    "href": "posts/20221111-gitleaks-config/index.html#custom-rule-with-a-low-entropy-threshold",
    "title": "Custom rules for gitleaks",
    "section": "Custom rule with a low entropy threshold",
    "text": "Custom rule with a low entropy threshold\nWe create a file ~/.config/gitleaks.toml. Let’s say we want to catch anything that remotely looks like a password (though still restricted by lines starting with the characters password):\n# Title for the gitleaks configuration file.\ntitle = \"J-M's gitleaks config\"\n[extend]\n# useDefault will extend the base (this) configuration with the default gitleaks config:\n# https://github.com/zricethezav/gitleaks/blob/master/config/gitleaks.toml\nuseDefault = true\n\n[[rules]]\n# Unique identifier for this rule\nid = \"any-kind-of-password\"\n# Short human readable description of the rule.\ndescription = \"Any passwords\"\nregex = '''password.*:.*'''\n# Float representing the minimum shannon entropy a regex group must have to be considered a secret.\nentropy = 0\ngitleaks -c ~/.config/gitleaks.toml detect -r report.json\n {\n  \"Description\": \"Any passwords\",\n  \"StartLine\": 70,\n  \"EndLine\": 70,\n  \"StartColumn\": 2,\n  \"EndColumn\": 19,\n  \"Match\": \"password: password\",\n  \"Secret\": \"password: password\",\n  \"File\": \"README.md\",\n  \"SymlinkFile\": \"\",\n  \"Commit\": \"5c4782a83541baa26e98f8c162d2eede1744d316\",\n  \"Entropy\": 3.0588138,\n  \"Author\": \"J-M\",\n  \"Email\": \"blah@blah.au\",\n  \"Date\": \"2022-11-11T06:13:53Z\",\n  \"Message\": \"Try to trigger rule for password\",\n  \"Tags\": [],\n  \"RuleID\": \"any-kind-of-password\",\n  \"Fingerprint\": \"5c4782a83541baa26e98f8c162d2eede1744d316:README.md:any-kind-of-password:70\"\n },"
  },
  {
    "objectID": "posts/2022-07-03-lithology-classification-hugging-face-3.html",
    "href": "posts/2022-07-03-lithology-classification-hugging-face-3.html",
    "title": "Lithology classification using Hugging Face, part 3",
    "section": "",
    "text": "This is a continuation of Lithology classification using Hugging Face, part 2.\nThe “Part 2” post ended up on an error on calling trainer.train, with incompatible tensor dimensions in a tensor multiplication. It was not clear at all (to me) what the root issue was. After getting back to basics and looking at the HF Text classification how-to, I noticed that my Dataset contained pytorch tensors or lists thereof, where the how-do just had simple data types.\nLong story short, I removed the tokernizer’s parameter return_tensors=\"pt\", and did not call tok_ds.set_format(\"torch\"), and surprised, it worked. I had added these because the initial trial complained about a mix of GPU and CPU data.\n\n\nAt this stage, it is worthwhile laying out a roadmap of where this line of work may go:\n\nComplete a classification on at least a subset of the Namoi dataset (this post)\nUpload a trained model to Hugging Face Hub, or perhaps fastai X Hugging Face Group 2022\nSet up a Gradio application on HF Spaces\nProject proposal at work. Weekend self-teaching can only go so far."
  },
  {
    "objectID": "posts/2022-07-03-lithology-classification-hugging-face-3.html#dealing-with-imbalanced-classes-with-weights",
    "href": "posts/2022-07-03-lithology-classification-hugging-face-3.html#dealing-with-imbalanced-classes-with-weights",
    "title": "Lithology classification using Hugging Face, part 3",
    "section": "Dealing with imbalanced classes with weights",
    "text": "Dealing with imbalanced classes with weights\n\nsorted_counts = litho_logs_kept[MAJOR_CODE].value_counts()\nclass_weights = (1 - sorted_counts / sorted_counts.sum()).values\nclass_weights = torch.from_numpy(class_weights).float().to(\"cuda\")"
  },
  {
    "objectID": "posts/2022-07-03-lithology-classification-hugging-face-3.html#tokenisation",
    "href": "posts/2022-07-03-lithology-classification-hugging-face-3.html#tokenisation",
    "title": "Lithology classification using Hugging Face, part 3",
    "section": "Tokenisation",
    "text": "Tokenisation\n\np = Path(\"./tokz_pretrained\")\npretrained_model_name_or_path = p if p.exists() else STARTING_MODEL\n\n# Tokenizer max length\nmax_length = 128\n\n# https://discuss.huggingface.co/t/sentence-transformers-paraphrase-minilm-fine-tuning-error/9612/4\ntokz = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=True, max_length=max_length, model_max_length=max_length)\nif not p.exists():\n    tokz.save_pretrained(\"./tokz_pretrained\")\n\nWe know from the previous post that we should work with lowercase descriptions to have a more sensible tokenisation\n\nlitho_logs_kept[DESC] = litho_logs_kept[DESC].str.lower()\nlitho_logs_kept_mini = litho_logs_kept[[MAJOR_CODE_INT, DESC]]\nlitho_logs_kept_mini.sample(n=10)\n\n\n\n\n\n  \n    \n      \n      MajorLithoCodeInt\n      Description\n    \n  \n  \n    \n      88691\n      3\n      shale\n    \n    \n      77323\n      11\n      siltstone\n    \n    \n      42318\n      0\n      clay fine sandy water supply\n    \n    \n      85089\n      1\n      gravel; as above, except gravels 70% 2-10mm, 3...\n    \n    \n      112223\n      0\n      clay; 70%, light brown. coarse sand to fine gr...\n    \n    \n      35510\n      0\n      clay\n    \n    \n      106351\n      0\n      clay\n    \n    \n      80478\n      0\n      clay; ligth grey with brown streaks - with som...\n    \n    \n      20290\n      1\n      gravel\n    \n    \n      23426\n      0\n      clay, gravelly, blueish"
  },
  {
    "objectID": "posts/2022-07-03-lithology-classification-hugging-face-3.html#create-dataset-and-tokenisation",
    "href": "posts/2022-07-03-lithology-classification-hugging-face-3.html#create-dataset-and-tokenisation",
    "title": "Lithology classification using Hugging Face, part 3",
    "section": "Create dataset and tokenisation",
    "text": "Create dataset and tokenisation\nWe will use a subset sample of the full dataset to train on, for the sake of execution speed, for now\n\nlen(litho_logs_kept_mini)\n\n123657\n\n\n\nlitho_logs_kept_mini_subset = litho_logs_kept_mini.sample(len(litho_logs_kept_mini) // 4)\nlen(litho_logs_kept_mini_subset)\n\n30914\n\n\n\nds = Dataset.from_pandas(litho_logs_kept_mini_subset)\n\ndef tok_func(x):\n    return tokz(\n        x[DESC],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n        # return_tensors=\"pt\", ## IMPORTANT not to use return_tensors=\"pt\" here, perhaps conter-intuitively\n    )\n\n\ntok_ds = ds.map(tok_func)\nnum_labels = len(labels_kept)\n\nParameter 'function'=<function tok_func at 0x7f49f6e17a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n\n\n\n\n\n\n# NOTE: the local caching may be superflous\np = Path(\"./model_pretrained\")\n\nmodel_name = p if p.exists() else STARTING_MODEL\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, max_length=max_length)\n                                                           # label2id=label2id, id2label=id2label).to(device) \nif not p.exists():\n    model.save_pretrained(p)\n\n\nprint(type(model))\n\n<class 'transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification'>\n\n\n\ntok_ds = tok_ds.rename_columns({MAJOR_CODE_INT: \"labels\"})\n\n\n# Keep the description column, which will be handy later despite warnings at training time.\ntok_ds = tok_ds.remove_columns(['__index_level_0__'])\n# tok_ds = tok_ds.remove_columns(['Description', '__index_level_0__'])\n\n\n# Not sure why, but cannot set the labels class otherwise `train_test_split` complains\n# tok_ds.features['labels'] = labels\ndds = tok_ds.train_test_split(test_size=0.25, seed=42)\n\n\n# Defining the Trainer to compute Custom Loss Function, adapted from [Simple Training with the 🤗 Transformers Trainer, around 840 seconds](https://youtu.be/u--UVvH-LIQ?t=840)\nclass WeightedLossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Feed inputs to model and extract logits\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # Extract Labels\n        labels = inputs.get(\"labels\")\n        # Define loss function with class weights\n        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n        # Compute loss\n        loss = loss_func(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\ndef compute_metrics(eval_pred):\n    labels = eval_pred.label_ids\n    predictions = eval_pred.predictions.argmax(-1)\n    f1 = f1_score(labels, predictions, average=\"weighted\")\n    return {\"f1\": f1}\n\n\noutput_dir = \"./hf_training\"\nbatch_size = 64 # 128 causes a CUDA out of memory exception... Maybe I shoudl consider dynamic padding instead. Later.\nepochs = 3 # low, but for didactic purposes will do.\nlr = 8e-5  # inherited, no idea whether appropriate. is there an lr_find in hugging face?\n\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=epochs,\n    learning_rate=lr,\n    lr_scheduler_type=\"cosine\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size * 2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    logging_steps=len(dds[\"train\"]),\n    fp16=True,\n    push_to_hub=False,\n    report_to=\"none\",\n)\n\n\nmodel = model.to(\"cuda:0\")\n\nThe above nay not be strictly necessary, depending on your version of transformers. I bumped into the following issue, which was probably the transformers 4.11.3 bug: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dds[\"train\"],\n    eval_dataset=dds[\"test\"],\n    tokenizer=tokz,\n    compute_metrics=compute_metrics,\n)\n\nUsing amp half precision backend"
  },
  {
    "objectID": "posts/2022-07-03-lithology-classification-hugging-face-3.html#training",
    "href": "posts/2022-07-03-lithology-classification-hugging-face-3.html#training",
    "title": "Lithology classification using Hugging Face, part 3",
    "section": "Training",
    "text": "Training\n\ntrainer.train()\n\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/home/abcdef/miniconda/envs/hf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 23185\n  Num Epochs = 3\n  Instantaneous batch size per device = 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1089\n\n\n\n\n    \n      \n      \n      [1089/1089 04:57, Epoch 3/3]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      F1\n    \n  \n  \n    \n      1\n      No log\n      0.072295\n      0.983439\n    \n    \n      2\n      No log\n      0.063188\n      0.985492\n    \n    \n      3\n      No log\n      0.061934\n      0.986534\n    \n  \n\n\n\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 7729\n  Batch size = 128\nSaving model checkpoint to ./hf_training/checkpoint-500\nConfiguration saved in ./hf_training/checkpoint-500/config.json\nModel weights saved in ./hf_training/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./hf_training/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./hf_training/checkpoint-500/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 7729\n  Batch size = 128\nSaving model checkpoint to ./hf_training/checkpoint-1000\nConfiguration saved in ./hf_training/checkpoint-1000/config.json\nModel weights saved in ./hf_training/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./hf_training/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./hf_training/checkpoint-1000/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 7729\n  Batch size = 128\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(global_step=1089, training_loss=0.16952454397938907, metrics={'train_runtime': 297.9073, 'train_samples_per_second': 233.479, 'train_steps_per_second': 3.655, 'total_flos': 2304099629568000.0, 'train_loss': 0.16952454397938907, 'epoch': 3.0})"
  },
  {
    "objectID": "posts/2022-07-03-lithology-classification-hugging-face-3.html#exploring-results",
    "href": "posts/2022-07-03-lithology-classification-hugging-face-3.html#exploring-results",
    "title": "Lithology classification using Hugging Face, part 3",
    "section": "Exploring results",
    "text": "Exploring results\nThis part is newer compared to the previous post, so I will elaborate a bit.\nI am not across the high level facilities to assess model predictions (visualisation, etc.) so what follows may be sub-optimal and idiosyncratic.\n\ntest_pred = trainer.predict(trainer.eval_dataset)\n\nThe following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: Description. If Description are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 7729\n  Batch size = 128\n\n\n\n    \n      \n      \n      [61/61 00:08]\n    \n    \n\n\n\ntest_pred\n\nPredictionOutput(predictions=array([[-0.519 , -2.127 ,  0.61  , ..., -1.751 , -1.145 , -2.832 ],\n       [ 0.8223,  2.123 ,  9.27  , ..., -3.254 , -0.8325, -1.929 ],\n       [-1.003 , -0.469 , -1.233 , ..., -1.084 , -0.7856, -0.4966],\n       ...,\n       [-0.53  , -1.396 , -0.5615, ..., -2.506 , -1.985 , -3.44  ],\n       [-0.453 , -1.442 , -0.621 , ..., -2.424 , -1.973 , -3.44  ],\n       [-1.388 , -2.346 , -1.186 , ..., -1.94  , -0.6084, -2.22  ]],\n      dtype=float16), label_ids=array([4, 2, 8, ..., 5, 5, 6]), metrics={'test_loss': 0.061934199184179306, 'test_f1': 0.9865336898918051, 'test_runtime': 8.7643, 'test_samples_per_second': 881.873, 'test_steps_per_second': 6.96})\n\n\nThis is lower level than I anticipated. The predictions array appear to be the logits. Note that I was not sure label_ids was, and it is not the predicted label, but the “true” label.\n\ntest_df = trainer.eval_dataset.to_pandas()\ny_true = test_df.labels.values.astype(int)\ny_true\n\narray([4, 2, 8, ..., 5, 5, 6])\n\n\nTo get the predicted labels, I seem to need to do the following song and dance:\n\npreds_tf = torch.asarray(test_pred.predictions, dtype=float)\npredictions = torch.nn.functional.softmax(preds_tf, dim=-1)\nhighest = np.argmax(predictions, axis=1)\ny_pred = np.array(highest)\ny_pred\n\narray([4, 2, 8, ..., 5, 5, 6])\n\n\n\ndiffer = np.logical_not(y_true == y_pred)\nprint(\"There are {0} records in the validation data set that differ from true labels\".format(np.sum(differ)))\n\nThere are 104 records in the validation data set that differ from true labels\n\n\nLet’s look at where we fail to match the labels:\n\ndiffering = test_df[differ]\n\n\nlbl_true = labels.int2str(differing.labels.values)\ndescriptions = differing.Description.values \nlbl_pred = labels.int2str(y_pred[differ])\n\n\npd.options.display.max_colwidth = 150\npd.options.display.max_rows = 110\n\n\npd.DataFrame.from_dict({\n    \"label_true\": lbl_true,\n    \"label_pred\": lbl_pred,\n    \"desc\": descriptions,\n})\n\n\n\n\n\n  \n    \n      \n      label_true\n      label_pred\n      desc\n    \n  \n  \n    \n      0\n      TPSL\n      GRNT\n      topoil; granite, grey\n    \n    \n      1\n      TPSL\n      CLAY\n      none\n    \n    \n      2\n      SDSN\n      CLAY\n      none\n    \n    \n      3\n      SHLE\n      CLAY\n      clay multicoloured sandy\n    \n    \n      4\n      TPSL\n      CLAY\n      none\n    \n    \n      5\n      SAND\n      GRNT\n      granite sand\n    \n    \n      6\n      SDSN\n      CLAY\n      gray\n    \n    \n      7\n      TPSL\n      CLAY\n      none\n    \n    \n      8\n      CLAY\n      SDCY\n      sandy clay\n    \n    \n      9\n      SDSN\n      CLAY\n      none\n    \n    \n      10\n      TPSL\n      CLAY\n      clay - brown, silty\n    \n    \n      11\n      SDSN\n      CLAY\n      brown\n    \n    \n      12\n      CLAY\n      SHLE\n      grey\n    \n    \n      13\n      SLSN\n      SDSN\n      grey soft silstone\n    \n    \n      14\n      SAND\n      SDCY\n      sandy bands, brown\n    \n    \n      15\n      SDCY\n      CLAY\n      clay sandy\n    \n    \n      16\n      BSLT\n      CLAY\n      none\n    \n    \n      17\n      UNKN\n      CLAY\n      carbonaceous wood - dark bluish black to black with associated associated minor khaki and dark grey clay.  sample retained for analysis\n    \n    \n      18\n      GRNT\n      SAND\n      grantie; ligh pinkish grey, medium, fragments of quartz, hornblende & mica, increased pink feldspar\n    \n    \n      19\n      UNKN\n      CLAY\n      none\n    \n    \n      20\n      SDSN\n      CLAY\n      none\n    \n    \n      21\n      ROCK\n      UNKN\n      missing\n    \n    \n      22\n      SLSN\n      SDSN\n      silstone\n    \n    \n      23\n      CLAY\n      GRVL\n      light grey medium to coarse sandy gravel - 30%, and gravelly clay - 70%.  gravel mainly basalt and jasper\n    \n    \n      24\n      CLAY\n      SDCY\n      sandy clay\n    \n    \n      25\n      SDSN\n      CLAY\n      none\n    \n    \n      26\n      GRVL\n      SAND\n      sand and gravel\n    \n    \n      27\n      SAND\n      SOIL\n      soil + sand\n    \n    \n      28\n      UNKN\n      CLAY\n      none\n    \n    \n      29\n      SHLE\n      CLAY\n      brown\n    \n    \n      30\n      CLAY\n      SAND\n      clayey  sand (brown) - fine-medium\n    \n    \n      31\n      UNKN\n      CLAY\n      white puggy some slightly hard\n    \n    \n      32\n      GRNT\n      CLAY\n      none\n    \n    \n      33\n      SHLE\n      CLAY\n      none\n    \n    \n      34\n      BSLT\n      CLAY\n      none\n    \n    \n      35\n      GRVL\n      CLAY\n      none\n    \n    \n      36\n      SHLE\n      CLAY\n      none\n    \n    \n      37\n      CLAY\n      SDCY\n      sandy and gravel aquifer with bands of clay\n    \n    \n      38\n      SDCY\n      CLAY\n      clay sandy\n    \n    \n      39\n      CLAY\n      SDSN\n      silty\n    \n    \n      40\n      CLAY\n      SDCY\n      sandy clay, light grey, fine\n    \n    \n      41\n      BSLT\n      SHLE\n      blue bassalt\n    \n    \n      42\n      CLAY\n      SDCY\n      sandy brown clay\n    \n    \n      43\n      CLAY\n      SAND\n      sand - silty up to 1mm, clayey\n    \n    \n      44\n      SOIL\n      CLAY\n      none\n    \n    \n      45\n      GRVL\n      SAND\n      wash alluvial\n    \n    \n      46\n      CLAY\n      SDCY\n      sandy clay\n    \n    \n      47\n      GRNT\n      CLAY\n      none\n    \n    \n      48\n      UNKN\n      CLAY\n      none\n    \n    \n      49\n      SDSN\n      SAND\n      sand - mostly white very fine to very coarse gravel\n    \n    \n      50\n      GRVL\n      CLAY\n      gravelly sandy clay\n    \n    \n      51\n      SOIL\n      CLAY\n      none\n    \n    \n      52\n      BSLT\n      SDSN\n      brown weathered\n    \n    \n      53\n      GRVL\n      SAND\n      brown sand and fine gravel\n    \n    \n      54\n      GRVL\n      SAND\n      course sand and gravel, w/b\n    \n    \n      55\n      SDCY\n      GRNT\n      silt, sandy/silty sand\n    \n    \n      56\n      TPSL\n      BSLT\n      blue basalt\n    \n    \n      57\n      GRVL\n      CLAY\n      stones clay\n    \n    \n      58\n      ROCK\n      CLAY\n      ochrs yellow\n    \n    \n      59\n      GRVL\n      ROCK\n      stone, clayed to semi formed sandstone\n    \n    \n      60\n      UNKN\n      SAND\n      soak water bearing\n    \n    \n      61\n      BSLT\n      SDSN\n      balsalt: weathered\n    \n    \n      62\n      SOIL\n      CLAY\n      none\n    \n    \n      63\n      UNKN\n      CLAY\n      very\n    \n    \n      64\n      GRVL\n      CLAY\n      gravelly sandy clay\n    \n    \n      65\n      SDSN\n      GRNT\n      granite sand\n    \n    \n      66\n      SHLE\n      CLAY\n      none\n    \n    \n      67\n      ROCK\n      UNKN\n      water bearing\n    \n    \n      68\n      BSLT\n      SAND\n      h/frac, quartz\n    \n    \n      69\n      MDSN\n      COAL\n      coal 80% & mudstone, 20%; dark grey, strong, carbonaceous\n    \n    \n      70\n      GRVL\n      CLAY\n      as above\n    \n    \n      71\n      CLAY\n      GRVL\n      gravelly clay\n    \n    \n      72\n      GRVL\n      SAND\n      sand + gravel (water)\n    \n    \n      73\n      CLAY\n      GRVL\n      with gravel\n    \n    \n      74\n      SAND\n      SDCY\n      sandy yellow\n    \n    \n      75\n      CLAY\n      SOIL\n      brown soil and clay\n    \n    \n      76\n      CLAY\n      SDCY\n      sandy clay\n    \n    \n      77\n      UNKN\n      CLAY\n      hard slightly stoney\n    \n    \n      78\n      SDSN\n      CLAY\n      none\n    \n    \n      79\n      SDSN\n      ROCK\n      sandsstone\n    \n    \n      80\n      TPSL\n      CLAY\n      none\n    \n    \n      81\n      SOIL\n      CLAY\n      none\n    \n    \n      82\n      SHLE\n      BSLT\n      shae (brown)\n    \n    \n      83\n      BSLT\n      CLAY\n      none\n    \n    \n      84\n      CLAY\n      SDCY\n      sandy clay\n    \n    \n      85\n      SAND\n      SOIL\n      surface soil\n    \n    \n      86\n      GRVL\n      CLAY\n      none\n    \n    \n      87\n      SAND\n      CLAY\n      none\n    \n    \n      88\n      SDSN\n      CLAY\n      none\n    \n    \n      89\n      CLAY\n      SHLE\n      grey\n    \n    \n      90\n      SDCY\n      CLAY\n      clay sandy water supply\n    \n    \n      91\n      GRVL\n      BSLT\n      blue/dark mixed\n    \n    \n      92\n      GRVL\n      SAND\n      sand + gravel + white clay\n    \n    \n      93\n      UNKN\n      SHLE\n      grey very hard\n    \n    \n      94\n      UNKN\n      CLAY\n      white fine, and clay, nodular\n    \n    \n      95\n      CLAY\n      SLSN\n      yellow clayey siltstone\n    \n    \n      96\n      SDSN\n      CLAY\n      none\n    \n    \n      97\n      SDSN\n      SAND\n      brown sand + stones (clean)\n    \n    \n      98\n      SDSN\n      CLAY\n      yellow\n    \n    \n      99\n      BSLT\n      UNKN\n      broken\n    \n    \n      100\n      CLAY\n      SDCY\n      sandy clay stringers\n    \n    \n      101\n      SAND\n      CLAY\n      none\n    \n    \n      102\n      SDSN\n      ROCK\n      bedrock - sandstone; whitish greyish blue, highly weathered, fine grains, angular to subangular, predominantly clear quartz. very small amounts of...\n    \n    \n      103\n      TPSL\n      CLAY\n      none"
  },
  {
    "objectID": "posts/2022-07-03-lithology-classification-hugging-face-3.html#observations",
    "href": "posts/2022-07-03-lithology-classification-hugging-face-3.html#observations",
    "title": "Lithology classification using Hugging Face, part 3",
    "section": "Observations",
    "text": "Observations\nThe error rate is rather low for a first trial, though admittedly we know that many descriptions are fairly unambiguous. If we examine the failed predictions, we can make a few observations:\n\nThere are many none descriptions that are picked up as CLAY, but given that the true labels are not necessarily UNKN for these, one cannot complain too much about the model. The fact that some true labels are set to CLAY for these hints at the use of contextual information, perhaps nearby lithology log entries being classified as CLAY.\nThe model picks up several sandy clay as SDCY, which is a priori more suited than the true labels, at least without other information context explaining why the “true” classification ends up being another category such as CLAY\nTypographical errors such as ssandstone are throwing the model off, which is extected. A production pipeline would need to have an orthographic correction step.\ngrammatically unusual expressions such as clay sandy and clayey/gravel brown are also a challenge for the model.\nMore nuanced descriptions such as light grey medium to coarse sandy gravel - 30%, and gravelly clay - 70%. gravel mainly basalt and jasper where a human reads that the major class is clay, not gravel, or broken rock is more akin to gravel than rock.\n\nStill, the confusion matrix is overall really encouraging. Let’s have a look:\n\nimport seaborn as sns\n\nfrom matplotlib.ticker import FixedFormatter\n\ndef plot_cm(y_true, y_pred, title, figsize=(10,10), labels=None):\n    ''''\n    input y_true-Ground Truth Labels\n          y_pred-Predicted Value of Model\n          title-What Title to give to the confusion matrix\n    \n    Draws a Confusion Matrix for better understanding of how the model is working\n    \n    return None\n    \n    '''\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    ff = FixedFormatter(labels)\n    ax.yaxis.set_major_formatter(ff)\n    ax.xaxis.set_major_formatter(ff)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n\ndef roc_curve_plot(fpr,tpr,roc_auc):\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' %roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n\nplot_cm(y_true, y_pred, title=\"Test set confusion matrix\", figsize=(16,16), labels=labels.names)\n\n/tmp/ipykernel_29992/2038836972.py:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.yaxis.set_major_formatter(ff)\n/tmp/ipykernel_29992/2038836972.py:38: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.xaxis.set_major_formatter(ff)"
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "",
    "text": "For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. It is a stack with a core in C++. For a variety of reasons (licencing, habits, inertia) it is still compiled with the microsoft VCPP2013 toolchain. The most recent versions of some of the third party dependencies are starting to not be compiling with fthat 10 year old compiler, making maintenance more difficult.\nThis is not a particularly popular topic, but I am posting to at least organise and plan a migration."
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#dependencies",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#dependencies",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "Dependencies",
    "text": "Dependencies\nThe following figure gives an overview of the dependencies of the main DLLs. There are already several versions of the MS C runtimes, which is possible so long as libraries interact in a binary compatible way (C API). Usually, C++ level binary compatibility is not achievable. Never, in practice, so far as I recall."
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#boost",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#boost",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "Boost",
    "text": "Boost\nRead Boost 1.79.0 for windows.\n\nCompile from source\nI first tried to compile from source, out of curiosity, but this failed. For the record a summary follows.\nOpening the Visual Studio 2017 Developer Command Prompt v15.0, then 7z x boost_1_79_0.7z. Note that running bootstrap indicated that it was Using 'vc143' toolset so this may be an issue.\nThen doing:\nmkdir c:\\tmp\\boost\nb2.exe  --prefix=c:\\tmp\\boost\nc:/src/tmp/boost_1_79_0/tools/build/src/build\\targets.jam:617: in start-building from module targets\nerror: Recursion in main target references\nerror: the following target are being built currently:\nerror: ./forward -> ./stage -> ./stage-proper -> ***libs/filesystem/build/stage*** -> libs/filesystem/build/stage-dependencies -> libs/log/build/stage -> libs/log/build/stage-dependencies -> ***libs/filesystem/build/stage***\nGo for the prebuilt windows binaries then…\n\n\nprecompiled Boost binaries\nhttps://www.boost.org/users/download/ leads to boost_1_79_0-msvc-14.1-64.exe on sourceforge\nCopying a subset of the libraries:\nset vcpp_ver=14.1\nset boost_ver=1_79_0\nset vcpp_ver_s=141\n\nset src_root_dir=C:\\local\\boost_%boost_ver%\nset src_lib_dir=%src_root_dir%\\lib64-msvc-%vcpp_ver%\nset src_header_dir=%src_root_dir%\\boost\n\nset dest_root_dir=C:\\local\nset dest_dbg_root_dir=C:\\localdev\nset dest_lib_dir=%dest_root_dir%\\libs\\64\nset dest_dbg_lib_dir=%dest_dbg_root_dir%\\libs\\64\nset dest_header_dir=%dest_root_dir%\\include\n:: Cleanup or backup?\n\nmkdir %dest_lib_dir%\n\nset COPYOPTIONS=/Y /R /D\n\nxcopy %src_lib_dir%\\boost_chrono-vc%vcpp_ver_s%* %dest_lib_dir%\\  %COPYOPTIONS%\nxcopy %src_lib_dir%\\boost_date_time-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\boost_filesystem-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\boost_system-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\boost_thread-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\boost_regex-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\n\nxcopy %src_lib_dir%\\libboost_chrono-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\libboost_date_time-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\libboost_filesystem-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\libboost_system-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\libboost_thread-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %src_lib_dir%\\libboost_regex-vc%vcpp_ver_s%* %dest_lib_dir%\\ %COPYOPTIONS%\n\n:: header files\nmkdir %dest_header_dir%\nmv %src_header_dir% %dest_header_dir%\\"
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#moirai",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#moirai",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "Moirai",
    "text": "Moirai\nmoirai is the workorse for reference counting opaque pointers.\nOpen project with ms vstudio 2019 or more does offer an upgrade of the project. There are options to upgrade the windows sdk to a couple of versions.including the 10.0.17763.0 we noted. Note that the Platform toolset option however does not include v141.\nSo, a manual modification of the .vcxproj file is needed (or via project properties from vsstudio after opening the project, it is available this time)\n    <WindowsTargetPlatformVersion>10.0.17763.0</WindowsTargetPlatformVersion>\n    <PlatformToolset>v141</PlatformToolset>\nAnd this appears to compile."
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#netcdf",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#netcdf",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "netCDF",
    "text": "netCDF\nAgain may be interesting to compile from source, but not essential. netcdf windows binaries document. Appears to be compiled with vcpp 2017.\nDownloading netCDF4.9.0-NC4-64.exe\nIt appears to run on top of the v140 platform tools.That’s OK since this is a C API. Try. May see if we can compile from source with v141 target later.\nxcopy C:\\tmp\\netcdf\\bin\\*.dll %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy C:\\tmp\\netcdf\\lib\\*.lib %dest_lib_dir%\\ %COPYOPTIONS%\n\nmkdir %dest_header_dir%\\netcdf\nxcopy C:\\tmp\\netcdf\\include\\*.h %dest_header_dir%\\netcdf\\ %COPYOPTIONS%"
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#jsoncpp",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#jsoncpp",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "jsoncpp",
    "text": "jsoncpp\nI have a fork of jsoncpp and a branch with a customised .vcxproj file. Modifying with:\n    <PlatformToolset>v141</PlatformToolset>\n    <WindowsTargetPlatformVersion>10.0.17763.0</WindowsTargetPlatformVersion>\nAnd this compiles fine. Note that without WindowsTargetPlatformVersion in the project file, this failed to compile.\nNow, how do I automate the building of these? TODO perhaps streamline the powershell script to deal with jsoncpp too. Even if likely very infrequent.\nset jsoncpp_srcdir=C:\\src\\jsoncpp\nset jsoncpp_builddir=%jsoncpp_srcdir%\\makefiles\\custom\\x64\nxcopy %jsoncpp_builddir%\\Release\\jsoncpp.dll %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %jsoncpp_builddir%\\Release\\jsoncpp.lib %dest_lib_dir%\\ %COPYOPTIONS%\n:: Debug also needed, likely. Know of very odd crashes when mixing debug/nondebug in the past. May not be the case anymore. \n\nmkdir %dest_dbg_lib_dir%\nxcopy %jsoncpp_builddir%\\Debug\\jsoncpp.dll %dest_dbg_lib_dir%\\ %COPYOPTIONS%\nxcopy %jsoncpp_builddir%\\Debug\\jsoncpp.lib %dest_dbg_lib_dir%\\ %COPYOPTIONS%\nxcopy %jsoncpp_builddir%\\Debug\\jsoncpp.pdb %dest_dbg_lib_dir%\\ %COPYOPTIONS%\n\nset robocopy_opt=/MIR /MT:1 /R:2 /NJS /NJH /NFL /NDL /XX\nrobocopy %jsoncpp_srcdir%\\include\\json %dest_header_dir%\\json  %robocopy_opt%"
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#yaml-cpp",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#yaml-cpp",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "yaml-cpp",
    "text": "yaml-cpp\nset yamlcpp_srcdir=C:\\src\\yaml-cpp\nset yamlcpp_builddir=%yamlcpp_srcdir%\\vsproj\\x64\nxcopy %yamlcpp_builddir%\\Release\\yaml-cpp.dll %dest_lib_dir%\\ %COPYOPTIONS%\nxcopy %yamlcpp_builddir%\\Release\\yaml-cpp.lib %dest_lib_dir%\\ %COPYOPTIONS%\n:: Debug also needed, likely. Know of very odd crashes when mixing debug/nondebug in the past. May not be the case anymore. \nxcopy %yamlcpp_builddir%\\Debug\\yaml-cpp.dll %dest_dbg_lib_dir%\\ %COPYOPTIONS%\nxcopy %yamlcpp_builddir%\\Debug\\yaml-cpp.lib %dest_dbg_lib_dir%\\ %COPYOPTIONS%\nxcopy %yamlcpp_builddir%\\Debug\\yaml-cpp.pdb %dest_dbg_lib_dir%\\ %COPYOPTIONS%\n\nrobocopy %yamlcpp_srcdir%\\include\\yaml-cpp %dest_header_dir%\\yaml-cpp  %robocopy_opt%"
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#uchronia-datatypes",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#uchronia-datatypes",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "uchronia / datatypes",
    "text": "uchronia / datatypes\nUchronia - time series handling for ensembles simulations and forecasts in C++ is the bedrock for data handling in our stack.\nAt this point I do need to copy the files for the catch unit testing framework, from config-utils, which the unit tests rely on. I notice also a bit of a minor issue, because the unit tests compile against the deployed headers, not the ones from source. This may be a gotcha to watch for, because my vcpp_settings.props file is not set up for development mode compilation. Beware when setting a build pipeline.\nrobocopy C:\\src\\config-utils\\catch\\include\\catch %dest_header_dir%\\catch  %robocopy_opt%\nThe compilation and deploymebnt of this is relatively streamlined by using a high level powershell script."
  },
  {
    "objectID": "posts/2022-06-26-vcpp-compilation-upgrade.html#swift",
    "href": "posts/2022-06-26-vcpp-compilation-upgrade.html#swift",
    "title": "Upgrading a C++ streamflow compilation toolchain",
    "section": "swift",
    "text": "swift\nswift uses wila and its multithreading optimnisers. We need to install a complementary threadpool tool.\nrobocopy C:\\src\\threadpool\\boost %dest_header_dir%\\boost  %robocopy_opt%\nAlso uses in-house numerical header-only files:\nrobocopy C:\\src\\numerical-sl-cpp\\algorithm\\include\\sfsl %dest_header_dir%\\sfsl  %robocopy_opt%\nrobocopy C:\\src\\numerical-sl-cpp\\math\\include\\sfsl %dest_header_dir%\\sfsl  %robocopy_opt%\nError   C2039   'tuple': is not a member of 'boost::math' (compiling source file channel_muskingum_nl.cpp)  libswift    c:\\src\\swift\\libswift\\include\\swift\\channel_muskingum_nl.h  74  \nSeems I just an explicit additional #include <boost/math/tools/tuple.hpp> now with a more recent version of boost.\nrobocopy C:\\local\\include_old\\tclap %dest_header_dir%\\tclap  %robocopy_opt%\n:: <!-- and for fogss later on: -->\nrobocopy C:\\local\\include_old\\eigen3 %dest_header_dir%\\eigen3  %robocopy_opt%\nError   C2079   'outfile' uses undefined class 'std::basic_ofstream<char,std::char_traits<char>>'   swiftcl c:\\src\\swift\\applications\\swiftcl\\run_calibration.cpp   120 \n#include <fstream> is now needed.\nAfter that, unit tests are mostly as expected, a few watchpoint that may be red herring."
  },
  {
    "objectID": "posts/2022-08-09-python-profiling-interop.html",
    "href": "posts/2022-08-09-python-profiling-interop.html",
    "title": "Runtime profiling of python bindings for a C/C++ library",
    "section": "",
    "text": "Profiling python code with cProfile"
  },
  {
    "objectID": "posts/2022-08-09-python-profiling-interop.html#creating-synthethic-hydrologic-model",
    "href": "posts/2022-08-09-python-profiling-interop.html#creating-synthethic-hydrologic-model",
    "title": "Runtime profiling of python bindings for a C/C++ library",
    "section": "Creating synthethic hydrologic model",
    "text": "Creating synthethic hydrologic model\nThe overall purpose of the exercise will be to measure performance under various conditions: model structure, size, time steps, etc. We will not do all that in this post; suffice to say we define a set of functions to create synthetic model setups. We do not show the full definitions. To give a flavour\n\nfrom cinterop.timeseries import mk_hourly_xarray_series\n\ndef create_pulses(nTimeSteps, start) : return mk_hourly_xarray_series( createPulse(nTimeSteps, 5.0, 48), start)\n\ndef create_uniform(nTimeSteps, start) : return mk_hourly_xarray_series( createUniform(nTimeSteps, 1.0), start)\n\ndef set_test_input(ms:'Simulation', subAreaName:'str', rainVarName, petVarName, rain_ts, pet_ts):\n    p_id = mk_full_data_id('subarea', subAreaName, rainVarName)\n    e_id = mk_full_data_id('subarea', subAreaName, petVarName)\n    ms.play_input( rain_ts, p_id)\n    ms.play_input( pet_ts, e_id)\n    \n# def create_line_system(n_time_steps, n_links, rain_varname = \"P\", pet_varname = \"E\", area_km2 = 1):\n# et caetera    \n\nWe are now ready to create our catchment simulation. Before we plunge into cProfiler let’s use a simpler way to assess the runtime from notebooks:"
  },
  {
    "objectID": "posts/2022-08-09-python-profiling-interop.html#using-notebooks-time",
    "href": "posts/2022-08-09-python-profiling-interop.html#using-notebooks-time",
    "title": "Runtime profiling of python bindings for a C/C++ library",
    "section": "Using notebook’s %%time",
    "text": "Using notebook’s %%time\nLet’s create a setup with 15 subareas, hourly time step over 10 years.\n\n%%time \nn_time_step = 10 * 365 * 24\nms = create_line_system(n_time_step, 15)\n\nCPU times: user 888 ms, sys: 8.94 ms, total: 897 ms\nWall time: 897 ms\n\n\nWell, this was only the create of the baseline model, not even execution, and this already takes close to a second. Granted, there are a fair few hours in a decade. Still, a whole second!\nWhat about the simulation runtime? Let’s parameterise minimally to avoid possible artefacts, and execute.\n\nfrom swift2.doc_helper import configure_hourly_gr4j, get_free_params\nconfigure_hourly_gr4j(ms)\np = create_parameteriser('Generic subarea', get_free_params(\"GR4J\"))\np.apply_sys_config(ms)\n\nDouble check we are indeed running hourly over 10 years:\n\nms.get_simulation_span()\n\n{'start': datetime.datetime(1989, 1, 1, 0, 0),\n 'end': datetime.datetime(1998, 12, 29, 23, 0),\n 'time step': 'hourly'}\n\n\n\n%%time\nms.exec_simulation()\n\nCPU times: user 314 ms, sys: 691 µs, total: 315 ms\nWall time: 314 ms\n\n\nThis is actually quite good, and “unexpectedly” less than the model creation itself. This is actually not all that surprising. All of the model execution happens in C++ land. The model setup involves much more operations in python.\nLet’s look at an operation exchanging data from the C++ engine for display in Python. The model simulation has some of its states receiving input time series:\n\nms.get_played_varnames()[:6]\n\n['subarea.1.E',\n 'subarea.1.P',\n 'subarea.10.E',\n 'subarea.10.P',\n 'subarea.11.E',\n 'subarea.11.P']\n\n\nLet’s see what happens in the retrieval of one of these input time series:\n\n%%time\nts = ms.get_played(\"subarea.1.E\")\n\nCPU times: user 441 ms, sys: 106 µs, total: 441 ms\nWall time: 440 ms\n\n\nThis is substantial; more than the native execution over a catchment with 15 subareas. So:\n\nCan we identify the hotspot(s)?\nCan we do something to improve it."
  },
  {
    "objectID": "posts/2022-08-09-python-profiling-interop.html#profiling",
    "href": "posts/2022-08-09-python-profiling-interop.html#profiling",
    "title": "Runtime profiling of python bindings for a C/C++ library",
    "section": "Profiling",
    "text": "Profiling\nEnter cProfile, as we will stick with this in this post. Adapting some of the sample code shown in the Python documentation on profilers\n\nimport cProfile\n\n\nimport pstats, io\npr = cProfile.Profile()\npr.enable()\nts = ms.get_played(\"subarea.1.E\")\npr.disable()\n\n\ns = io.StringIO()\nsortby = pstats.SortKey.CUMULATIVE\nps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n\nWe will print only the top 5 % of the list of function calls, and see if we can spot the likely hotspot\n\nps.print_stats(.05)\nprint(s.getvalue())\n\n         6137 function calls (5964 primitive calls) in 0.445 seconds\n\n   Ordered by: cumulative time\n   List reduced from 588 to 29 due to restriction <0.05>\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        2    0.000    0.000    0.445    0.222 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3361(run_code)\n        2    0.000    0.000    0.445    0.222 {built-in method builtins.exec}\n        1    0.000    0.000    0.445    0.445 /tmp/ipykernel_34182/2688031072.py:4(<cell line: 4>)\n        1    0.000    0.000    0.445    0.445 /home/abcdef/src/swift/bindings/python/swift2/swift2/classes.py:471(get_played)\n        1    0.000    0.000    0.445    0.445 /home/abcdef/src/swift/bindings/python/swift2/swift2/play_record.py:190(get_played)\n        1    0.000    0.000    0.444    0.444 /home/abcdef/src/swift/bindings/python/swift2/swift2/internal.py:94(internal_get_played_tts)\n        1    0.000    0.000    0.444    0.444 /home/abcdef/src/datatypes/bindings/python/uchronia/uchronia/data_set.py:10(get_multiple_time_series_from_provider)\n        1    0.000    0.000    0.444    0.444 /home/abcdef/src/datatypes/bindings/python/uchronia/uchronia/internals.py:69(internal_get_multiple_time_series)\n        1    0.000    0.000    0.441    0.441 /home/abcdef/src/datatypes/bindings/python/uchronia/uchronia/internals.py:76(<listcomp>)\n        1    0.000    0.000    0.441    0.441 /home/abcdef/src/datatypes/bindings/python/uchronia/uchronia/internals.py:74(f)\n        1    0.000    0.000    0.441    0.441 /home/abcdef/src/datatypes/bindings/python/uchronia/uchronia/internals.py:79(internal_get_single_model_time_series)\n        1    0.002    0.002    0.441    0.441 /home/abcdef/src/swift/bindings/python/swift2/swift2/wrap/swift_wrap_custom.py:216(get_played_pkg)\n        1    0.000    0.000    0.438    0.438 /home/abcdef/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:386(geom_to_xarray_time_series)\n        1    0.000    0.000    0.368    0.368 /home/abcdef/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:367(ts_geom_to_even_time_index)\n        1    0.000    0.000    0.368    0.368 /home/abcdef/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py:22(create_even_time_index)\n        1    0.368    0.368    0.368    0.368 /home/abcdef/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py:25(<listcomp>)\n       16    0.000    0.000    0.071    0.004 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/dataarray.py:365(__init__)\n        1    0.000    0.000    0.070    0.070 /home/abcdef/src/rcpp-interop-commons/bindings/python/cinterop/cinterop/cffi/marshal.py:356(create_ensemble_series)\n        9    0.000    0.000    0.070    0.008 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:74(as_variable)\n        1    0.000    0.000    0.070    0.070 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/dataarray.py:90(_infer_coords_and_dims)\n       36    0.000    0.000    0.070    0.002 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:181(as_compatible_data)\n    35/31    0.060    0.002    0.060    0.002 {built-in method numpy.asarray}\n        1    0.000    0.000    0.009    0.009 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/xarray/core/variable.py:172(_possibly_convert_objects)\n        1    0.000    0.000    0.009    0.009 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/series.py:323(__init__)\n        2    0.000    0.000    0.009    0.005 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/construction.py:470(sanitize_array)\n        2    0.000    0.000    0.009    0.005 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/construction.py:695(_try_cast)\n        1    0.000    0.000    0.009    0.009 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1466(maybe_infer_to_datetimelike)\n        2    0.000    0.000    0.006    0.003 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/arrays/datetimes.py:1994(_sequence_to_dt64ns)\n        1    0.000    0.000    0.006    0.006 /home/abcdef/miniconda/envs/hydrofc/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1499(try_datetime)\n\n\n\n\n\nin a file rcpp-interop-commons/bindings/python/cinterop/cinterop/timeseries.py the function create_even_time_index appears to be where a lengthy operation occurs. More precisely, when this function does a list comprehension (listcomp). Note that I infer this because in the cumtime (cumulative time) column there is a drop from 0.396 ms for listcomp to 0.071 for the rest of the operations under this function. I think this is the right way to interpret it in this case, but it may not be the case in other profiling context.\nThe code for create_even_time_index is at this permalink\ndef create_even_time_index(start:ConvertibleToTimestamp, time_step_seconds:int, n:int) -> List:\n    start = as_timestamp(start)\n    delta_t = np.timedelta64(time_step_seconds, 's')\n    return [start + delta_t * i for i in range(n)]\n[start + delta_t * i for i in range(n)] is the bulk of the time, .396 out of a total 0.477 ms.\nThis list is created as the basis for a time index for the creation of the xarray object returned by the overall get_played function. So, is there a faster way to create this time series index?"
  },
  {
    "objectID": "posts/2022-08-09-python-profiling-interop.html#performance-tuning",
    "href": "posts/2022-08-09-python-profiling-interop.html#performance-tuning",
    "title": "Runtime profiling of python bindings for a C/C++ library",
    "section": "Performance tuning",
    "text": "Performance tuning\n\nstart = pd.Timestamp(year=2000, month=1, day=1)\n\n\nn= 24*365*10\n\n\ndef test_index_creation(start, n:int) -> List:\n    start = as_timestamp(start)\n    time_step_seconds = 3600\n    delta_t = np.timedelta64(time_step_seconds, 's')\n    return [start + delta_t * i for i in range(n)]\n\n\n%%time\na = test_index_creation(start, n)\n\nCPU times: user 352 ms, sys: 562 µs, total: 353 ms\nWall time: 351 ms\n\n\nstart is a pandas Timestamp, and we add to it an object of type np.timedelta64 87600 times. I doubt this is the main issue, but let’s operate in numpy types as much as we can by converting the pd.Timestamp once:\n\nstart\n\nTimestamp('2000-01-01 00:00:00')\n\n\n\nstart.to_datetime64()\n\nnumpy.datetime64('2000-01-01T00:00:00.000000000')\n\n\n\ndef test_index_creation(start, n:int) -> List:\n    start = as_timestamp(start).to_datetime64()\n    time_step_seconds = 3600\n    delta_t = np.timedelta64(time_step_seconds, 's')\n    return [start + delta_t * i for i in range(n)]\n\n\n%%time\na = test_index_creation(start, n)\n\nCPU times: user 293 ms, sys: 8.48 ms, total: 301 ms\nWall time: 300 ms\n\n\nThis is actually more of an improvement than I anticipated. OK. What else can we do?\nPandas has the helpful Time series / date functionality page in its documentation. The function from which we started is generic, but for important cases such as hourly and daily time steps, there are options to use the freq argument to the date_range function\n\n%%time\npd.date_range(start, periods=n, freq=\"H\")\n\nCPU times: user 242 µs, sys: 686 µs, total: 928 µs\nWall time: 520 µs\n\n\nDatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 01:00:00',\n               '2000-01-01 02:00:00', '2000-01-01 03:00:00',\n               '2000-01-01 04:00:00', '2000-01-01 05:00:00',\n               '2000-01-01 06:00:00', '2000-01-01 07:00:00',\n               '2000-01-01 08:00:00', '2000-01-01 09:00:00',\n               ...\n               '2009-12-28 14:00:00', '2009-12-28 15:00:00',\n               '2009-12-28 16:00:00', '2009-12-28 17:00:00',\n               '2009-12-28 18:00:00', '2009-12-28 19:00:00',\n               '2009-12-28 20:00:00', '2009-12-28 21:00:00',\n               '2009-12-28 22:00:00', '2009-12-28 23:00:00'],\n              dtype='datetime64[ns]', length=87600, freq='H')\n\n\nIt is two order of magnitude faster… Definitely worth a re-engineering of the features in the timeseries.py we started from.\nThis probably does not solve the issue for many other cases (irregular time steps, e.g. monthly), but there are many cases where we could benefit. The date_range documentation specifies that an arbitrary DateOffset to its freq argument (freq: str or DateOffset, default ‘D’). How efficient is this operation on our 87600 data points?\n\nd_offset = pd.tseries.offsets.DateOffset(minutes=15)\n\n\nstart + d_offset\n\nTimestamp('2000-01-01 00:15:00')\n\n\n\n%%time\npd.date_range(start, periods=n, freq=d_offset)\n\nCPU times: user 1.5 s, sys: 1.32 ms, total: 1.5 s\nWall time: 1.5 s\n\n\nDatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 00:15:00',\n               '2000-01-01 00:30:00', '2000-01-01 00:45:00',\n               '2000-01-01 01:00:00', '2000-01-01 01:15:00',\n               '2000-01-01 01:30:00', '2000-01-01 01:45:00',\n               '2000-01-01 02:00:00', '2000-01-01 02:15:00',\n               ...\n               '2002-07-01 09:30:00', '2002-07-01 09:45:00',\n               '2002-07-01 10:00:00', '2002-07-01 10:15:00',\n               '2002-07-01 10:30:00', '2002-07-01 10:45:00',\n               '2002-07-01 11:00:00', '2002-07-01 11:15:00',\n               '2002-07-01 11:30:00', '2002-07-01 11:45:00'],\n              dtype='datetime64[ns]', length=87600, freq='<DateOffset: minutes=15>')\n\n\nIt looks like in this case this is actually a fair bit slower than my original implementation. Interesting. And using start.to_datetime64() makes no difference too."
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html",
    "href": "posts/20221008-nbdev-windows-2/index.html",
    "title": "Using nbdev on Windows - part 2",
    "section": "",
    "text": "This post is a follow up on Using nbdev on Windows, which explains the motivation for assessing how nbdev works on Windows, natively (i.e. not from a unix emulation or WSL).\nThe author of these lines is a near exclusive Linux user, so I am not posting this to advocate for Windows. This is because I have many colleagues who may consider using nbdev but for whom, based on past experience, adoption is much more likely if it is natively working on Windows."
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html#prerequisites",
    "href": "posts/20221008-nbdev-windows-2/index.html#prerequisites",
    "title": "Using nbdev on Windows - part 2",
    "section": "Prerequisites",
    "text": "Prerequisites\nSee the nbdev install instructions for reference, but “ignore” the advice it does not work on windows “cmd”. See the nbdev walkthrough installation.\nFor this post:\n\nif you use pip and venv:\n\nA virtual environment set up with pip and venv, with nbdev\n\nif you use conda:\n\nMiniconda3 (or anaconda3) for Windows installed\nA conda environment set up, but with nbdev and 3 other packages from the fastai stable set up with pip, as described in Using nbdev on Windows part 1. Jupyter notebooks (or lab).\n\ngit for windows installed: where git succeeds in locating git from a CMD prompt\nInstall Quarto, preferably from the Quarto web site unless you know you have a recent enough version already.\n\n\n\n\n\n\n\nNote\n\n\n\nMost commands in this post, notably nbdev_* commands, are run from a miniconda terminal prompt (CMD) on Windows, unless otherwise stated. I am deliberately not adding a path to a MinGW (Minimalist GNU for Windows) folder. It has unix commands I dearly miss, but the purpose of this post is to detect features not working for a typical Windows user.\n\n\n\n\n\nMiniconda3 prompt on Windows\n\n\nUsing Windows powershell is not tested in the present post.\n\n\n\n\n\n\n\n\nTip\n\n\n\nI am executing the git clone, git push and similar commands from a git bash prompt rather than the Miniconda3 CMD prompt used for all other commands. This is because I know how to relatively easily set up SSH authentication with eval `ssh-agent -s`, ssh-add and then all git commands interacting with the github remote just work. There are ways to set that up natively on Windows, I believe.\n\n\n\n\n\nGit bash prompt on Windows"
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html#starting-the-walkthrough",
    "href": "posts/20221008-nbdev-windows-2/index.html#starting-the-walkthrough",
    "title": "Using nbdev on Windows - part 2",
    "section": "Starting the walkthrough",
    "text": "Starting the walkthrough\nWe create the skeleton with the following command. Let’s use a custom package name from your activated conda (resp virtual) environment.\nAfter cloning the empty repository from github:\nnbdev_new --lib_name winbdev\nwhich outputs the following. There are a few odd characters, but this is probably of no consequence.\nUserWarning: Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated\n  else: warn('Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated')\n←[94mrepo←[39m = nbdev-tutorial-windows # Automatically inferred from git\n←[94mbranch←[39m = main # Automatically inferred from git\n←[94muser←[39m = jmp75 # Automatically inferred from git\n←[94mauthor←[39m = J-M # Automatically inferred from git\n←[94mauthor_email←[39m = abcdef@csiro.au # Automatically inferred from git\n←[94mdescription←[39m = Test nbdev when used natively on Windows.  # Automatically inferred from git\nsettings.ini created.\npandoc -o README.md\n  to: gfm+footnotes+tex_math_dollars-yaml_metadata_block\n  output-file: index.html\n  standalone: true\n  default-image-extension: png\n\nmetadata\n  description: Test nbdev when used natively on Windows.\n  title: nbdev-tutorial-windows\n\nOutput created: _docs\\README.md\nCheck the created files and directories, and it seems to be as expected. I am a bit surprised the package folder name is not winbdev as per the custom package name, but this later showed to all work as expected.\n\n\n\nFile/Dir\n\n\n\n\n.github/\n\n\n.gitignore\n\n\nLICENSE\n\n\nMANIFEST.in\n\n\nnbdev_tutorial_windows/\n\n\nnbs/\n\n\nREADME.md\n\n\nsettings.ini\n\n\nsetup.py\n\n\n_proc/\n\n\n\nTime for the first commit:\nwhere git\ngit add .\nA few warnings courtesy of the “carriage return” character, but again not something of real consequence.\nwarning: LF will be replaced by CRLF in .github/workflows/deploy.yaml.\nThe file will have its original line endings in your working directory\ngit commit -m \"initial commit\""
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html#git-push",
    "href": "posts/20221008-nbdev-windows-2/index.html#git-push",
    "title": "Using nbdev on Windows - part 2",
    "section": "git push",
    "text": "git push\n\n\n\n\n\n\nTip\n\n\n\nAs mentioned previously, git push and similar commands performed from a git bash prompt\n\n\ngit push\nThe first action run on Github Actions failed with:\nRun import ghapi.core,nbdev.config,sys\nError:  Please enable GitHub Pages to publish from the root of the `gh-pages` branch per these instructions - https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-from-a-branch\nHTTP Error 403: Forbidden\nWhile the error message provides a link, follow instead the nbdev instructions for enabling github pages, which say to use gh-page to deploy from. It may be a minor flow in the nbdev walkthrough not to have this section earlier.\nThe action then succeeds, but upon accessing, I note a\n404\nFile not found\nThis may be transient. Anyway, probably nothing specific to working from Windows.\nNext, nbdev_install_hooks seems to complete fine reporting that Hooks are installed."
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html#install-your-package",
    "href": "posts/20221008-nbdev-windows-2/index.html#install-your-package",
    "title": "Using nbdev on Windows - part 2",
    "section": "Install your package",
    "text": "Install your package\nIf you run the command pip install -e '.[dev]' as per the nbdev documentation you end up with:\nERROR: '.[dev]' is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\nYou’ll want to use double quotes, which works fine: pip install -e \".[dev]\""
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html#preview",
    "href": "posts/20221008-nbdev-windows-2/index.html#preview",
    "title": "Using nbdev on Windows - part 2",
    "section": "Preview",
    "text": "Preview\n\n\n\n\n\n\nTip\n\n\n\nCheck you have the right location of quarto in the PATH environment variable: where quarto should return a location such as:\nC:\\Users\\abcdef\\AppData\\Local\\Programs\\Quarto\\bin\\quarto.cmd\n\n\nnbdev_preview seems to work fine for now:\n\n\n\n\n\nInitial documentation preview with nbdev_preview"
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html#nbdev_prepare",
    "href": "posts/20221008-nbdev-windows-2/index.html#nbdev_prepare",
    "title": "Using nbdev on Windows - part 2",
    "section": "nbdev_prepare",
    "text": "nbdev_prepare\nnbdev_prepare returns: success!\nminor modification to the git commit message (backticks do not play nice in CMD): git commit -m \"Add say hello and some tests\". Git actions complete and the web site is up on github pages as expected."
  },
  {
    "objectID": "posts/20221008-nbdev-windows-2/index.html#nbdev_preview-with-added-code",
    "href": "posts/20221008-nbdev-windows-2/index.html#nbdev_preview-with-added-code",
    "title": "Using nbdev on Windows - part 2",
    "section": "nbdev_preview with added code",
    "text": "nbdev_preview with added code\nAfter going a bit further into the tutorial (adding the first class, or perhaps a bit before that), I notice in a terminal with a running nbdev_preview:\n  GET: /core.html\n  Traceback (most recent call last):\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\site-packages\\nbdev\\quarto.py\", line 278, in _f\n      try: serve_drv.main(res)\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\site-packages\\nbdev\\serve_drv.py\", line 22, in main\n      if src.suffix=='.ipynb': exec_nb(src, dst, x)\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\site-packages\\nbdev\\serve_drv.py\", line 15, in exec_nb\n      nb = read_nb(src)\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\site-packages\\execnb\\nbio.py\", line 57, in read_nb\n      res = dict2nb(_read_json(path, encoding='utf-8'))\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\site-packages\\execnb\\nbio.py\", line 16, in _read_json\n      return loads(Path(self).read_text(encoding=encoding, errors=errors))\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\json\\__init__.py\", line 346, in loads\n      return _default_decoder.decode(s)\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\json\\decoder.py\", line 337, in decode\n      obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\json\\decoder.py\", line 355, in raw_decode\n      raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n  json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\nThis seems not to be a full blocker, as the documentation for the new class seems to show up after a couple of refreshes."
  },
  {
    "objectID": "posts/2022-06-13-lithology-classification-hugging-face-2.html",
    "href": "posts/2022-06-13-lithology-classification-hugging-face-2.html",
    "title": "Lithology classification using Hugging Face, part 2",
    "section": "",
    "text": "This is a continuation of Lithology classification using Hugging Face, part 1.\nWe saw in the previous post that the Namoi lithology logs data had their primary (major) lithology mostly completed. A substantial proportion had the label None nevertheless, despite descriptions that looked like they would obviously lead to a categorisation. There were many labels, with a long-tailed frequency histogram.\nThe aim of this post is (was) to get a classification training happening.\nSpoiler alert: it won’t. Almost.\nRather than write a post after the fact pretending it was a totally smooth journey, the following walktrough deliberately keeps and highlights issues, albeit succinctly. Don’t jump to the conclusion that we will not get there eventually, or that Hugging Face is not good. When you adapt prior work to your own use case, you will likely stumble, so this post will make you feel in good company."
  },
  {
    "objectID": "posts/2022-06-13-lithology-classification-hugging-face-2.html#imbalanced-data-sets",
    "href": "posts/2022-06-13-lithology-classification-hugging-face-2.html#imbalanced-data-sets",
    "title": "Lithology classification using Hugging Face, part 2",
    "section": "Imbalanced data sets",
    "text": "Imbalanced data sets\nFrom the histogram above, it is pretty clear that labels are also not uniform an we have a class imbalance. Remember to skim Lithology classification using Hugging Face, part 1 for the initial data exploration if you have not done so already.\nFor the sake of the exercise in this post, I will reduce arbitrarily the number of labels used in this post, by just “forgetting” the less represented classes.\nThere are many resources about class imbalances. One of them is 8 Tactics to combat imbalanced classes in your machine learning dataset\nLet’s see what labels we may want to keep for this post:\n\ndef sample_desc_for_code(major_code, n=50, seed=None):\n    is_code = litho_logs[MAJOR_CODE] == major_code\n    coded = litho_logs.loc[is_code][DESC]\n    if seed is not None:\n        np.random.seed(seed)\n    return coded.sample(n=50)\n\n\nsample_desc_for_code(\"UNKN\", seed=123)\n\n134145     (UNKNOWN), NO SAMPLE COLLECTED DUE TO WATER LOSS\n134715    (UNKNOWN); COULD NOT BE LOGGED BECAUSE NO CUTT...\n122303                                          GREY SHALEY\n133856                                              NOMINAL\n134378                                                 None\n133542                                              DRILLER\n122258                                        WATER BEARING\n127916                                         WATER SUPPLY\n133676                                              DRILLER\n134399                                              DRILLER\n134052                                              DRILLER\n128031                         VERY SANDY STONES SOME LARGE\n134140                                       SAMPLE MISSING\n122282                              REDDISH YELLOW VOLCANIC\n133623                                    WHITE CRYSTALLINE\n134505                                              MISSING\n133694                                              DRILLER\n133585                                              DRILLER\n134201                                              MISSING\n134627                                              NO DATA\n133816                                              DRILLER\n133893                                              DRILLER\n134232                                              DRILLER\n133687                                              DRILLER\n133871                                              DRILLER\n133698                                              DRILLER\n134752                                              MISSING\n128077                           WATER BEARING WATER SUPPLY\n122253                                         WATER SUPPLY\n133607                                              DRILLER\n133617                                              DRILLER\n133643                                                 HARD\n134526                                  (UNKNOWN) CORE LOSS\n133709                                        SANDY STREAKS\n123254                                 NOMINAL WATER SUPPLY\n122219                                         WATER SUPPLY\n133525                                              DRILLER\n127799                                         WATER SUPPLY\n133940                                              DRILLER\n124775                              (UNKNOWN) WATER BEARING\n126814                             (UNKNOWN); WATER BEARING\n133965                                              DRILLER\n134074                                              DRILLER\n134395                                              DRILLER\n133970                                              DRILLER\n134262                                              DRILLER\n122407                                         WATER SUPPLY\n144370                                            S/S LT BR\n125023                             (UNKNOWN); WATER BEARING\n133675                                              DRILLER\nName: Description, dtype: object\n\n\nThe “unknown” category is rather interesting in fact, and worth keeping as a valid class."
  },
  {
    "objectID": "posts/2022-06-13-lithology-classification-hugging-face-2.html#subsetting",
    "href": "posts/2022-06-13-lithology-classification-hugging-face-2.html#subsetting",
    "title": "Lithology classification using Hugging Face, part 2",
    "section": "Subsetting",
    "text": "Subsetting\nLet’s keep “only” the main labels, for the sake of this exercise. We will remove None however, despite its potential interest. We will (hopefully) revisit this in another post.\n\nlabels_kept = df_most_common[\"token\"][:17].values  # 17 first classes somewhat arbitraty\nlabels_kept = labels_kept[labels_kept != \"None\"]\nlabels_kept\n\narray(['CLAY', 'GRVL', 'SAND', 'SHLE', 'SDSN', 'BSLT', 'TPSL', 'SOIL',\n       'ROCK', 'GRNT', 'SDCY', 'SLSN', 'CGLM', 'MDSN', 'UNKN', 'COAL'],\n      dtype=object)\n\n\n\nkept = [x in labels_kept for x in litho_classes]\nlitho_logs_kept = litho_logs[kept].copy()  # avoid warning messages down the track.\nlitho_logs_kept.sample(10)\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      BoreID\n      HydroCode\n      RefElev\n      RefElevDesc\n      FromDepth\n      ToDepth\n      TopElev\n      BottomElev\n      MajorLithCode\n      MinorLithCode\n      Description\n      Source\n      LogType\n      OgcFidTemp\n    \n  \n  \n    \n      70655\n      526412\n      10072593\n      GW031851.1.1\n      None\n      UNK\n      53.94\n      59.13\n      None\n      None\n      CLAY\n      NaN\n      CLAY SANDY\n      UNK\n      1\n      9308381\n    \n    \n      7173\n      64072\n      10043001\n      GW001815.1.1\n      None\n      UNK\n      31.39\n      44.5\n      None\n      None\n      SHLE\n      NaN\n      SHALE\n      UNK\n      1\n      8732384\n    \n    \n      30076\n      197788\n      10152523\n      GW099036.1.1\n      None\n      UNK\n      181.0\n      228.0\n      None\n      None\n      SHLE\n      NaN\n      SHALE: GREY, FINE\n      UNK\n      1\n      8870150\n    \n    \n      93967\n      701859\n      10105392\n      GW031140.1.1\n      None\n      UNK\n      0.0\n      8.84\n      None\n      None\n      SOIL\n      NaN\n      SOIL CLAY\n      UNK\n      1\n      9327759\n    \n    \n      115538\n      803595\n      10099300\n      GW970770.1.1\n      None\n      UNK\n      36.6\n      38.1\n      None\n      None\n      SAND\n      NaN\n      SAND; FINE TO COARSE, BROWN\n      UNK\n      1\n      9435886\n    \n    \n      107173\n      762000\n      10122945\n      GW018629.1.1\n      None\n      UNK\n      72.54\n      74.37\n      None\n      None\n      SDSN\n      NaN\n      SANDSTONE YELLOW HARD\n      UNK\n      1\n      9389679\n    \n    \n      106769\n      760370\n      10111007\n      GW026576.1.1\n      None\n      UNK\n      65.23\n      71.32\n      None\n      None\n      SDSN\n      NaN\n      SANDSTONE WATER SUPPLY\n      UNK\n      1\n      9388007\n    \n    \n      13553\n      114744\n      10116235\n      GW022175.1.1\n      None\n      UNK\n      37.8\n      39.01\n      None\n      None\n      GRVL\n      NaN\n      GRAVEL FINE-COARSE\n      UNK\n      1\n      8784472\n    \n    \n      142398\n      971715\n      10074454\n      GW901230.1.1\n      None\n      UNK\n      20.0\n      24.0\n      None\n      None\n      GRVL\n      NaN\n      GRAVEL\n      UNK\n      1\n      9567221\n    \n    \n      9664\n      85061\n      10043586\n      GW011521.1.1\n      None\n      UNK\n      12.19\n      20.73\n      None\n      None\n      CLAY\n      NaN\n      CLAY YELLOW GRAVEL\n      UNK\n      1\n      8753973\n    \n  \n\n\n\n\n\nlabels = ClassLabel(names=labels_kept)\nint_labels = np.array([\n    labels.str2int(x) for x in litho_logs_kept[MAJOR_CODE].values\n])\nint_labels = int_labels.astype(np.int8) # to mimick chapter3 HF so far as I can see\n\n\nlitho_logs_kept[MAJOR_CODE_INT] = int_labels"
  },
  {
    "objectID": "posts/2022-06-13-lithology-classification-hugging-face-2.html#class-imbalance",
    "href": "posts/2022-06-13-lithology-classification-hugging-face-2.html#class-imbalance",
    "title": "Lithology classification using Hugging Face, part 2",
    "section": "Class imbalance",
    "text": "Class imbalance\nEven our subset of 16 classes is rather imbalanced; the number of “clay” labels is looking more than 30 times that of “coal” just by eyeballing.\nThe post by Jason Brownlee 8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset, outlines several approaches. One of them is to resample from labels, perhaps with replacement, to equalise classes. It is a relatively easy approach to implement, but there are issues, growing with the level of imbalance. Notably, if too many rows from underrepresented classes are repeated, there is an increased tendency to overfitting at training.\nThe video Simple Training with the 🤗 Transformers Trainer (at 669 seconds) also explains the issues with imbalances and crude resampling. It offers instead a solution with class weighting that is more robust. That approach is evoked in Jason’s post, but the video has a “Hugging Face style” implementation ready to repurpose.\n\nResample with replacement\nJust for information, what we’d do with a relatively crude resampling may be:\n\ndef sample_major_lithocode(dframe, code, n=10000, seed=None):\n    x = dframe[dframe[MAJOR_CODE] == code]\n    replace = n > len(x)\n    return x.sample(n=n, replace=replace, random_state=seed)\n\n\nsample_major_lithocode(litho_logs_kept, \"CLAY\", n=10, seed=0)\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      BoreID\n      HydroCode\n      RefElev\n      RefElevDesc\n      FromDepth\n      ToDepth\n      TopElev\n      BottomElev\n      MajorLithCode\n      MinorLithCode\n      Description\n      Source\n      LogType\n      OgcFidTemp\n      MajorLithoCodeInt\n    \n  \n  \n    \n      106742\n      760246\n      10144429\n      GW030307.1.1\n      279.5\n      NGS\n      54.3\n      72.2\n      225.2\n      207.3\n      CLAY\n      NaN\n      CLAY LIGHT BROWN GRAVEL\n      UNK\n      1\n      9387877\n      0\n    \n    \n      138850\n      950521\n      10147004\n      GW036015.2.2\n      236.0\n      NGS\n      73.15\n      74.676\n      162.85\n      161.324\n      CLAY\n      NaN\n      CLAY; AS ABOVE, MORE MICACEOUS & FINE GRAVEL (...\n      ?? - WC&IC\n      2\n      9543085\n      0\n    \n    \n      30006\n      197243\n      10049338\n      GW062392.1.1\n      None\n      UNK\n      63.0\n      64.0\n      None\n      None\n      CLAY\n      NaN\n      CLAY SANDY\n      UNK\n      1\n      8869540\n      0\n    \n    \n      3225\n      29304\n      10142901\n      GW014623.1.1\n      None\n      UNK\n      22.86\n      23.47\n      None\n      None\n      CLAY\n      NaN\n      CLAY SANDY\n      UNK\n      1\n      8696556\n      0\n    \n    \n      9795\n      86262\n      10121680\n      GW009977.1.1\n      None\n      UNK\n      39.01\n      42.67\n      None\n      None\n      CLAY\n      NaN\n      CLAY YELLOW PUGGY\n      UNK\n      1\n      8755205\n      0\n    \n    \n      49588\n      427460\n      10067562\n      GW964964.1.1\n      None\n      UNK\n      11.0\n      14.0\n      None\n      None\n      CLAY\n      NaN\n      CLAY\n      UNK\n      1\n      9199868\n      0\n    \n    \n      136116\n      943202\n      10055892\n      GW971627.1.1\n      None\n      UNK\n      14.0\n      20.0\n      None\n      None\n      CLAY\n      NaN\n      GREY WET CLAY\n      UNK\n      1\n      9534634\n      0\n    \n    \n      5723\n      50788\n      10049974\n      GW010017.1.1\n      None\n      UNK\n      14.02\n      24.38\n      None\n      None\n      CLAY\n      NaN\n      CLAY RED SANDY\n      UNK\n      1\n      8718677\n      0\n    \n    \n      94938\n      706287\n      10018922\n      GW022845.1.1\n      None\n      UNK\n      1.22\n      11.58\n      None\n      None\n      CLAY\n      NaN\n      CLAY\n      UNK\n      1\n      9332267\n      0\n    \n    \n      38277\n      287347\n      10132392\n      GW042735.1.1\n      None\n      UNK\n      0.75\n      6.0\n      None\n      None\n      CLAY\n      NaN\n      CLAY\n      UNK\n      1\n      8942094\n      0\n    \n  \n\n\n\n\n\nbalanced_litho_logs = [\n    sample_major_lithocode(litho_logs_kept, code, n=10000, seed=0)\n    for code in labels_kept\n]\nbalanced_litho_logs = pd.concat(balanced_litho_logs)\nbalanced_litho_logs.head()\n\n\n\n\n\n  \n    \n      \n      OBJECTID\n      BoreID\n      HydroCode\n      RefElev\n      RefElevDesc\n      FromDepth\n      ToDepth\n      TopElev\n      BottomElev\n      MajorLithCode\n      MinorLithCode\n      Description\n      Source\n      LogType\n      OgcFidTemp\n      MajorLithoCodeInt\n    \n  \n  \n    \n      106742\n      760246\n      10144429\n      GW030307.1.1\n      279.5\n      NGS\n      54.3\n      72.2\n      225.2\n      207.3\n      CLAY\n      NaN\n      CLAY LIGHT BROWN GRAVEL\n      UNK\n      1\n      9387877\n      0\n    \n    \n      138850\n      950521\n      10147004\n      GW036015.2.2\n      236.0\n      NGS\n      73.15\n      74.676\n      162.85\n      161.324\n      CLAY\n      NaN\n      CLAY; AS ABOVE, MORE MICACEOUS & FINE GRAVEL (...\n      ?? - WC&IC\n      2\n      9543085\n      0\n    \n    \n      30006\n      197243\n      10049338\n      GW062392.1.1\n      None\n      UNK\n      63.0\n      64.0\n      None\n      None\n      CLAY\n      NaN\n      CLAY SANDY\n      UNK\n      1\n      8869540\n      0\n    \n    \n      3225\n      29304\n      10142901\n      GW014623.1.1\n      None\n      UNK\n      22.86\n      23.47\n      None\n      None\n      CLAY\n      NaN\n      CLAY SANDY\n      UNK\n      1\n      8696556\n      0\n    \n    \n      9795\n      86262\n      10121680\n      GW009977.1.1\n      None\n      UNK\n      39.01\n      42.67\n      None\n      None\n      CLAY\n      NaN\n      CLAY YELLOW PUGGY\n      UNK\n      1\n      8755205\n      0\n    \n  \n\n\n\n\n\nplot_freq(token_freq(balanced_litho_logs[MAJOR_CODE].values, 50))\n\n<AxesSubplot:xlabel='token'>\n\n\n\n\n\n\n\nDealing with imbalanced classes with weights\nInstead of the resampling above, we adapt the approach creating weights for the Trainer we will run.\n\nsorted_counts = litho_logs_kept[MAJOR_CODE].value_counts()\nsorted_counts\n\nCLAY    43526\nGRVL    15824\nSAND    15317\nSHLE    10158\nSDSN     9199\nBSLT     7894\nTPSL     5300\nSOIL     4347\nROCK     2549\nGRNT     1852\nSDCY     1643\nSLSN     1443\nCGLM     1233\nMDSN     1207\nUNKN     1125\nCOAL     1040\nName: MajorLithCode, dtype: int64\n\n\n\nsorted_counts / sorted_counts.sum()\n\nCLAY    0.351990\nGRVL    0.127967\nSAND    0.123867\nSHLE    0.082147\nSDSN    0.074391\nBSLT    0.063838\nTPSL    0.042860\nSOIL    0.035154\nROCK    0.020613\nGRNT    0.014977\nSDCY    0.013287\nSLSN    0.011669\nCGLM    0.009971\nMDSN    0.009761\nUNKN    0.009098\nCOAL    0.008410\nName: MajorLithCode, dtype: float64\n\n\n\nclass_weights = (1 - sorted_counts / sorted_counts.sum()).values\nclass_weights\n\narray([0.64801022, 0.87203312, 0.87613317, 0.91785342, 0.92560874,\n       0.93616213, 0.95713951, 0.96484631, 0.97938653, 0.98502309,\n       0.98671325, 0.98833062, 0.99002887, 0.99023913, 0.99090225,\n       0.99158964])\n\n\nWe check that cuda is available (of course optional)\n\nassert torch.cuda.is_available()\n\nOn Linux if you have a DELL laptop with an NVIDIA card, but nvidia-smi returns: NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running, you may need to change your kernel specification file $HOME/.local/share/jupyter/kernels/hf/kernel.json. This behavior seems to depend on the version of Linux kernel you have. It certainly changed out of the blue for me from yesterday, despite no change that I can tell.\noptirun nvidia-smi returning a proper graphic card report should be a telltale sign you have to update your kernel.json like so:\n{\n \"argv\": [\n  \"optirun\",\n  \"/home/your_ident/miniconda/envs/hf/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Hugging Face\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\nYou may need to restart jupyter-lab, or visual studio code, etc., for change to take effect. Restarting the kernel may not be enough, conter-intuitively.\nBackground details about optirun architecture at [Bumblebee Debian]https://wiki.debian.org/Bumblebee\n\nclass_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\nclass_weights\n\ntensor([0.6480, 0.8720, 0.8761, 0.9179, 0.9256, 0.9362, 0.9571, 0.9648, 0.9794,\n        0.9850, 0.9867, 0.9883, 0.9900, 0.9902, 0.9909, 0.9916],\n       device='cuda:0')\n\n\n\nmodel_nm = \"microsoft/deberta-v3-small\""
  },
  {
    "objectID": "posts/2022-06-13-lithology-classification-hugging-face-2.html#tokenisation",
    "href": "posts/2022-06-13-lithology-classification-hugging-face-2.html#tokenisation",
    "title": "Lithology classification using Hugging Face, part 2",
    "section": "Tokenisation",
    "text": "Tokenisation\n\nBump on the road; download operations taking too long\nAt this point I spent more hours than I wish I had on an issue, perhaps very unusual.\nThe operation tokz = AutoTokenizer.from_pretrained(model_nm) was taking an awful long time to complete:\nCPU times: user 504 ms, sys: 57.9 ms, total: 562 ms\nWall time: 14min 13s\nTo cut a long story short, I managed to figure out what was going on. It is documented on the Hugging Face forum at: Some HF operations take an excessively long time to complete. If you have issues where HF operations take a long time, read it.\nNow back to the tokenisation story. Note that the local caching may be superflous if you do not encounter the issue just mentioned.\n\nmax_length = 128\n\n\np = Path(\"./tokz_pretrained\")\npretrained_model_name_or_path = p if p.exists() else model_nm\n# https://discuss.huggingface.co/t/sentence-transformers-paraphrase-minilm-fine-tuning-error/9612/4\ntokz = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=True, max_length=max_length, model_max_length=max_length)\nif not p.exists():\n    tokz.save_pretrained(\"./tokz_pretrained\")\n\nLet’s see what this does on a typical lithology description\n\ntokz.tokenize(\"CLAY, VERY SANDY\")\n\n['▁C', 'LAY', ',', '▁VERY', '▁S', 'ANDY']\n\n\nWell, the vocabulary is probably case sensitive and all the descriptions being uppercase in the source data are likely problematic. Let’s check what happens on lowercase descriptions:\n\ntokz.tokenize(\"clay, very sandy\")\n\n['▁clay', ',', '▁very', '▁sandy']\n\n\nThis looks better. So let’s change the descriptions to lowercase; we are not loosing any relevent information in this case, I think.\n\n# note: no warnings because we used .copy() earlier to create litho_logs_kept\nlitho_logs_kept[DESC] = litho_logs_kept[DESC].str.lower()\n\n\nlitho_logs_kept_mini = litho_logs_kept[[MAJOR_CODE_INT, DESC]]\nlitho_logs_kept_mini.sample(n=10)\n\n\n\n\n\n  \n    \n      \n      MajorLithoCodeInt\n      Description\n    \n  \n  \n    \n      8256\n      5\n      basalt\n    \n    \n      96820\n      4\n      sandstone\n    \n    \n      36776\n      2\n      sand\n    \n    \n      110231\n      0\n      clay; light brown, very silty\n    \n    \n      80270\n      1\n      gravel & large stones\n    \n    \n      17592\n      1\n      gravel water supply\n    \n    \n      74437\n      0\n      clay\n    \n    \n      22904\n      5\n      basalt stones\n    \n    \n      71578\n      1\n      gravel very clayey water supply\n    \n    \n      73030\n      3\n      shale"
  },
  {
    "objectID": "posts/2022-06-13-lithology-classification-hugging-face-2.html#create-dataset-and-tokenisation",
    "href": "posts/2022-06-13-lithology-classification-hugging-face-2.html#create-dataset-and-tokenisation",
    "title": "Lithology classification using Hugging Face, part 2",
    "section": "Create dataset and tokenisation",
    "text": "Create dataset and tokenisation\nWe want to create a dataset such that tokenised data is of uniform shape (better for running on GPU) Applying the technique in this segment of the HF course video. Cheating a bit on guessing the length (I know from offline checks that max is 90 tokens)\n\nds = Dataset.from_pandas(litho_logs_kept_mini)\n\ndef tok_func(x):\n    return tokz(\n        x[DESC],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\",\n    )\n\nThe Youtube video above suggests to use tok_ds = ds.map(tok_func, batched=True) for a faster execution; however I ended up with the foollowing error:\nTypeError: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`.\nThe following non-batched option works in a reasonable time:\n\ntok_ds = ds.map(tok_func)\n\nParameter 'function'=<function tok_func at 0x7f0d047695e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 123657/123657 [00:24<00:00, 4962.06ex/s]\n\n\n\ntok_ds_tmp = tok_ds[:5]\ntok_ds_tmp.keys()\n\ndict_keys(['MajorLithoCodeInt', 'Description', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\n# check the length of vectors is indeed 128:\nlen(tok_ds_tmp[\"input_ids\"][0][0])\n\n128\n\n\n\nnum_labels = len(labels_kept)\n\n\n# NOTE: the local caching may be superflous\np = Path(\"./model_pretrained\")\n\nmodel_name = p if p.exists() else model_nm\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, max_length=max_length)\n                                                           # label2id=label2id, id2label=id2label).to(device) \nif not p.exists():\n    model.save_pretrained(p)\n\n\nprint(type(model))\n\n<class 'transformers.models.deberta_v2.modeling_deberta_v2.DebertaV2ForSequenceClassification'>\n\n\n\n# Different approach, but one that I am not sure how to progress to a hugging face Dataset. Borrowed from [this video](https://youtu.be/1pedAIvTWXk?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o&t=143)\n# litho_desc_list = [x for x in litho_logs_kept_mini[DESC].values]\n# input_descriptions = tokz(litho_desc_list, padding=True, truncation=True, max_length=256, return_tensors='pt')\n# input_descriptions['input_ids'].shape\n# model(input_descriptions['input_ids'][:5,:], attention_mask=input_descriptions['attention_mask'][:5,:]).logits\n\n\ntok_ds\n\nDataset({\n    features: ['MajorLithoCodeInt', 'Description', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 123657\n})\n\n\nTransformers always assumes that your labels has the column name “labels”. Odd, but at least this fosters a consistent system, so why not:\n\ntok_ds = tok_ds.rename_columns({MAJOR_CODE_INT: \"labels\"})\n\n\ntok_ds = tok_ds.remove_columns(['Description', '__index_level_0__'])\n\n\n# We want to make sure we work on the GPU, so at least make sure we have torch tensors.\n# Note that HF is supposed to take care of movind data to the GPU if available, so you should not ahve to manually copy the data to the GPU device\ntok_ds.set_format(\"torch\")\n\n\n# args = TrainingArguments(output_dir='./litho_outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n#     evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n#     num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n\n\ndds = tok_ds.train_test_split(0.25, seed=42)\n\n\ndds.keys()\n\ndict_keys(['train', 'test'])\n\n\n\ntok_ds.features['labels'] = labels\n\n\ntok_ds.features\n\n# TODO:\n#     This differs from chapter3 of HF course https://huggingface.co/course/chapter3/4?fw=pt    \n# {'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n#  'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n#  'labels': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], id=None),\n#  'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n\n{'labels': ClassLabel(num_classes=16, names=array(['CLAY', 'GRVL', 'SAND', 'SHLE', 'SDSN', 'BSLT', 'TPSL', 'SOIL',\n        'ROCK', 'GRNT', 'SDCY', 'SLSN', 'CGLM', 'MDSN', 'UNKN', 'COAL'],\n       dtype=object), id=None),\n 'input_ids': Sequence(feature=Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), length=-1, id=None),\n 'token_type_ids': Sequence(feature=Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), length=-1, id=None),\n 'attention_mask': Sequence(feature=Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), length=-1, id=None)}\n\n\n\ntok_ds['input_ids'][0]\n\n[tensor([    1,  3592, 14432,  8076,     2,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0])]\n\n\n\n# https://huggingface.co/docs/transformers/training\n\n\n# from Jeremy's notebook:\n# def compute_metrics(eval_pred):\n#     logits, labels = eval_pred\n#     predictions = np.argmax(logits, axis=-1)\n#     return metric.compute(predictions=predictions, references=labels)\n\n\n# Defining the Trainer to compute Custom Loss Function, adapted from [Simple Training with the 🤗 Transformers Trainer, around 840 seconds](https://youtu.be/u--UVvH-LIQ?t=840)\nclass WeightedLossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Feed inputs to model and extract logits\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # Extract Labels\n        labels = inputs.get(\"labels\")\n        # Define loss function with class weights\n        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n        # Compute loss\n        loss = loss_func(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\ndef compute_metrics(eval_pred):\n    labels = eval_pred.label_ids\n    predictions = eval_pred.predictions.argmax(-1)\n    f1 = f1_score(labels, predictions, average=\"weighted\")\n    return {\"f1\": f1}\n\n\noutput_dir = \"./hf_training\"\nbatch_size = 64 # 128\nepochs = 5\nlr = 8e-5\n\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=epochs,\n    learning_rate=lr,\n    lr_scheduler_type=\"cosine\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size * 2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    logging_steps=len(dds[\"train\"]),\n    fp16=True,\n    push_to_hub=False,\n    report_to=\"none\",\n)\n\n\nmodel = model.to(\"cuda:0\")\n\nThe above nay not be strictly necessary, depending on your version of transformers. I bumped into the following issue, which was probably the transformers 4.11.3 bug: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dds[\"train\"],\n    eval_dataset=dds[\"test\"],\n    tokenizer=tokz,\n    compute_metrics=compute_metrics,\n)\n\nUsing amp half precision backend"
  },
  {
    "objectID": "posts/2022-06-13-lithology-classification-hugging-face-2.html#training",
    "href": "posts/2022-06-13-lithology-classification-hugging-face-2.html#training",
    "title": "Lithology classification using Hugging Face, part 2",
    "section": "Training?",
    "text": "Training?\nYou did read the introduction and its spoiler alert, right?\n\ntrainer.train()\n\n/home/abcdef/miniconda/envs/hf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 92742\n  Num Epochs = 5\n  Instantaneous batch size per device = 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 7250\n\n\nRuntimeError: The size of tensor a (768) must match the size of tensor b (128) at non-singleton dimension 3"
  },
  {
    "objectID": "posts/2022-08-22-doxygen-doxybook-mkdocs.html",
    "href": "posts/2022-08-22-doxygen-doxybook-mkdocs.html",
    "title": "Presenting doxygen C++ API documentation via MkDocs with doxybook2",
    "section": "",
    "text": "The codebase Uchronia - time series handling for ensembles simulations and forecasts in C++ comprises a C++ core with Python and R bindings. The Python package documentation is using MkDocs to generate its API documentation, in line with its dependencies c-interop and Python refcount.\nThe C++ core is (to some extent) documented in the source code with formatted comments, and are extracted with Doxygen, a long established de-facto standard.\nWhile the Python, R and C++ parts of the code base usually address different types of users, I was wondering whether it is possible to host at least the Python and C++ API documentation jointly in a MkDocs based site. While it is somewhat subjective, one purpose is to host the documentation via a pleasant Web based interface such as this."
  },
  {
    "objectID": "posts/2022-08-22-doxygen-doxybook-mkdocs.html#installing-software",
    "href": "posts/2022-08-22-doxygen-doxybook-mkdocs.html#installing-software",
    "title": "Presenting doxygen C++ API documentation via MkDocs with doxybook2",
    "section": "Installing software",
    "text": "Installing software\nDoxygen is mainstream and on a Debian linux available with sudo apt install doxygen\n\ndoxybook2\ndoxybook2 has precompiled Windows binaries. For Linux, installing from the source code appears the way to go.\ngit clone --depth=1 https://github.com/matusnovak/doxybook2.git\ncd doxybook2/\nThe instructions are to use Microsoft vcpkg to manage dependencies, which worried me at first. Usually I would assume dependencies would be available via apt, but the list catch2 fmt dirent fmt nlohmann-json inja cxxopts spdlog makes me think there would be some not readily available. But, vcpkg appears to be multi-platform these days; I had not looked closely for years. Let’s give it a try.\ncd $HOME/src\ngit clone https://github.com/microsoft/vcpkg\n./vcpkg/bootstrap-vcpkg.sh\nThis appears to work fine.\nNow on to building doxybook2. I had to adjust a bit the instructions given by the doxybook2 Readme. There are probably some assumptions as to how vcpkg was bootstrapped, perhaps with sudo. Anyway, figuring out was not that difficult; I had to locate vcpkg.cmake, and the executable doxybook2 output was not in the place I expected it to be.\ncmake -B ./build -G \"Unix Makefiles\"     -DCMAKE_BUILD_TYPE=MinSizeRel     -DCMAKE_TOOLCHAIN_FILE=${HOME}/src/vcpkg/scripts/buildsystems/vcpkg.cmake\ncmake --build ./build --config MinSizeRel  # could do with a -j4 perhaps\n${HOME}/src/doxybook2/build/src/DoxybookCli/doxybook2 --help\nLet’s define a shorcut:\nDX2=${HOME}/src/doxybook2/build/src/DoxybookCli/doxybook2"
  },
  {
    "objectID": "posts/2022-08-22-doxygen-doxybook-mkdocs.html#configuration",
    "href": "posts/2022-08-22-doxygen-doxybook-mkdocs.html#configuration",
    "title": "Presenting doxygen C++ API documentation via MkDocs with doxybook2",
    "section": "Configuration",
    "text": "Configuration\n\nDoxygen settings\nAdjust the doxygen settings in the Doxyfile. Below are some settings changed compared to the default config file, some found only after the fact (i.e. seeing the final output through MkDocs).\nPROJECT_NAME           = \"uchronia\"\n###  Also PROJECT_BRIEF, etc.\nOUTPUT_DIRECTORY       = \"./doxyoutput\"\n###\nINPUT   = ../datatypes/datatypes/include/datatypes\n###\nFULL_PATH_NAMES        = NO\n###\nINPUT_FILTER           = \" sed 's/^\\s*DATATYPES_API\\s*//g;s/^\\s*DATATYPES_DLL_LIB\\s*//g;s/DATATYPES_DLL_LIB//g' \"\n###\nGENERATE_XML           = YES\n###\nXML_OUTPUT             = xml\nFULL_PATH_NAMES = NO is useful to avoid having your full machine-specific file paths shown in the resulting output. INPUT_FILTER is less obvious, and relate to one technique in C libraries that may not be all that unusual (since derived from a Microsoft best practice as I recall). uchronia headers use macros such as #define DATATYPES_DLL_LIB __declspec(dllexport) on Windows, necessary to define which functions are exported (or imported). This results in C / C++ code such as :\n    DATATYPES_API char** GetEnsembleDatasetDataIdentifiers(ENSEMBLE_DATA_SET_PTR dataLibrary, int* size);\n    //, or:\n    class DATATYPES_DLL_LIB TimeSeriesChecks{\n        //\n    }\nThis confuses doxygen and/or doxybook2, resulting in noisy or even incorrect rendered HTML output. INPUT_FILTER is here to strip out these macros prior to parsing. You may need to find such statement for your library. Consider using regex101 if you, like me, are shamefully not on top of regular expressions.\ndoxygen Doxyfile\n\n\ndoxybook2 settings\nThe doxybook2 repo suggests looking at the MkDocs + Material theme example, as this is the theme I use.\nThe config-doxybook2.json requires one change for the links. It took me a few trial and errors to figure out that I needed to set \"baseUrl\": \"/uchronia-ts-doc/cpp/\" for URL links to work in the final API documentation generated on github pages.\n{\n  \"baseUrl\": \"/uchronia-ts-doc/cpp/\",\n  \"indexInFolders\": true,\n  \"linkSuffix\": \"/\",\n  \"indexClassesName\": \"index\",\n  \"indexFilesName\": \"index\",\n  \"indexGroupsName\": \"index\",\n  \"indexNamespacesName\": \"index\",\n  \"indexRelatedPagesName\": \"index\",\n  \"indexExamplesName\": \"index\",\n  \"mainPageInRoot\": true,\n  \"mainPageName\": \"index\"\n}\nThis \"baseUrl\": \"/uchronia-ts-doc/cpp/\" deserves a bit of an explanation so that you hopefully don’t have to trial much (it is not rocket science, but…).\nThe markdown documentation for the site will be located under a docs folder, the mkdocs default. The C++ derived markdown documents will be under a subfolder docs/cpp. doxygen did not know about this upfront (though there may be a way to do it, but nevermind), so doxybook2 is where this prefix can be added to building URLs.\nThe MkDocs settings mkdocs.yml will have a site_url: https://csiro-hydroinformatics.github.io/uchronia-ts-doc/ specified. See the MkDocs configuration doc for explanations. To be honest I am very sketchy in Web URL things, and not sure how to explain reliably what is going on in detail. Just know that \"baseUrl\": \"/uchronia-ts-doc/cpp/\" was required in the doxybook2 config file, with leading and trailing / characters, for links to work. I hope this helps you speed up finding your own exact settings.\n\n\nRunning doxybook2\nSo, our doxygen Doxyfile has created a doxyoutput folder. Now let us convert to markdown:\nmkdir -p docs/cpp\n$DX2 --input ./doxyoutput/xml --output ./docs/cpp --config config-doxybook2.json\n# . ~/config/baseconda\n# conda activate mkdocsenv\nmkdocs build --clean --site-dir _build/html --config-file mkdocs.yml\nmkdocs serve"
  },
  {
    "objectID": "posts/2022-08-22-doxygen-doxybook-mkdocs.html#finally",
    "href": "posts/2022-08-22-doxygen-doxybook-mkdocs.html#finally",
    "title": "Presenting doxygen C++ API documentation via MkDocs with doxybook2",
    "section": "Finally",
    "text": "Finally\nUpdating the github pages site is simply:\nmkdocs gh-deploy --clean --site-dir _build/html --config-file mkdocs.yml\nSome rough corners left, but it largely works:\n\n\n\n“uchronia-c-api-doxybook2.png”"
  },
  {
    "objectID": "posts/2022-06-18-conda-channels-prep-work.html",
    "href": "posts/2022-06-18-conda-channels-prep-work.html",
    "title": "Setting up a private conda channel - part 1",
    "section": "",
    "text": "I have a conda package for a compiled native library (moirai). While it may be in a position to be submitted to conda-forge, I will also be interested at some point in setting up “my” own conda channel, with a view to distribute packages on my enterprise intranet. I may as well use that library moirai to test setting up a custom conda channel.\nThis post will be a review of the possible ways to do this."
  },
  {
    "objectID": "posts/2022-06-18-conda-channels-prep-work.html#conda-documentation",
    "href": "posts/2022-06-18-conda-channels-prep-work.html#conda-documentation",
    "title": "Setting up a private conda channel - part 1",
    "section": "conda documentation",
    "text": "conda documentation\nThe main conda documentation has sections on managing channels and creating custom channels."
  },
  {
    "objectID": "posts/2022-06-18-conda-channels-prep-work.html#fast.ai",
    "href": "posts/2022-06-18-conda-channels-prep-work.html#fast.ai",
    "title": "Setting up a private conda channel - part 1",
    "section": "fast.ai",
    "text": "fast.ai\nJeremy Howard has written the post “fastchan, a new conda mini-distribution”. It is a good read giving the rationale for setting up this channel. The repo of the conda channel ‘fastconda’ has some elements of pipelines but may be a good starting point for channels hosted by Anaconda. I am not sure I can repurpose this for a private channel."
  },
  {
    "objectID": "posts/2022-06-18-conda-channels-prep-work.html#reusing-conda-forge",
    "href": "posts/2022-06-18-conda-channels-prep-work.html#reusing-conda-forge",
    "title": "Setting up a private conda channel - part 1",
    "section": "Reusing conda-forge?",
    "text": "Reusing conda-forge?\nSince I intuit that a process similar to that of conda-forge could be what is emulated internally, an option may be to upfront borrow from the conda-forge documentation\nThe workhorse of conda-forge appears to be using conda-smithy to manage your CI, though that section actually rather confused me at the first read. The github readme of conda-smithy may be a better starting point.\nhttps://github.com/conda-forge/astra-toolbox-feedstock is a recently accepted recipe. Likely a good template to study for the swift stack."
  },
  {
    "objectID": "posts/2022-06-18-conda-channels-prep-work.html#commercial-offerings",
    "href": "posts/2022-06-18-conda-channels-prep-work.html#commercial-offerings",
    "title": "Setting up a private conda channel - part 1",
    "section": "Commercial offerings",
    "text": "Commercial offerings\n\nAt an enterprise level it may be better to use Anaconda Server rather than cook up something. I probably cannot afford the time to trial standing one up, but my IM&T business unit may look at it. In particular, if there are use cases for sophisticated user based access controls, “free” solutions may not be up to scratch nor to scale.\nAnaconda cloud thingy\nChannel on anaconda.org"
  },
  {
    "objectID": "posts/2022-06-18-conda-channels-prep-work.html#other",
    "href": "posts/2022-06-18-conda-channels-prep-work.html#other",
    "title": "Setting up a private conda channel - part 1",
    "section": "Other",
    "text": "Other\n\nHow To: Set up a local Conda channel for installing the ArcGIS Python API\nBuilding a Private Conda Channel\nSetting up a feedstock from scratch\nThe github repo Private Conda Repository may be a good first step.\n\nBy the way one misleading thing if you google “How do I set up a conda channel?”: the erronously titled video How to create new channel in Anaconda (python) is actually demonstrating the creation of a conda environment. Skip."
  },
  {
    "objectID": "posts/20230315-windows-installer-wix/index.html",
    "href": "posts/20230315-windows-installer-wix/index.html",
    "title": "Windows installer with WiX 4 - part 1",
    "section": "",
    "text": "“WiX logo”"
  },
  {
    "objectID": "posts/20230315-windows-installer-wix/index.html#wix",
    "href": "posts/20230315-windows-installer-wix/index.html#wix",
    "title": "Windows installer with WiX 4 - part 1",
    "section": "WiX",
    "text": "WiX\nVersion 3 of the WiX toolset is mostly documented as of March 2023, V4 is understanbly not as well documented. I was looking for “cheap” options, i.e. adapting a suitable template, but did not find this kind of resource in the WiX doc. A very good third party resource I found is WiX Installer Examples. It is based in WiX 3 for now, but a resource I am likely to look at even if I trial WiX version 4.\nA useful resource to get acquainted with WiX 4, as I write, is a playlist of coding dojos (note that episodes are in reverse chronological order) by the lead developer of WiX, Rob Mensching. The code repository used for the successive sessions may be here. Note that as these videos were posted weekly, WiX 4 was in preview, so expect that some things there may change, and have changed as we’ll see below.\nThere are new project templates in a Visual Studio extension Heatwave, by FireGiant, but for this first post I will probably stick to manually crafting the files. Nevertheless, installing the extension brings some intellisense goodness to Visual Studio, so if you have VS you should install the Heatwave extension.\nFor subsequent posts not this present one: we will likely use the WiX doc section on code signing for WiX, albeit for version 3. Also, requested a certificate via digicert using this procedure. Note the key length is 3K minimum now."
  },
  {
    "objectID": "posts/20230315-windows-installer-wix/index.html#windows-sandbox",
    "href": "posts/20230315-windows-installer-wix/index.html#windows-sandbox",
    "title": "Windows installer with WiX 4 - part 1",
    "section": "Windows Sandbox",
    "text": "Windows Sandbox\nThe dojo playlist has an entry on Building a windows sandbox. Sounds like a very good idea. Following the Windows Sandbox MS doc.\nI will only give the essential steps, checking the video gives more information.\nin PowerShell as Administrator :\n\n\n\n\n\n\nWarning\n\n\n\nCAUTION: the following powershell admin command below will reboot your machine without asking permission.\nEnable-WindowsOptionalFeature -FeatureName \"Containers-DisposableClientVM\" -All -Online\n\n\nThen as per the Building a windows sandbox video, setting up a Windows sandbox file and adapting a little bit the DeploymentDojo/BeltTest to my context.\n\n\n\ndojo.wsb\n\n<Configuration>\n  <MappedFolders>\n    <MappedFolder>\n      <HostFolder>C:\\src\\sffs-docs</HostFolder>\n      <SandboxFolder>C:\\dojo</SandboxFolder>\n    </MappedFolder>\n  </MappedFolders>\n  <LogonCommand>\n    <Command>C:\\dojo\\wip\\dojostart.cmd</Command>\n  </LogonCommand>\n</Configuration>\n\n\nThe cmd file has the very same content as the github repo:\n\n\n\ndojostart.cmd\n\nstart appwiz.cpl\nstart \"\" \"C:\\dojo\"\nstart \"\" \"C:\\Program Files (x86)\"\n@ rem start regedit\n\n\n.\\dojo.wsb works fine."
  },
  {
    "objectID": "posts/20230315-windows-installer-wix/index.html#step-1",
    "href": "posts/20230315-windows-installer-wix/index.html#step-1",
    "title": "Windows installer with WiX 4 - part 1",
    "section": "Step 1",
    "text": "Step 1\nStarting from https://youtu.be/IXc6_i0Pm4E?list=PLDlzbQXIs18slmqmdlS10_de_Cps-QRg6 to adapt to my context\n<Wix xmlns=\"http://wixtoolset.org/schemas/v4/wxs\">\n  <Package Name=\"SF\" Manufacturer=\"CSIRO\" Version=\"0.1\" UpgradeCode=\"2e7d7f1d-1111-1111-1111-b363c7ce3a1e\">\n    <MajorUpgrade DowngradeErrorMessage=\"A newer version was detected\"/>\n    <Feature Id=\"All\">\n      <Component Directory=\"InstallFolder\">\n        <File Source=\"C:\\local\\libs\\64\\swift.dll\"/>\n      </Component>\n    </Feature>\n  </Package>\n  <StandardDirectory Id=\"ProgramFilesFolder\">\n    <Directory Id=\"InstallFolder\" Name=\"SF\">\n    </Directory>\n  </StandardDirectory>\n</Wix>\nwix build .\\sf.wxs\nerror WIX0005: The Wix element contains an unexpected child element ‘StandardDirectory’.\nOn a hunch looking at the code repo, I reckon I may try to put the standarddirectory block inside a Fragment. No squigly line anymore.\n<Wix xmlns=\"http://wixtoolset.org/schemas/v4/wxs\">\n  <Package Name=\"SF\" Manufacturer=\"CSIRO\" Version=\"0.1\" UpgradeCode=\"2e7d7f1d-8a18-47ea-ae43-b363c7ce3a1e\">\n    <MajorUpgrade DowngradeErrorMessage=\"A newer version was detected\"/>\n    <Feature Id=\"All\">\n      <Component Directory=\"InstallFolder\">\n        <File Source=\"C:\\local\\libs\\64\\swift.dll\"/>\n      </Component>\n    </Feature>\n  </Package>\n  <Fragment>\n    <StandardDirectory Id=\"ProgramFilesFolder\">\n      <Directory Id=\"InstallFolder\" Name=\"SF\">\n    </Directory>\n  </StandardDirectory>\n  </Fragment>\n</Wix>\nand it builds.\n\n\n\n\n\n\nWarning\n\n\n\nNote: I first launched the “Add or Remove Programs” in the sandbox to check the app installation. But it did not show the installed application, with a continuous spinning wheel. You need to open the older interface with appwiz.cpl (as in the startup batch file)"
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "",
    "text": "Azure pipeline run"
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#outputs",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#outputs",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Outputs",
    "text": "Outputs\n\nDebian packages for installing C++ pre-compiled libraries and header files\nWindows DLLs of these C++ pre-compiled libraries, compiled with Microsoft Visual C++ 2019.\nPython wheels of packages accessing these C++ libraries\nR packages accessing these C++ libraries. Source tarballs for Linux, and binary packages for Windows\nMatlab functions accessing these C++ libraries\n\nConda packages are an option for later - see appendix.\nThe outputs of this/these pipelines is used for installation as described for instance in these instructions"
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#inputs",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#inputs",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Inputs",
    "text": "Inputs\nThese C++, Python and R packages are in the following source code repositories.\n\nThird party libraries: boost libraries, netcdf, yaml-cpp and jsoncpp, threadpool, Catch headers.\n\nBoost C++\nnetCDF4\nyaml-cpp\njsoncpp\nthreadpool\n\nOpen source repositories with generic capabilities:\n\nrcpp-interop-commons: supporting C API interoperability with Python, R, etc.\nmoirai: Manage C++ Objects’s lifetime when exposed through a C API\npyrefcount: Reference counting facilities for Python\nwila: C++ header-only metaheuristics optimisation\nconfig-utils\nefts\nefts-python\nmhplot: R visualisation for metaheuristics optimisation\nuchronia-time-series: time series handling for ensembles simulations and forecasts in C++\n\nClosed source, domain specific:\n\nswift: Streamflow Water Information Forecasting Tools\nFoGSS: A model for generated forecast guided stochastic scenarios of monthly streamflows out to 12 months\nA repository with semi-automated build scripts\n\n\nAnother repository worth mentioning even if I do not envisage it being used is c-api-wrapper-generation. It contains some fairly sophisticated capabilities for generating language bindings on top of C APIs."
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#pipelines-code-repositories",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#pipelines-code-repositories",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Pipelines code repositories",
    "text": "Pipelines code repositories\nA clone of the the azure devops pipelines described in this post are now mirrored at:\n\nhydro-forecast-linux-pipeline\nhydro-forecast-windows-pipeline"
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#build-platforms",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#build-platforms",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Build platforms",
    "text": "Build platforms\nFor Linux I actually ended up building a Debian docker image on top of the ubuntu-latest image Azure Devops offers. This was partly an arbitrary choice (habits, and starting by reusing another pipeline).\nFor Windows, the windows-2019 image is more or less a given. Note that if I had chosen to, or had to, build with an older compiler version (for example, Python and conda on Windows I believe is at least recommending the 2017 toolchain currently), installing the Microsoft build toolchain would have been needed."
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#checking-out-source-code",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#checking-out-source-code",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Checking out source code",
    "text": "Checking out source code\nThere is a description of how to check out multiple repositories in your pipeline, which I may move to in the future but frankly felt cumbersome when I gave it a try, for that many repositories. Besides, I am starting from existing build scripts (bash or DOS) which I have an incentive to reuse.\nWhich brings us to dealing safely with checking out closed source code, requiring authentication.\n\nUsing Personal Access Tokens (PAT)\nDisclaimer: I believe the following recipe to be in line with best practices, but it is up to you to decide.\nWhen googling for the documentation, Use personal access tokens tends to be what you land on, but this is more confusing than helpful. Set secret variables contains the key recipe to set up a secret PAT and pass it on to a pipeline task. Assuming you already have a PAT that you defined in a pipeline variable SWIFT_PAT, for which you checked Keep this value secret, you need to map this value to pass it to the script checking out source code:\n    - script: |\n        call dos-setup.bat\n        call checkout.bat\n      env:\n        SWIFT_PAT_ENV_VAR: $(SWIFT_PAT) # the recommended way to map to an env variable\n        # NOTE: you cannot have the same name:\n        # SWIFT_PAT: $(SWIFT_PAT) # <-- fails with a circular definition issue.\nIn your checkout.bat script you can retrieve set MY_PAT=%SWIFT_PAT_ENV_VAR% and use it as part of the URL portion e.g. set MY_BITBUCKET_URL_ROOT=https://%YOUR_USERNAME%:%MY_PAT%@bitbucket.csiro.au/scm\n\n\nA gotcha with Bitbucket PATs\nif your closed source code is on Bitbucket, the bitbucket documentation HTTP access tokens is quite clear, but fails to document something. Bitbucket PAT generation tends to create them with slash characters “/” which is a special character for URLs so it needs to be replace with “%2F” before you use it as a value for your Azure Pipeline, otherwise you end up with an invalid URL error when checking out. Note also that %2 is fraught when used in DOS scripts, but if you use the recipe above, you should not come across an issue with this."
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#building-c-code",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#building-c-code",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Building C++ code",
    "text": "Building C++ code\n\nLinux\nCompilation on Linux is performed using cmake. The pipeline is not calling cmake and make commands directly, but building debian packages. One purpose of this build pipeline is to produce binary installable packages. It took me a fair bit of trial and error, two years ago, to find a suitable working recipe for Debian packaging of “my” libraries. There is documentation and plenty of examples out there, but plenty of variances in how it is done. An example can be found in the Moirai repository, and the pipeline build commands are in build_debian_pkgs.sh.\n\n\nWindows\nCompilation is done using microsoft visual c++ 2019, using visual studio solution files. Years ago, after enough frustrations invikig MSBuild.exe from DOS scripts, I moved to using powershell. Currently I am using the powershell Invoke-MsBuild as a third party tool to invoke compilation, rather than mdsbuild.exe directly. This is a nifty basis to build my own powershell module (not open source yet), with cmdlets such as:\nInstall-SharedLibsMultiCfg -Solutions $lvlTwoSlns -LibsDirs $libsDirs -BuildPlatforms $buildPlatforms -BuildMode $buildMode -ToolsVersion $toolsVersion -LibNames $lvlTwoLibnames\nwhich is roughly an equivalent of make && make install on Linux. The script for the step is build-stack.ps1, and the bulk of the runtime in the pipeline."
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#unit-tests",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#unit-tests",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Unit tests",
    "text": "Unit tests\nAll C++ unit tests are run on the Windows platform. (Debian to follow soon). There are some lessons learnt in setting these up in a pipeline, but for now most of the code remains closed source and may be explored in another post."
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#python-wheels-r-package-tarballs",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#python-wheels-r-package-tarballs",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Python wheels, R package tarballs",
    "text": "Python wheels, R package tarballs\nThe python package involved are “pure python” by design, and the wheels built are platform-agnostic and built only once on Linux (build_python_pkgs.sh). Most packages do deal a lot with interoperability with native libraries with a C API, but this is done via cffi as a runtime step.\nR source packages can be built on either platform. Except R Windows binary packages, which are much preferable for Windows. build_r_pkgs.sh for the Linux pipeline builds these, also building tutorials (“vignettes”) which are contributing to the testing regime, notably for the interoperability with the native libraries of the stack.\nI decided not to build the R tarballs on Windows, but try instead to downloadm, from the Window pipeline, the last output resulting the Linux pipeline. Partly for the sake of gaining know-how. I ended up with way more confusion and frustration with this than I anticipated, as reflected in my stackoverflow answer and previous blog post.\nDownloading the artifact from Windows is still done via a shell script fetch-pkgs.sh. The script is run mapping a AZURE_DEVOPS_EXT_PAT environment variable. The rather stunted Sign in with a personal access token (PAT) page somewhat explains why this is used.\n\nBash on windows in Azure Pipelines: a gotcha with backslash directories\nThis is an occasion to point to an annoyance of backslashes as directory separators on Windows. I had defined a pipeline variable linux_packages_dir: $(Build.ArtifactStagingDirectory)\\swift_linux, which would have been something like D:\\a\\s\\a\\swift_linux. In the script of a pipeline task task: Bash@3, if you use the variable evaluation $(linux_packages_dir) you loose the backslash separators. I had to rebuild the path and replace with forward slashes using the environment variable BUILD_ARTIFACTSTAGINGDIRECTORY, which is adding to the entropy.\n    inputs:\n        targetType: 'inline'\n        script: |\n          # using $(linux_packages_dir) here fails; dir separators are lost.\n          # linux_packages_dir=$(linux_packages_dir)\n          # linux_packages_fwd_dir=\"${linux_packages_dir//\\\\//}\"\n          # instead have to do:\n          linux_packages_fwd_dir=\"${BUILD_ARTIFACTSTAGINGDIRECTORY//\\\\//}/swift_linux\"\n          ./fetch-pkgs.sh ${linux_packages_fwd_dir}\n      env:\n        AZURE_DEVOPS_EXT_PAT: $(AZ_ARTIFACT_DL_PAT)"
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#azure-artifacts",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#azure-artifacts",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Azure Artifacts",
    "text": "Azure Artifacts\nOutputs are published as collections of files in a “Universal Package”. I have (had - may have changed) not found in the azure devops API way to query the artifacts for the version without downloading the whole artifact. A workaround is to publish two packages: a small one with the version number, and the real artifact.\nTo get a custom $(Build.BuildNumber), and r is a counter reset to 1 every change of the major/minor versions, use name: '0.1.$(Rev:r)' on your pipeline. Then the publishing tasks can be:\n    - task: UniversalPackages@0\n      displayName: Publish output bundle\n      inputs:\n        command: publish\n        publishDirectory: '$(Build.ArtifactStagingDirectory)\\release'\n        vstsFeedPublish: 'my_project_name/hydro_forecast_win'\n        vstsFeedPackagePublish: 'swift_win'\n        versionOption: custom\n        versionPublish: '$(Build.BuildNumber)'\n        packagePublishDescription: 'Windows packages for swift and co.'\n\n    - task: UniversalPackages@0\n      displayName: Publish output bundle version\n      inputs:\n        command: publish\n        publishDirectory: '$(Build.ArtifactStagingDirectory)\\version'\n        vstsFeedPublish: 'my_project_name/hydro_forecast_win'\n        vstsFeedPackagePublish: 'swift_win_version'\n        versionOption: custom\n        versionPublish: '$(Build.BuildNumber)'\n        packagePublishDescription: 'Version number for windows swift and co. bundle'"
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#other-possibilities",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#other-possibilities",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Other possibilities",
    "text": "Other possibilities\n\nI have explored using conda packaging to distribute the whole software stack. This appears feasible but is a leap I am not ready to do for several reasons (time, resources and confirmed user demands being the main ones). There is a substantial learning curve, technical and in terms of governance of a private conda channel.\nCaching some of the downloaded test data and third party libraries.\nPublishing a linux debian docker image ready to use as a baseline."
  },
  {
    "objectID": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#resources",
    "href": "posts/2022-07-31-azure-pipeline-vcpp-stack.html#resources",
    "title": "Azure devops CI pipeline for hydrologic forecasting software",
    "section": "Resources",
    "text": "Resources\nURLs which may or may not have influenced this work:\n\nConda-forge default azure pipeline\nazure devops pipeline variables\nvisual-studio-build-tools-2017-silent-install may be handy if 2017 builds are needed e.g for conda.\nWindows2019 image environment\nAzure pipelines for Marian NMT\nTrigger Build Task"
  },
  {
    "objectID": "posts/2022-09-03-c-api-wrapper-generation.html",
    "href": "posts/2022-09-03-c-api-wrapper-generation.html",
    "title": "Generate programming language bindings to a C API",
    "section": "",
    "text": "gluecode-generation"
  },
  {
    "objectID": "posts/2022-09-03-c-api-wrapper-generation.html#history",
    "href": "posts/2022-09-03-c-api-wrapper-generation.html#history",
    "title": "Generate programming language bindings to a C API",
    "section": "History",
    "text": "History\nIn 2014 after settling for C++ for a modelling core. I had R, Matlab and Python users, some need to interop with C# code. And some constraints on using different C++ compilers in the stack. So I decided on designing C APIs, the lingua franca of in-process interop. I had some prior exposure to the Mono C core and interop with R. But not extensive first hand.\nMy first port of call for glue code was a no-brainer: SWIG, which I first encountered in an internship in… 1997. I did not document my tribulations in details, but I encountered many difficulties when testing feasibility. Basically, I concluded that SWIG was just not a good fit for a C API like mine, especially one with opaque pointers (void*).\nThere were other offering for code generation for R, or Python. But apart from SWIG, I do not recall locating any suitable glue code generation for multiple target languages. I may revisit in more details the tribulations in another post, but this was the reason for the inception of c-api-wrapper-generation."
  },
  {
    "objectID": "posts/2022-09-03-c-api-wrapper-generation.html#foreword---google-search",
    "href": "posts/2022-09-03-c-api-wrapper-generation.html#foreword---google-search",
    "title": "Generate programming language bindings to a C API",
    "section": "Foreword - Google search",
    "text": "Foreword - Google search\nBefore starting this post, I wanted to check out what was the state of play out there and I was googling using the terms “generate bindings for a C API”. Our c-api-wrapper-generation was coming up in the first few items. I thought Google was biasing the search given the wealth of information it has, I mean, come on, there must be thousands of similar endeavours out there. No way.\nBut, after some friends help and simulating searches from another geolocation, it seems Google is not cheating.\nNot sure what to make of this yet. Oh, and searching “generating binding to a c api” instead of “generating bindings to a c api” wields a different list…\nAnyway, a quick review before looking at c-api-wrapper-generation."
  },
  {
    "objectID": "posts/2022-09-03-c-api-wrapper-generation.html#brief-scan-of-the-state-of-the-art",
    "href": "posts/2022-09-03-c-api-wrapper-generation.html#brief-scan-of-the-state-of-the-art",
    "title": "Generate programming language bindings to a C API",
    "section": "Brief scan of the state of the art",
    "text": "Brief scan of the state of the art\nThere is an excellent post Automatic Language Bindings by Andrew Weissflog, which I recommend. Many statements and observations resonate with me. Related directly or indirectly to the post above (I intuit) I found C2CS. I am very intrigued by their use of an abstract syntax tree (AST), I think. My approach was more crude (but perhaps pragmatic).\nFrom the Rust community this thread and rust-bindgen\nA wealth of resources in BindingCodeToLua\npybind11 is something I already looked at 3 years ago in this post. I probably would have settled on it if it was purely about Python bindings, directly to the C++ code."
  },
  {
    "objectID": "posts/2022-09-03-c-api-wrapper-generation.html#c-api",
    "href": "posts/2022-09-03-c-api-wrapper-generation.html#c-api",
    "title": "Generate programming language bindings to a C API",
    "section": "C API",
    "text": "C API\nThe codebase uchronia has a C API with functions such as:\n    DATATYPES_API char** GetEnsembleDatasetDataIdentifiers(ENSEMBLE_DATA_SET_PTR dataLibrary, int* size);\n    DATATYPES_API ENSEMBLE_FORECAST_TIME_SERIES_PTR CreateEnsembleForecastTimeSeries(date_time_to_second start, int length, const char* timeStepName);\n    DATATYPES_API void GetTimeSeriesValues(TIME_SERIES_PTR timeSeries, double * values, int arrayLength);\nThe low-level Python binding generated from parsing this C API result in the following type of code. Note that opaque pointers such as ENSEMBLE_DATA_SET_PTR have corresponding higher level Python class equivalents such as TimeSeriesLibrary, even in the “low-level” glue code.\ndef GetEnsembleDatasetDataIdentifiers_py(dataLibrary:'TimeSeriesLibrary'):\n    dataLibrary_xptr = wrap_as_pointer_handle(dataLibrary)\n    size = marshal.new_int_scalar_ptr()\n    values = uchronia_so.GetEnsembleDatasetDataIdentifiers(dataLibrary_xptr.ptr, size)\n    result = charp_array_to_py(values, size[0], True)\n    return result\n\ndef CreateEnsembleForecastTimeSeries_py(start:datetime, length:int, timeStepName:str) -> 'EnsembleForecastTimeSeries':\n    # glue code\ndef GetTimeSeriesValues_py(timeSeries:'TimeSeries', values:np.ndarray, arrayLength:int) -> None:\n    # glue code\nFor information, the equivalent generated glue code for an R package uchronia are in files src/rcpp_generated.cpp and R/uchronia-pkg-wrap-generated.r."
  },
  {
    "objectID": "posts/2022-09-03-c-api-wrapper-generation.html#outline-generating-the-python-glue-code",
    "href": "posts/2022-09-03-c-api-wrapper-generation.html#outline-generating-the-python-glue-code",
    "title": "Generate programming language bindings to a C API",
    "section": "Outline generating the python glue code",
    "text": "Outline generating the python glue code\nSee the wrapper generator c-api-wrapper-generation. The README.md has setup instructions for dotnet, if need be.\nThere is a history behind why the conversion engine runs on .NET (and is written in C#), but I will not dwell on it in this post. Note that as of August 2022, even on Linux packages from the .NET ecosystem are readily available from Microsoft or even Ubuntu official package repositories.\nIf you have dotnet installed dotnet --info returns as of August 2022:\n.NET SDK (reflecting any global.json):\n Version:   6.0.302\n Commit:    c857713418\n\nRuntime Environment:\n OS Name:     debian\n OS Version:  11\n OS Platform: Linux\n RID:         debian.11-x64\n Base Path:   /usr/share/dotnet/sdk/6.0.302/\n\nglobal.json file:\n  Not found\n\nHost:\n  Version:      6.0.7\n  Architecture: x64\n  Commit:       0ec02c8c96\n\n.NET SDKs installed:\n  6.0.302 [/usr/share/dotnet/sdk]\n\n.NET runtimes installed:\n  Microsoft.AspNetCore.App 6.0.7 [/usr/share/dotnet/shared/Microsoft.AspNetCore.App]\n  Microsoft.NETCore.App 3.1.27 [/usr/share/dotnet/shared/Microsoft.NETCore.App]\n  Microsoft.NETCore.App 6.0.7 [/usr/share/dotnet/shared/Microsoft.NETCore.App]\nThe codegen engine is C#, and we’ll give an overview later. Compiling it is done like so:\ncd ~/src/c-api-wrapper-generation\ncd engine/ApiWrapperGenerator\ndotnet restore ApiWrapperGenerator.sln\ndotnet build --configuration Debug --no-restore ApiWrapperGenerator.sln\nThis will create the library c-api-wrapper-generation/engine/ApiWrapperGenerator/bin/Debug/netstandard2.0/ApiWrapperGenerator.dll, which contains the glue code generation engine. More details about it later, but for now we will walk through the usage first.\nOne handy interactive option to “script” binding generation for a library is the F# language. A script to generate the python bindings for uchronia is as follow. Note that most of the F# scripting code below is also (re-)used for other C APIs than uchronia’s\nNote: the following code is not yet publicly available. It should be in a revised post\nopen System.Collections.Generic\n\n#r \"/home/abcdef/src/c-api-wrapper-generation/engine/ApiWrapperGenerator/bin/Debug/netstandard2.0/ApiWrapperGenerator.dll\"\n// #r @\"C:\\src\\github_jm\\c-api-wrapper-generation\\engine\\ApiWrapperGenerator\\bin\\Debug\\netstandard2.0\\ApiWrapperGenerator.dll\"\nopen ApiWrapperGenerator\n\n#load \"gen_py_common.fsx\"\nopen Gen_py_common\n\ngen_py_common.fsx\ngen_py_common.fsx is a way to reuse boilerplate to generate code across several codebases. Typically, it includes a templated header with mostly similar functions with a few variations across use cases.\nlet prependHeaderTemplate = \"##################\n# \n# *** THIS FILE IS GENERATED ****\n# DO NOT MODIFY IT MANUALLY, AS YOU ARE VERY LIKELY TO LOSE WORK\n# \n##################\n\n# Some module imports such as:\nfrom datetime import datetime\nfrom refcount.interop import CffiData, OwningCffiNativeHandle, DeletableCffiNativeHandle, wrap_as_pointer_handle\nfrom cinterop.cffi.marshal import as_bytes, TimeSeriesGeometryNative\n# imports with string templating, for your specific package \nfrom {0}.wrap.ffi_interop import marshal, {1}, check_exceptions\n# Additional specific imports for this package\n{3}\n\n# Some low-level conversion functions that will be called in the generated code:\ndef char_array_to_py(values:CffiData, dispose:bool=True) -> str:\n    pystring = marshal.c_string_as_py_string(values)\n    if dispose:\n        {1}.DeleteAnsiString(values)\n    return pystring\n\n## etc. etc.\n\n\"\n{0} will be replace with your python module name, {1} with the cffi wrapper interface to invoke the native library, e.g. mylibrary_so. This templated header is then used by a function createPyWrapGen that creates and configures the code generator:\nlet createPyWrapGen (pkgName:string, cffiObjName:string, wrapperClassNames:string, otherImports:string, nativeDisposeFunction:string, typeMap:Dictionary<string, string>) : PythonCffiWrapperGenerator = \n    let pyCffiGen = PythonCffiWrapperGenerator()\nThe body of createPyWrapGen consists mostly of definitions mapping types across languages, C and Python in this case, so that the generator pyCffiGen knows how to convert from Python to C before calling a native C function, or how to convert or wrap a function returned from a C API call. There is provision for custom templates for “peculiar” functions.\n    pyCffiGen.SetTypeMap(\"char*\", \"str\")\n    pyCffiGen.SetReturnedValueConversion(\"char*\", \"char_array_to_py(C_ARGNAME, dispose=True)\")\n    pyCffiGen.SetTransientArgConversion( \"char*\"  , \"_charp\", \"C_ARGNAME = wrap_as_pointer_handle(as_bytes(RCPP_ARGNAME))\", \"# no cleanup for char*?\")\n    let returnscharptrptr = pyCffiGen.ReturnsCharPtrPtrWrapper()\n    pyCffiGen.AddCustomWrapper(returnscharptrptr)\n\n\nBack to the main script\nWe can then call this pyCffiGen function from the main script customised for the uchronia package:\nlet pkgName = \"uchronia\" // what goes into {0}\nlet cffiObjName = \"uchronia_so\"\n\n// higher level package classes can be injected into the generated lower level binding, to make them more intelligible for user.\nlet wrapperClassNames=\"    from uchronia.classes import (\n        EnsembleTimeSeries,\n        TimeSeriesLibrary,\n        EnsembleForecastTimeSeries,\n        TimeSeries,\n        EnsemblePtrTimeSeries,\n        TimeSeriesProvider,\n    )\n\"\n\nlet typeMap = Dictionary<string, string>()\ntypeMap.Add(\"DATATYPES_ENSEMBLE_TIME_SERIES_DOUBLE_PTR\", \"'EnsembleTimeSeries'\")\ntypeMap.Add(\"ENSEMBLE_DATA_SET_PTR\", \"'TimeSeriesLibrary'\")\ntypeMap.Add(\"ENSEMBLE_FORECAST_TIME_SERIES_PTR\", \"'EnsembleForecastTimeSeries'\")\ntypeMap.Add(\"TIME_SERIES_PTR\", \"'TimeSeries'\")\n// etc. etc.\n\nlet otherImports = \"\"\nlet nativeDisposeFunction = \"DisposeSharedPointer_py\"\n\nlet pyCffiGen = createPyWrapGen (pkgName, cffiObjName, wrapperClassNames, otherImports, nativeDisposeFunction, typeMap)\n\n// Specify a marker that identifiers a function as being part of the C API\nlet exportModifierPattern = [|\"DATATYPES_API\"|] \napiFilter.ContainsAny <- exportModifierPattern\n// In this instance, this marker is not part of the C ANSI 99 function, so this is one of the strings to strip out of lines. \napiFilter.ToRemove <- exportModifierPattern \n\nlet gen =  WrapperGenerator(pyCffiGen, apiFilter)\n\n// Path to input C API file \nlet root = srcDir +/ \"datatypes\"\nlet apiHeaderFile = root +/ \"datatypes\" +/ \"include\" +/ \"datatypes\" +/ \"extern_c_api.h\"\nlet outfile = root +/ \"bindings\" +/ \"python\" +/ \"uchronia\" +/ \"uchronia\" +/ \"wrap\" +/ \"uchronia_wrap_generated.py\"\n\ngen.CreateWrapperHeader(apiHeaderFile, outfile)\nThis may seem like a lot of work to generate relatively little code, but trust me this is much preferable for one’s sanity compared to a “bovine”, manual glue code writing and maintenance over the long term."
  },
  {
    "objectID": "posts/2022-07-27-azure-devops-negative-experience.html",
    "href": "posts/2022-07-27-azure-devops-negative-experience.html",
    "title": "A whinge about Azure Devops",
    "section": "",
    "text": "Foreword: You’ll find a minimal working example towards the end of the post, if you want to skip the rants and googling tribulations\nHow was your day?\nToday, Azure Devops took five-ish hours of my life I’ll never get back. Not for the first time. I’ll try to keep expletive out of this post.\nI’ve been using Azure Devops for about a year to set up build and deployment pipelines. Not on a regular basis, but several instances. This week I am trying to finalise a new build pipeline. And I feel like venting some frustration. It may make some other sufferers feel less alone. Earlier this week I had a training webinar on taking on board UX (user experience) techniques in our work, implicitely for the delivery of digital products. This adds irony to my resentment against Microsoft for landing Azure Devops where it stands.\nI don’t have the time or energy to capture in compelling details all the frustrations or puzzlements in this post. After another week more than I would have liked to spend on it, Azure Devops feels obtuse, alienating, unpleasant. I have not tried many other similar offering recently, so I don’t know if the grass is greener over the fence. And I doubt it.\nToday’s tribulations."
  },
  {
    "objectID": "posts/2022-07-27-azure-devops-negative-experience.html#getting-a-pipeline-to-publish-a-package-in-a-project-feed",
    "href": "posts/2022-07-27-azure-devops-negative-experience.html#getting-a-pipeline-to-publish-a-package-in-a-project-feed",
    "title": "A whinge about Azure Devops",
    "section": "Getting a pipeline to publish a package in a project feed",
    "text": "Getting a pipeline to publish a package in a project feed\nI was putting the final step of a long-running build pipeline (C++, R), to publish an artifact using the task UniversalPackages@0 in the azure-pipeline.yml. And I had managed to do that in another pipeline some months ago. After bumping into an issue of my making (need to create a new feed in the project to host these artifacts), I end up with the glorious error message:\n{\"@t\":\"2022-07-26T10:22:42.4886223Z\",\"@m\":\"An error occurred on the service. User 'aaaaaaaa-2efe-46ec-b780-ffffffffffff' lacks permission to complete this action. You need to have 'AddPackage'.\",\"@i\":\"bbbbbbbb\",\"@l\":\"Error\",\"SourceContext\":\"ArtifactTool.Program\",\"UtcTimestamp\":\"2022-07-26 10:22:42.488Z\"}\n“You”? Who? Me? Who is user ‘aaaaaaaa-2efe-46ec-b780-ffffffffffff’? Is it me? I am admin in this organisation, and owner of the pipeline.\nAnd on with the wild goose chase.\nFirst port of call, compare every setting to the other, working pipeline. I notice that the group Project Collection Build Service (my-az-organisation) is not present with a contributor Role. Fix that.\nNope, still the same. (Later, Looking back, I must have missed something in the feed permission list (Collaborator vs. Contributor role))\nHelp! Google.\nThis StackOverflow thread has hints about needing to add a Build Service with a Contributor role. There are also references at needing to go to an “…” button to change an allowed scope, but as of 2022 the user interface has changed. Other posts such as this and this suggest there is no shortage of confusion besides myself.\nIn the Microsoft documentation maze Manage build service account permissions is the wrong place to look at. Configure feed settings is the right place, but despite not being incorrect it is too generic and unhelpful for many users who just want a specific, working recipe for their use case.\nIn the end I managed to find a recipe after setting a minimal example from scratch."
  },
  {
    "objectID": "posts/2022-07-27-azure-devops-negative-experience.html#a-minimal-working-example",
    "href": "posts/2022-07-27-azure-devops-negative-experience.html#a-minimal-working-example",
    "title": "A whinge about Azure Devops",
    "section": "A minimal, working example",
    "text": "A minimal, working example\nSince at least the user interface has changed since the 2019 Stack Overflow posts:\nCaution: these screen captures below are taken from an Azure Devops organisation where the name of the single project in that organisation is the same as the organisation. These are blacked out, but you can see it partly. This stems from my employer’s policies. Not something I can do anything about, but not the best context for explanation. In your case your organisation name and project name may differ, and probably should to avoid confusion.\nFirst, create the new feed in the artifact section. If you already have a feed, I am not sure.\n\nCreate this new feed, setting the scope to the project, which is recommended anyway. This is important for subsequent permission settings: if you choose organisation, this example may not pan out. The visibility option may not matter for this example, but am not sure.\n\nBy default, the permission list created for the new feed is as below. Note that the [project_name] Build Service ([organisation_name]) user or group has the role Collaborator by default. This may be the key stumbling block users trip over.\n\nAs I write, you cannot change the role; you have to remove the [project_name] Build Service ([organisation_name]) user or group and add it again with the Contributor role.\n\nWith that in place, the following pipeline works:\ntrigger:\n- main\n\nresources:\n- repo: self\n\nvariables:\n  tag: '$(Build.BuildId)'\n\n# to get a custom '$(Build.BuildNumber)', and 'r' is a counter reset to 1 every change of the major/minor versions\nname: '0.1.$(Rev:r)'\n\nstages:\n- stage: Build\n  displayName: Build packages\n  jobs:\n  - job: Build\n    displayName: Build packages\n    pool:\n      vmImage: windows-2019\n    steps:\n    - checkout: self\n    - script: |\n        mkdir $(Build.ArtifactStagingDirectory)\\release \n        cd $(Build.ArtifactStagingDirectory)\\release\n        echo \"test\" > blah.txt\n      displayName: 'create mock artifact'\n    - task: UniversalPackages@0\n      displayName: Publish output bundle\n      inputs:\n        command: publish\n        publishDirectory: '$(Build.ArtifactStagingDirectory)\\release'\n        vstsFeedPublish: 'OD222236-DigWaterAndLandscapes/sandpit_win'\n        vstsFeedPackagePublish: 'sandpit_win'\n        versionOption: custom\n        versionPublish: '$(Build.BuildNumber)'\n        packagePublishDescription: 'Test package publication'"
  },
  {
    "objectID": "posts/2022-07-27-azure-devops-negative-experience.html#parting-words",
    "href": "posts/2022-07-27-azure-devops-negative-experience.html#parting-words",
    "title": "A whinge about Azure Devops",
    "section": "Parting words",
    "text": "Parting words\nThis was but one illustration of the inadequate UX with Azure Devops. Looking back on this one, this is a relatively simple and common use case, and after the fact I feel I should have second-guessed the settings. But I and visibly many others have been derailed and confused by the process, trying to find a didactic example to work from by similarity. It seem that the Microsoft documentation draws you into having to absorb a web of overly complicated and abstract concepts that are overwhelming unless you are a full-time devops."
  },
  {
    "objectID": "posts/2022-06-04-conda-packages-conda-forge.html",
    "href": "posts/2022-06-04-conda-packages-conda-forge.html",
    "title": "Submitting your first conda package to conda-forge",
    "section": "",
    "text": "For years I’ve been contributing to and maintaining, at work, a software stack for streamflow forecasting. It is a stack with a core in C++, but accessible via a C API by users from R, Matlab, Python and so on. A whole article could be written about the design rationale, successes and shortcomings of this stack, and the interplay of people, organisations and technologies in using these tools and how. But this will not be this post.\nFocussing on the Python side of things, these streamflow forecasting tools are used mostly on Windows and Linux, the core is deployed as dynamic libraries (.dll or .so) on disk, and python packages access these using cffi for interoperability. The python packages contain solely python code; there is no cython or straight C.\nI’ve come to appreciate (mostly) conda environments for managing software stacks for various projects. This post is a start to test packaging some of “my” software with conda, in the hope this reduces the surprisingly strong impedance, technical but not only, towards usage by a broader audience.\nA bit picture end point would be a corporate equivalent to a conda-forge channel, with the full software stack available for any employee."
  },
  {
    "objectID": "posts/2022-06-04-conda-packages-conda-forge.html#first-trial-conda-build-locally",
    "href": "posts/2022-06-04-conda-packages-conda-forge.html#first-trial-conda-build-locally",
    "title": "Submitting your first conda package to conda-forge",
    "section": "First trial: conda build locally?",
    "text": "First trial: conda build locally?\nOf course python is necessary and a conda environment a given. I do . ~/config/baseconda because I never have conda activated by default from .bashrc.\nI am actually not sure from the conda-build tutorial in which environment I should install conda-build. Let’s try the base environment and see whether we get stuck or not.\nmamba install -c conda-forge conda-build (installing from conda-forge is an idiosyncrasy. I strongly recommend mamba)\nYou should at least skim through the concepts. This is rather dry to read throughly upfront.\nThe tutorial Building conda packages from scratch quickly confused me; I was trying to transpose it to refcount but this does not look like the right template to start from. The section editing-the-meta-yaml-file appears out of sync with the “correct” meta.yaml file. Baffling."
  },
  {
    "objectID": "posts/2022-06-04-conda-packages-conda-forge.html#preparing-a-pr-to-conda-forgestaged-recipes",
    "href": "posts/2022-06-04-conda-packages-conda-forge.html#preparing-a-pr-to-conda-forgestaged-recipes",
    "title": "Submitting your first conda package to conda-forge",
    "section": "Preparing a PR to conda-forge/staged-recipes",
    "text": "Preparing a PR to conda-forge/staged-recipes\nEnter two new resources: 3 simple stept to build a python package for conda-forge and conda-forge documentation: Contributing packages. From these it becomes clear I should use grayskull to get a starting point as a meta.yaml file\ncd ~/src\ngit clone --depth 1 git@github.com:jmp75/staged-recipes.git\ncd ~/src/staged-recipes/recipes\ngit remote add upstream git@github.com:conda-forge/staged-recipes.git\nmamba install -c conda-forge grayskull\ngrayskull pypi refcount\n#### Initializing recipe for refcount (pypi) ####\n\nRecovering metadata from pypi...\nStarting the download of the sdist package refcount\nrefcount 100% Time:  0:00:00  15.3 MiB/s|###############################################################################################################################################################################################################################################|\nRecovering information from setup.py\nExecuting injected distutils...\nRecovering metadata from setup.cfg\nNo data was recovered from setup.py. Forcing to execute the setup.py as script\nRecovering metadata from setup.cfg\nChecking >> cffi 100% |##########################################################################################################################################################################################################################################|[Elapsed Time: 0:00:00]\nMatching license file with database from Grayskull...\nMatch percentage of the license is 59%. Low match percentage could mean that the license was modified.\nLicense type: BSD-3-Clause\nLicense file: LICENSE.txt\nHost requirements:\n  - pip\n  - python\n\nRun requirements:\n  - cffi\n  - python\n\nRED: Missing packages\nGREEN: Packages available on conda-forge\n\nMaintainers:\n   - j-m\n\n#### Recipe generated on /home/abcdef/src/staged-recipes/recipes for refcount ####\nThe output meta.yaml (which is actually a ninja template file), is a good start, however you should revise it a bit rather than accept wholesale\nMostly fine, however this did not pick up a requirement cffi >=1.11.5, and second guessing from reading this gallon.me post, a minimum python version is necessary to get accepted.\nrequirements:\n  host:\n    - pip\n    - python >=3.6\n  run:\n    - cffi >=1.11.5\n    - python >=3.6\nPerhaps optional, remove a hard-coded package string “refcount”in the source url section.\nIt is instructive to look at the existing pull requests on staged-recipes. Notably I realise that the github ID extracted by grayskull is not the correct one; I am not the ID j-m, unfortunately (could have been judging by history length).\nextra:\n  recipe-maintainers:\n    - j-m\nextra:\n  recipe-maintainers:\n    - jmp75\nThe end result should be something like:\n\n\n{% set name = \"refcount\" %}\n{% set version = \"0.9.3\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.zip\n  sha256: bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442\n\nbuild:\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n  number: 0\n\nrequirements:\n  host:\n    - pip\n    - python >=3.6\n  run:\n    - cffi >=1.11.5\n    - python >=3.6\n\ntest:\n  imports:\n    - refcount\n  commands:\n    - pip check\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/csiro-hydroinformatics/pyrefcount\n  summary: A Python package for reference counting and interop with native pointers\n  description: |\n    This package helps you achieve reliable management of memory \n    allocated in native libraries, written for instance in C++. While \n    it boils down to \"simply\" maintaining a set of counters, \n    it is deceptively complicated to do so properly and not end up \n    with memory leaks or crashes.\n    <https://pyrefcount.readthedocs.io/en/latest/>.\n  dev_url: https://github.com/csiro-hydroinformatics/pyrefcount\n  license: BSD-3-Clause\n  license_family: BSD\n  license_file: LICENSE.txt\n\nextra:\n  recipe-maintainers:\n    - jmp75\nSo; ready to submit a pull request? Wait, wait."
  },
  {
    "objectID": "posts/2022-06-04-conda-packages-conda-forge.html#building-locally",
    "href": "posts/2022-06-04-conda-packages-conda-forge.html#building-locally",
    "title": "Submitting your first conda package to conda-forge",
    "section": "Building locally",
    "text": "Building locally\ntest has a section on Running unit tests. Note that the default conda recipe above has a “pip check”, but nothing more. refcount unit tests use pytest, and has unit tests; tick that. refcount is a pure python package, but it is a package for (mostly) interoperability with native code via a C API. Unit tests do contain some c/c++ code.\nShould the recipe run fine upon submission, including unit tests? Before submitting a pull request that may trigger a failed check, let’s experiment with staging tests locally.\nso:\ncd ~/src/staged-recipes\npython ./build-locally.py linux64\n  File \"/home/abcdef/miniconda/lib/python3.9/subprocess.py\", line 373, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['.scripts/run_docker_build.sh']' returned non-zero exit status 1.\nI tried to conda install -c conda-forge shyaml which seems to be used by the scripts, but this did not alleviate the issue.\nThat took me some time to find a workaround to this one. The “exit status 1” is actually very misleading. The root cause is a docker run -it that exited with an error code 139. I seem to not be the only one to have bumped into this issue still open. I may have pointed to the workaround in the conda-forge FAQ.\nI needed to override the default docker image build-locally.py falls back to with:\nexport DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64\npython build-locally.py linux64\nthe build script works this time, but at some point:\nProcessing $SRC_DIR\n  Added file://$SRC_DIR to build tracker '/tmp/pip-build-tracker-bkn7ckr9'\n  Running setup.py (path:$SRC_DIR/setup.py) egg_info for package from file://$SRC_DIR\n  Created temporary directory: /tmp/pip-pip-egg-info-68li1y6t\n  Preparing metadata (setup.py): started\n  Running command python setup.py egg_info\n  Traceback (most recent call last):\n    File \"<string>\", line 2, in <module>\n    File \"<pip-setuptools-caller>\", line 34, in <module>\n    File \"/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/setup.py\", line 41, in <module>\n      with open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n    File \"/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pl/lib/python3.10/codecs.py\", line 905, in open\n      file = builtins.open(filename, mode, buffering)\n  FileNotFoundError: [Errno 2] No such file or directory: '/home/conda/staged-recipes/build_artifacts/refcount_1654312313810/work/README.md'\n  error: subprocess-exited-with-error\n  \n  × python setup.py egg_info did not run successfully.\n  │ exit code: 1\n  ╰─> See above for output.\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n\nThis is an issue that may be in my control.\ncd ~/src/staged-recipes/build_artifacts/refcount_1654312313810/work\nls\n## build_env_setup.sh  conda_build.sh  LICENSE.txt  MANIFEST.in  metadata_conda_debug.yaml  PKG-INFO  README.rst  refcount  refcount.egg-info  setup.cfg  setup.py\nrefcount has both a README.md and README.rst, the latter being an export from the former because pypi requires (or used to require) a README.rst to display correctly. The zip archive of the source code on pypi indeed does not have the README.md file included.\nI’ve inherited the practice to use in the packages setup.py the following, to limit redundances.\nwith open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n    long_description_content_type='text/markdown'\nPreviously I needed to convert on the fly to restructured Text, but Markdown is more supported. Still there is a lot of inertia with restructuredText.\nI may try to just nuke the README.rst. The only fly on the ointment is: is pypi ok with rendering markdown correctly these days? Probably; the packaging documentation is using README.md by default.\nSo, build and submit to pypi the updated refcount 0.9.4 with no README.rst. Looks fine, including the zip source archive.\nRuntimeError: SHA256 mismatch: '21567918cb1bb30bf8116ce3483d3f431de202618eabbc6887b4814b40a3b94a' != 'bf8bfabdac6f0d9fe3734f1c1830fda8b9b2d740c90ecf8caf8c2ef3ed9c8442'\nTraceback (most recent call last):\nRight, I forgot to change the checksum in the meta.yaml file.\nAnd… it seems to complete.\nimport: 'refcount'\n+ pip check\nNo broken requirements found.\n+ exit 0\n\nResource usage statistics from testing refcount:\n   Process count: 1\n   CPU time: Sys=0:00:00.0, User=-\n   Memory: 3.0M\n   Disk usage: 28B\n   Time elapsed: 0:00:02.1\n\nTEST END: /home/conda/staged-recipes/build_artifacts/noarch/refcount-0.9.4-pyhd8ed1ab_0.tar.bz2\nSubmit the pull request, and pleasantly:\n\n\n\n\n\nrefcount-conda-forge-pr-submission"
  },
  {
    "objectID": "posts/20221007-nbdev-windows/index.html",
    "href": "posts/20221007-nbdev-windows/index.html",
    "title": "Using nbdev on Windows",
    "section": "",
    "text": "nbdev is a system to “write, test, document, and distribute software packages and technical articles — all in one place, your notebook.”. I was aware of it for a while, but saw more of it while attending the Practical Deep Learning course a few months ago. I have a set of habits to develop python software, of course, and jupyter notebooks are part of what I use, but not to develop packages. It is good to trial other ways of working though, and I am in the midst of using nbdev “in anger” in one of my projects.\nI work mostly from Linux, but tested nbdev on Windows prior to suggesting my colleagues to give it a try. I reported this nbdev issue (which is too vague and premature, admitedly). As of Sept 2022 nbdev is not fully supported on Windows though, and clearly stated in the nbdev documentation at that time: “nbdev works on macOS, Linux, and most Unix-style operating systems. It works on Windows under WSL, but not under cmd or Powershell”.\nIn this post we will nevertheless try and see what does not work natively on Windows, with a view to assess whether I can contribute to nbdev for support on Windows natively."
  },
  {
    "objectID": "posts/20221007-nbdev-windows/index.html#creating-and-previewing-qith-quarto",
    "href": "posts/20221007-nbdev-windows/index.html#creating-and-previewing-qith-quarto",
    "title": "Using nbdev on Windows",
    "section": "Creating and previewing qith Quarto",
    "text": "Creating and previewing qith Quarto\nInitially where quarto returns C:\\Program Files\\RStudio\\bin\\quarto\\bin\\quarto.cmd which I inherited from my RStudio installation from my IT department. quarto --help returns:\n  Usage:   quarto\n  Version: 0.9.649\nI have Version: 1.1.165 on Linux, and suspect this is preferable to have the latest quarto. With quarto 0.9.649 nbdev_preview returns:\nTraceback (most recent call last):\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\Scripts\\nbdev_preview.exe\\__main__.py\", line 7, in <module>\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\site-packages\\fastcore\\script.py\", line 119, in _f\n    return tfunc(**merge(args, args_from_prog(func, xtra)))\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\site-packages\\nbdev\\quarto.py\", line 283, in nbdev_preview\n    with fs_watchdog(_f, path): subprocess.run(['quarto','preview']+xtra)\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\subprocess.py\", line 505, in run\n    with Popen(*popenargs, **kwargs) as process:\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\subprocess.py\", line 951, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\abcdef\\Miniconda3\\envs\\bm\\lib\\subprocess.py\", line 1420, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nFileNotFoundError: [WinError 2] The system cannot find the file specified\nIt may or may not be a quarto version issue."
  },
  {
    "objectID": "posts/20221007-nbdev-windows/index.html#upgrade-quarto",
    "href": "posts/20221007-nbdev-windows/index.html#upgrade-quarto",
    "title": "Using nbdev on Windows",
    "section": "Upgrade Quarto",
    "text": "Upgrade Quarto\nAfter installing the lates quarto, which nicely can be installed with Admin priviledges:\nset PATH=C:\\Users\\abcdef\\AppData\\Local\\Programs\\Quarto\\bin;%path%` \nquarto --version\n# returns 1.1.251\ncd C:\\Users\\abcdef\\src\\bmfb\nnbdev_preview\nAnd the package documentation seems to render fine…"
  },
  {
    "objectID": "posts/2022-06-01-lithology-classification-hugging-face.html",
    "href": "posts/2022-06-01-lithology-classification-hugging-face.html",
    "title": "Lithology classification using Hugging Face, part 1",
    "section": "",
    "text": "This is (perhaps) the start of a series of posts on using natural language processing for lithology classification. I hope to explore multi-label classification in subsequent posts.\n\n\n\nCourtesy of https://earthresources.vic.gov.au"
  },
  {
    "objectID": "posts/2022-06-01-lithology-classification-hugging-face.html#major-primary-lithology-codes",
    "href": "posts/2022-06-01-lithology-classification-hugging-face.html#major-primary-lithology-codes",
    "title": "Lithology classification using Hugging Face, part 1",
    "section": "Major (primary) lithology codes",
    "text": "Major (primary) lithology codes\n\nlitho_classes=litho_logs[MAJOR_CODE].values\ndf_most_common= token_freq(litho_classes, 50)\n\n\nplot_freq(df_most_common)\n\n<AxesSubplot:xlabel='token'>\n\n\n\n\n\nThis is a long-tailed distribution; quite a few labels. The limit to 4 characters labels suggest a form of controlled vocabulary. The numbers 20, 19, 23, 8, 1 look odd. “None” is an artefact though not misleading, quite a few records are such that no major lithology could be attributed."
  },
  {
    "objectID": "posts/2022-06-01-lithology-classification-hugging-face.html#minor-secondary-lithology-codes",
    "href": "posts/2022-06-01-lithology-classification-hugging-face.html#minor-secondary-lithology-codes",
    "title": "Lithology classification using Hugging Face, part 1",
    "section": "Minor (secondary) lithology codes",
    "text": "Minor (secondary) lithology codes\n\nlitho_classes=litho_logs[MINOR_CODE].values\ndf_most_common= token_freq(litho_classes, 50)\n\n\nplot_freq(df_most_common)\n\n<AxesSubplot:xlabel='token'>\n\n\n\n\n\nWell, no minor lithology codes, so this data set may not be a good start for multi-label classification. Still, I’ll persist with this data set, and reassess later on"
  },
  {
    "objectID": "posts/2022-06-01-lithology-classification-hugging-face.html#back-to-major-lithology-codes",
    "href": "posts/2022-06-01-lithology-classification-hugging-face.html#back-to-major-lithology-codes",
    "title": "Lithology classification using Hugging Face, part 1",
    "section": "Back to major lithology codes",
    "text": "Back to major lithology codes\nWhat are the flavour of descriptions leading to the most frequent classes (CLAY, GRVL, etc.), as well as “None”\n\nis_clay = litho_logs[MAJOR_CODE] == 'CLAY'\n\n\nclay_coded = litho_logs.loc[is_clay][DESC]\n\n\nimport numpy as np\nnp.random.seed(123)\nclay_coded.sample(n=50)\n\n144320                                                 CLAY\n117387                                                 CLAY\n18630                                                  CLAY\n35565                                       CLAY SOME SANDY\n61997                                                  CLAY\n13533                                  CLAY SANDSTONE BANDS\n117529                                          YELLOW CLAY\n82290                                                  CLAY\n73280                                       CLAY GREY SANDY\n106748                                          CLAY STONEY\n26265                                           CLAY YELLOW\n16456                                                  CLAY\n25032                                                  CLAY\n9911                                                   CLAY\n135852    CLAY, SANDY GRAVELLY, ORANGE BROWN, MEDIUM PLA...\n30596                                                  CLAY\n140698    CLAY, SANDY FAT; RED, DRY-MOIST, MEDIUM CONSIS...\n70267                                                  CLAY\n6534                                            CLAY YELLOW\n69927                                           CLAY GRAVEL\n76476                                            SANDY CLAY\n7329                                            CLAY GRITTY\n51685        CLAY; 40% BROWN, GRAVEL & SAND 60%, MOST 1-3MM\n137385                                                 CLAY\n35816                                                  CLAY\n73972                                                  CLAY\n102333                                     CLAY SOME STONES\n29218                                                  CLAY\n92266                                              RED CLAY\n26151                                            CLAY SANDY\n107456                                            CLAY GREY\n52550     CLAY - LIGHT GREY, EXTREMELY SANDY (FINE TO CO...\n24225                                                  CLAY\n120696                     CLAY; LIGHT GREY, TRACE OF WHITE\n9030                                                   CLAY\n14889                                                  CLAY\n25834                           CLAY LIGHT BROWN GREY SILTY\n44078                                                  CLAY\n114869                                                 CLAY\n109302                                                 CLAY\n38107                                           CLAY PATCHY\n30801                                                  CLAY\n82362                                     CLAY/BROWN ORANGE\n141490                         CLAY; BROWN, PINK, SOME BLUE\n23075                                     CLAY SANDY GRAVEL\n42110                                                  CLAY\n50493                                 CLAY, WHITE SEAM, RED\n90209                                     CLAY - DARK BROWN\n104019                                                 CLAY\n12690                                                  CLAY\nName: Description, dtype: object\n\n\nNothing too surprising in this, very intuitive, though the tail suggests there may be a few outliers:\n\nclay_coded.tail(10)\n\n144386            CLAY; LIGHT BROWN, SOME LIGHT GREY, SILTY\n144388                                          CLAY; BROWN\n144391                                    CLAY; LIGHT BROWN\n144393                                         CLAY - BROWN\n144394         CLAY - LIGHT BROWN, EXTREMELY SANDY (COARSE)\n144398    CONGLOMERATE - WEATHERED BUT STILL HOLDS TOGET...\n144401                                          CLAY; BROWN\n144402                                          CLAY - GREY\n144488                                                 CLAY\n144503                                                 None\nName: Description, dtype: object\n\n\nLooking at the “sand” code:\n\ndef sample_desc_for_code(major_code, n=50, seed=None):\n    is_code = litho_logs[MAJOR_CODE] == major_code\n    coded = litho_logs.loc[is_code][DESC]\n    if seed is not None:\n        np.random.seed(seed)\n    return coded.sample(n=50)\n\n\nsample_desc_for_code('SAND', seed=123)\n\n72256                         SAND GRAVEL FINE WATER SUPPLY\n109345                                   SAND CLAYEY GRAVEL\n66816                                     SAND WATER SUPPLY\n85097     SAND - 70% UP TO 2MM, GRAVEL 10% 2-5MM, CLAY 2...\n137476    SAND; LIGHT BROWN, MEDIUM TO COARSE, FINE GRAV...\n29946                                     SAND GRAVEL DIRTY\n37012                                COARSE SAND AND GRAVEL\n135985                                                 SAND\n74402              SAND WHITE GREY WELL SORTED WATER SUPPLY\n42422                                           SAND GRAVEL\n41225                                    SAND CLAYEY GRAVEL\n50700                                                  SAND\n33813                      BROWN SAND (VERY FINE TO MEDIUM)\n38452                       SAND GRAVEL MEDIUM WATER SUPPLY\n107707                                     SAND FINE-COARSE\n51759     SAND; 60%, SILTY, FINE 10%, MEDIUM 20% & COARS...\n102234                             SAND GRAVEL WATER SUPPLY\n74640                              SAND GRAVEL WATER SUPPLY\n13123                              SAND GRAVEL WATER SUPPLY\n23110                                                  SAND\n11373                              SAND GRAVEL WATER SUPPLY\n35508                        SAND SILTY GRAVEL WATER SUPPLY\n32744                                           SAND CLAYEY\n67310                                             SAND CLAY\n41813                              SAND GRAVEL WATER SUPPLY\n106457                             SAND GRAVEL WATER SUPPLY\n85591                    SAND AND GRAVEL - CLAYEY AND SILTY\n112156                         SAND, WHITE (VERY VERY FINE)\n111256                   SAND/GRAVEL; FINE TO COARSE, BROWN\n55808     SAND; POORLY GRADED, COARSE, SUBROUNDED, LIGHT...\n91458     SAND, MEDIUM-COARSE; SLIGHTLY SILTY, WET, LOOS...\n61608                              SAND, GRAVEL & LIMESTONE\n96382                                           SAND CLAYEY\n37119          MEDIUM BROWN SAND & GRAVEL SOME LARGE GRAVEL\n118473    SAND; 40% 1/20-1/2MM, 40% 1/2-1MM, 20% 1-3MM, ...\n70703                              SAND GRAVEL WATER SUPPLY\n92920                                            BROWN SAND\n80551     SAND, SILTY; AS ABOVE, TRACE CLAY, ORANGE, LOW...\n25175     COARSE SAND & FINE GRAVEL WITH ANGULAR FRAGMEN...\n38905                               SAND FINE WATER BEARING\n44248                              SAND GRAVEL WATER SUPPLY\n32261                                            SAND DRIFT\n34510                             SAND CLAYEY COARSE GRAVEL\n77431                   SAND AND GRAVEL FINE TO COARSE GREY\n138362                                           SAND BROWN\n103758                   SAND GRAVEL STONES SOME CLAY BANDS\n69238                                           SAND GRAVEL\n143566    SAND & GRAVEL; SAND 50% (FINE 30%, MEDIUM 20%,...\n135131                                                 SAND\n30361                               SAND CLAYEY WELL SORTED\nName: Description, dtype: object\n\n\nRather straighforward, consistent and and intuitive\n\nMajor lithology code “None”\nThis one is likely to be more curly and surprising. Let’s see\n\nsample_desc_for_code('None', seed=123)\n\n131075                            W.B. BROWN SHALE\n127170                      COARSE SAND AND GRAVEL\n126145                                   BLUE CLAY\n126889                         GREY AND BROWN CLAY\n132179                             GREY BROWN CLAY\n130033                            BROWN SANDY CLAY\n123436                                      GRAVEL\n128794                                        CLAY\n128242                        SAND AND FINE GRAVEL\n134156                                        None\n129636                                 W.B. BASALT\n122884                     LIGHT ORANGE SILTY CLAY\n124580                                      SHALES\n128684                                  W.B. SHALE\n131120                             SAND AND GRAVEL\n122690                              RED RIDGE CLAY\n132319                                  BLACK SOIL\n128528                                  BLACK CLAY\n123646                                    MUDSTONE\n129757                                 SAND & CLAY\n124353                             SOFT BROWN CLAY\n131921                                        CLAY\n133208                                 BROWN CLAYS\n132945                                 COARSE SAND\n122956                             SAND AND GRAVEL\n123723                             CALY AND GRAVEL\n122102                                    TOP SOIL\n126455    HARD SANDY BROWN & GREY CLAY WITH STONES\n125637                    SANDY BLUE AND GREY CLAY\n131154                            BROWN SANDY CLAY\n129931                                  RIDGE CLAY\n123557                                  RIDGE CLAY\n124964                             DARK BROWN CLAY\n129391                                  GREY SHALE\n132891                               WATER 0.37 LS\n132006                                  BROWN CLAY\n134673                                         SEP\n123692                                  BROWN CLAY\n126034                          REDDISH BROWN CLAY\n129690             SOFT GINGER/BROWN CLAY & STONES\n125560                            FINE SANDY CLAYS\n126238                             SHALE & (WATER)\n123009                                        SOIL\n124236                                        SOIL\n125814                                  BLUE SHALE\n130474                             RED & GREY CLAY\n127005                                  BROWN CLAY\n134212                                   BROWN AND\n125640                                        SOIL\n127355                                       SHALE\nName: Description, dtype: object\n\n\nWell, it does not require a fully trained geologist to think there should be obvious primary lithology codes for many of these, so why is there “None” for these descriptions?\nNot shown in the 50-odd sample above are descriptions which should indeed be unclassified (e.g. “no strata description”)\nThis is also an occasion to note the less obvious information and complications in descriptive logs, compared to earlier categories:\n\nCALY as a typo for CLAY\nSlang terms and acronyms, e.g. “W.B.” perhaps for “Weathered, broken”\n\nQuantitative and qualitative attributes such as SAND; 60%, SILTY, FINE 10%, MEDIUM 20% & COARSE, which may be valuable for some types of classification\n\n\nsample_desc_for_code('SDSN', seed=123)\n\n9423                                         SANDSTONE SOFT\n117911            SANDSTONE, SILTY; AS ABOVE, QUARTZ (+60%)\n6613                                         SAND ROCK GREY\n20345                                   SANDSTONE GREY HARD\n106885                                            SANDSTONE\n139088           SANDSTONE; DARK GREY, COARSE GRAINED, HARD\n5101                                       SANDSTONE ROTTEN\n26561                                      SANDSTONE YELLOW\n137304                                    SANDSTONE; COARSE\n56627     SANDSTONE; GREY, FINE-MEDIUM GRAINED, MOD WEAK...\n98974                          SANDSTONE WHITE WATER SUPPLY\n87187                                             SANDSTONE\n45141                 SANDSTONE, VERY PALE BROWN, HIGH CLAY\n70015                                      SANDSTONE YELLOW\n29492                                SANDSTONE WATER SUPPLY\n92918                                             SANDSTONE\n83555              SANDSTONE; LIGHT GREY, VERY FINE, STRONG\n136401                                            SANDSTONE\n48201                                             SANDSTONE\n34060     SANDSTONE, YELLOW, MEDIUM GRAINED, HIGH CLAY M...\n63752     SANDSTONE; WHITISH GREY, VERY FINE, ANGULAR, C...\n51878                     SANDSTONE, MEDIUM, WHITISH YELLOW\n59346                                       SANDSTONE, SOFT\n62968          SANDSTONE; OFF-WHITE GREY, VERY FINE, STRONG\n140930                SANDSTONE, LIGHT GREY, SOME FRACTURES\n6394                                              SAND ROCK\n113546                        SANDSTONE - HARD - LIGHT GREY\n49539     SANDSTONE; RED GREY, FINE-MEDIUM GRAINED, FRIA...\n60198                   SANDSTONE; LIGHT GREY, FINE GRAINED\n1366                                              SANDSTONE\n35923               SANDSTONE WEATHERED COARSE WATER SUPPLY\n141556    SANDSTONE, WHITE, IMPREGNATED WITH WATER WASHE...\n10100                                SANDSTONE WATER SUPPLY\n89138     SANDSTONE; OFF-WHITE GREY, MEDIUM-COARSE GRAIN...\n86046                      QUARTZ SANDSTONE; MEDIUM GRAINED\n76366     SANDSTONE, PALE BROWN, FINE-COARSE, LOW CLAY, ...\n46179     SANDSTONE, LIGHTYELLOW/RED BROWN MOTTLED, FINE...\n8613                                              SANDSTONE\n4537                                       SANDSTONE YELLOW\n60506                                             SANDSTONE\n8917                                              SANDSTONE\n1372                                 SANDSTONE WATER SUPPLY\n82975     SANDSTONE; LIGHT GREY, FINE GRAINED, MOD STRON...\n60166                                  SANDSTONE, VERY SOFT\n31674                                       SANDSTONE SHALE\n8397                                              SANDSTONE\n113310       SANDSTONE; OFF-WHITE, FINE GRAINED, MOD STRONG\n27980                                             SANDSTONE\n79933                                SANDSTONE WHITE BROKEN\n33654                               SANDSTONE YELLOW CLAYEY\nName: Description, dtype: object\n\n\nNow, what’s with the weird numbers as lithology codes?\n\nsample_desc_for_code('20', seed=123)\n\n130369                           SANDY CLAY; BROWN (COARSE)\n122444                                           CLAY SANDY\n127761                                           CLAY SANDY\n125771                                           SANDY CLAY\n127783                                           CLAY SANDY\n133014                SANDY CLAY AND CLAYEY SAND AND GRAVEL\n124007                              BROWN & GREY SANDY CLAY\n123445     CLAY LT BROWN GREY SANDY, MED-COARSE BROWN BANDS\n122180                                           CLAY SANDY\n127440                                           SANDY CLAY\n131498                                           SANDY CLAY\n127935                                       CLAY RED SANDY\n129304                                           SANDY CLAY\n130614                                          SANDY SHALE\n125543                                           SANDY CLAY\n125149                                           SANDY CLAY\n124971                                SANDY CLAY/GREY BROWN\n123939                                    SANDY CLAY, BROWN\n122392                                           CLAY SANDY\n123999                              BROWN & GREY SANDY CLAY\n129203                                           SANDY LOAM\n127933                                       CLAY RED SANDY\n134474                              SANDY CLAYEY MED. BROWN\n126071                                           SANDY CLAY\n127927                                           CLAY SANDY\n123587                                           CLAY SANDY\n126559                                           SANDY CLAY\n127928                                      CLAY GREY SANDY\n124643                                     SANDY CALY/BROWN\n122555                                           CLAY SANDY\n132628                                           SANDY CLAY\n129853                                           SANDY CLAY\n130187                                           SANDY CLAY\n129057                                           SANDY CLAY\n133300                                           SANDY CLAY\n122535                                           CLAY SANDY\n122732                                           CLAY SANDY\n127977                                    CLAY YELLOW SANDY\n126568                                           SANDY CLAY\n122339                                           CLAY SANDY\n128386                                    SANDY CLAY, BROWN\n128198                                    SANDY CLAY, RIDGE\n125530                                           SANDY CLAY\n126798                                           SANDY CLAY\n122452                                           CLAY SANDY\n123237                                    SANDY BROWN CLAYS\n126510    SANDY CLAY; 40% LIGHT GREY, EXTREMELY SANDY (C...\n132331                                         SANDY GRAVEL\n131807                                           SANDY CLAY\n132895                                           SANDY CLAY\nName: Description, dtype: object\n\n\nInteresting. There is a clear pattern. I know from my prior exposure that “Clayey sands” and “sandy clays” are not that uncommon (and gradations of mixes of sand and clay matter a great deal to estimate hydraulic conductivity)."
  },
  {
    "objectID": "posts/2022-06-01-lithology-classification-hugging-face.html#next",
    "href": "posts/2022-06-01-lithology-classification-hugging-face.html#next",
    "title": "Lithology classification using Hugging Face, part 1",
    "section": "Next",
    "text": "Next\nThis was the initial EDA. Next I’ll probably train a classifier on the major lithology code (or a subset thereof). I am keen to explore multi-label classification, but will have to decide whether to populate the secondary lithology code using regexp classification, or switch to a fully labelled dataset at some point.\nThis first post illustrated the need to have a look at data. This data was already collated and curated, and I have no doubt many people went through a lot of work to get there. But this may not be a fully labelled dataset amenable to be used for training a classifier. At least, not without further data preparation."
  },
  {
    "objectID": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html",
    "href": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html",
    "title": "Running a Windows VM with Vagrant and qemu",
    "section": "",
    "text": "(Logos are"
  },
  {
    "objectID": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#tutorials-and-related-work",
    "href": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#tutorials-and-related-work",
    "title": "Running a Windows VM with Vagrant and qemu",
    "section": "Tutorials and related work",
    "text": "Tutorials and related work\nSome related publications gleaned through the process.\nYou should at least scan Vagrant and the vagrant tutorials to get started if you are, like me, a late newcomer to Vagrant.\nDebian Wiki has an entry on Vagrant. It does list the known issue I bumped into (“Waiting for domain to get an IP address…”).\nThere is no shortage of blog posts for instance on medium.com on setting up Vagrant (some are subscriber only), but most deal with standing up Linux on top of Linux or MacOS via Vagrant, not Windows images. They were useful, but not spot on the need I have.\nA very nice, didactic resource I found was this note by Nicolas Iooss, probably the most relevant to the subject of the present post. A couple of commands in that note appear outdated, but they are minor.\nOther resources that brought partial information, or may in the future:\n\nHow To Use Vagrant With Libvirt KVM Provider\ngithub issue waiting for domain to get ip"
  },
  {
    "objectID": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#installing-prerequisites",
    "href": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#installing-prerequisites",
    "title": "Running a Windows VM with Vagrant and qemu",
    "section": "Installing prerequisites",
    "text": "Installing prerequisites\nOn Linux Debian I started with sudo apt install vagrant vagrant-lxc vagrant-libvirt. There are several vagrant-* plugins in Debian. I probably should have installed most of them. Vagrant can also install them from internet sources later, which I did for vagrant-mutate for instance with vagrant plugin install.\nwhich kvm indicates kvm is already installed, not sure from which Debian package. quemu-* commands are present."
  },
  {
    "objectID": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#windows-test-image",
    "href": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#windows-test-image",
    "title": "Running a Windows VM with Vagrant and qemu",
    "section": "Windows test image",
    "text": "Windows test image\nGetting from Microsoft download site a Win7 32bits image for Vagrant:\ncd ~/tmp/vm\nmv ~/Downloads/IE11.Win7.Vagrant.zip ./\nunzip IE11.Win7.Vagrant.zip \nls -l\n-rw-r--r-- 1 xxxyyy xxxyyy 4989880652 Sep 23  2015 'IE11 - Win7.box'\n-rw-r--r-- 1 xxxyyy xxxyyy 4989880932 Nov 22 11:08  IE11.Win7.Vagrant.zip\nI dislike spaces in filenames so mv IE11\\ -\\ Win7.box Win7_32.box"
  },
  {
    "objectID": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#creating-the-vagrant-box",
    "href": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#creating-the-vagrant-box",
    "title": "Running a Windows VM with Vagrant and qemu",
    "section": "Creating the Vagrant box",
    "text": "Creating the Vagrant box\nvagrant box add Win7-32 file:///${HOME}/tmp/vm/Win7_32.box\n==> box: Successfully added box 'Win7-32' (v0) for 'virtualbox'!\nxxxyyy@keywest-bm:~/tmp/vm$ vagrant box list\nWin7-32 (virtualbox, 0)\nFrom a vagrant tutorial:\n\nVagrant uses a base image to quickly clone a virtual machine. These base images are known as “boxes” in Vagrant, and specifying the box to use for your Vagrant environment is always the first step after creating a new Vagrantfile.\n\nWhile you can create a Vagrantfile from scratch, vagrant init Win7-32 creates a stub with defaults."
  },
  {
    "objectID": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#the-path-that-worked-vagrant-and-virtualbox",
    "href": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#the-path-that-worked-vagrant-and-virtualbox",
    "title": "Running a Windows VM with Vagrant and qemu",
    "section": "The path that worked: Vagrant and VirtualBox",
    "text": "The path that worked: Vagrant and VirtualBox\nI installed VirtualBox following some of the download instructions from virtualbox.org. There are several ways for Debian. virtualbox is not in the main repo, but available via a fast-track one. In the end, adding the external repository from Oracle in the “Debian-based Linux distributions” section seemed easier:\nsudo nano /etc/apt/sources.list\n# add additional repo\nwget -O- https://www.virtualbox.org/download/oracle_vbox_2016.asc | sudo gpg --dearmor --yes --output /usr/share/keyrings/oracle-virtualbox-2016.gpg\nmore /usr/share/keyrings/oracle-virtualbox-2016.gpg\nsudo apt-get update\nsudo apt-get install virtualbox-6.1\nRestart xterm and co to get changes to PATH env var, perhaps.\nAfter clearing up previous tests with vagrant box remove ... re-add the box, which will default to virtualbox in format:\nvagrant box add Win7_32_vb file:///${HOME}/tmp/vm/Win7_32.box\nGet a Vagrant file with vagrant init Win7_32_vb\nThen vagrant up --provider virtualbox mostly seems to works, but:\n    default: SSH address: 127.0.0.1:2222\n    default: SSH username: vagrant\n    default: SSH auth method: private key\n    default: Warning: Authentication failure. Retrying...\n    default: Warning: Authentication failure. Retrying...\nI forgot to add the following to the Vagrantfile:\n# Configure remote access\nconfig.ssh.username = \"IEUser\"\nconfig.ssh.password = \"Passw0rd!\"\n\n# Use 2 CPU and 4GB of RAM\nconfig.vm.provider :virtualbox do |v|\n  v.cpus = 2\n  v.memory = 4096\nend\nThen vagrant up  --provider virtualbox works and starting the VirtualBox GUI shows the running machine:"
  },
  {
    "objectID": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#the-path-id-like-to-work-vagrant-kvm-and-qemu",
    "href": "posts/20221122-win-vagrant-qemu-kvm--virtualbox/index.html#the-path-id-like-to-work-vagrant-kvm-and-qemu",
    "title": "Running a Windows VM with Vagrant and qemu",
    "section": "The path I’d like to work: Vagrant, KVM and Qemu",
    "text": "The path I’d like to work: Vagrant, KVM and Qemu\n\n\n\n\n\n\nWarning\n\n\n\nThis section contains a rough log of steps that did not eventuate as a working solution. Capture for my future self if I revisit this.\n\n\nvagrant up --provider=kvm\nThe provider 'kvm' could not be found, but was requested to\nback the machine 'default'. Please use a provider that exists.\n\nVagrant knows about the following providers: docker, hyperv, virtualbox, libvirt, lxc\nMuh, don’t know. Maybe:\nvagrant up --provider=lxc\nBringing machine 'default' up with 'lxc' provider...\n==> default: Box 'Win7-32' could not be found. Attempting to find and install...\n    default: Box Provider: lxc\n    default: Box Version: >= 0\n==> default: Box file was not detected as metadata. Adding it directly...\n==> default: Adding box 'Win7-32' (v0) for provider: lxc\n    default: Downloading: Win7-32\nAn error occurred while downloading the remote file. The error\nmessage, if any, is reproduced below. Please fix this error and try\nagain.\n\nCouldn't open file /home/xxxyyy/tmp/vm/Win7-32\nOK, fair enough, the box is for provider virtualbox by default. I need to mutate to something else. libvirt, it seems, inferring frin (REF?).\nClean up with vagrant box remove Win7-32, and restart the process.\nvagrant box add windows/Win7-32  Win7_32.box\nvagrant plugin install vagrant-mutate\nvagrant mutate windows/Win7-32 libvirt\nvagrant box remove --provider virtualbox windows/Win7-32\nvagrant init windows/Win7-32 \nvagrant up --provider libvirt --no-destroy-on-error\n==> default: Creating shared folders metadata...\n==> default: Starting domain.\n==> default: Waiting for domain to get an IP address...\nTaking cues from a libvirt github issue on “waiting for domain to get ip”\nsudo apt install vagrant-libvirt libvirt-daemon-system\nsudo usermod --append --groups libvirt $USER\nlog out and back in to have this update effective\ngroups | grep -o libvirt does not show the expected user name. WTH? Yet more /etc/group | grep virt does include libvirt:x:132:xxxyyy\nvirt-manager\nvagrant version\nfor net in $(virsh net-list --name); do virsh net-dhcp-leases ${net}; done\nvirsh --help\nvirsh --connect=qemu:///system list --all\nvirsh --connect=qemu:///system domiflist vm_default\nvirsh --connect=qemu:///system net-list --all \nsudo iptables -L -n | less looks like libvirt does update the iptables\nComing across how-to-disable-iptables-firewall-temporarily\nsudo su\niptables -F\niptables -X\niptables -P INPUT ACCEPT\niptables -P OUTPUT ACCEPT\niptables -P FORWARD ACCEPT\nvagrant up --provider libvirt\n==> default:  -- INPUT:             type=mouse, bus=ps2\n==> default: Creating shared folders metadata...\nError while activating network: Call to virNetworkCreate failed: internal error: Failed to apply firewall rules /usr/sbin/iptables -w --table filter --insert LIBVIRT_INP --in-interface virbr1 --protocol tcp --destination-port 67 --jump ACCEPT: iptables: No chain/target/match by that name.\nOK, I give up, no time for this anymore."
  },
  {
    "objectID": "posts/2022-06-10-conda-packages-conda-forge-2.html",
    "href": "posts/2022-06-10-conda-packages-conda-forge-2.html",
    "title": "Conda package for compiled native libraries",
    "section": "",
    "text": "I recently wrote about submitting your first conda package to conda-forge. You’ll find background on the “bigger picture” endeavour in that previous post.\nThis post is a follow-on with, perhaps, a second submission to conda-forge, this time not of a python package but a C++ library.\nI’ll try to package a relatively small C++ codebase MOIRAI: Manage C++ objects lifetime when exposed through a C API. While dealing with very similar needs as refcount (reference counting and memory management), there is no explicit dependency between them."
  },
  {
    "objectID": "posts/2022-06-10-conda-packages-conda-forge-2.html#starting-point",
    "href": "posts/2022-06-10-conda-packages-conda-forge-2.html#starting-point",
    "title": "Conda package for compiled native libraries",
    "section": "Starting point",
    "text": "Starting point\ngrayskull is userful to generate meta.yaml stubs out of python packages, and not applicable in this case.\nI learned the hard way that installing conda-build, grayskull and shyaml and conda update condain my conda base environment landed me in a broken mamba and incompatible package versions. So, this time create a new dedicate environment:\nconda create -n cf python=3.9 mamba -c conda-forge\nconda activate cf\nmamba install -c conda-forge conda-build grayskull # grayskull not for this post, but other future submissions\nIs there an example I can start from and work by inference and similarity rather than first principles?\nlibnetcdf-feedstock may be an appropriate case to start from, even if more sophisticated than my case. Rather than strip down this libnetcdf recipe though, I work from the example in the staged-recipe and add, staying closer to contributing packages\nI did bump into a couple of issues, but I got less issues than I thought to get a package compiling\nThe end result should be something like:\n\n{% set name = \"moirai\" %}\n{% set version = \"1.1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  # url: https://github.com/moirai/moirai/releases/download/{{ version }}/moirai-{{ version }}.tar.gz\n  # and otherwise fall back to archive:\n  url: https://github.com/csiro-hydroinformatics/moirai/archive/refs/tags/{{ version }}.tar.gz\n  sha256: b329353aee261ec42ddd57b7bb4ca5462186b2d132cdb2c9dacc9325899b85f3\n\nbuild:\n  number: 0\n\nrequirements:\n  build:\n    - cmake\n    - make  # [not win]\n    # - pkg-config  # [not win]\n    # - gnuconfig  # [unix]\n    - {{ compiler('c') }}\n    - {{ compiler('cxx') }}\n  run:\n    - curl # [win]\n    - libgcc # [unix]\n\ntest:\n  files:\n    - CMakeLists.txt\n  commands:\n    - ls\n\nabout:\n  home: https://github.com/csiro-hydroinformatics/moirai\n  summary: 'Manage C++ objects lifetime when exposed through a C API'\n  description: |\n    This C++ library is designed to help handling C++ objects from so called opaque pointers, via a C API, featuring:\n      * counting references via the C API to C++ domain objects\n      * handle C++ class inheritance even via opaque pointers\n      * mechanism for resilience to incorrect type casts\n      * thread-safe design\n  license: BSD-3-Clause\n  license_family: BSD\n  license_file: LICENSE.txt\n  doc_url: https://github.com/csiro-hydroinformatics/moirai/blob/master/doc/Walkthrough.md\n  dev_url: https://github.com/csiro-hydroinformatics/moirai\n\nextra:\n  recipe-maintainers:\n    - jmp75\n\n```yaml\n\nNote that you do need `make` as a build requirement besides `cmake`, otherwise you'd end up with :\n\n```text\nCMake Error: CMake was unable to find a build program corresponding to \"Unix Makefiles\".  CMAKE_MAKE_PROGRAM is not set.  You probably need to select a different build tool.\nCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\nCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\nbuild.sh:\n#!/bin/bash\n# Build moirai.\n\nmkdir -v -p build\ncd build\n\n# May be needed for unit tests down the track?\nexport TERM=\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PREFIX/lib\n\n# cmake -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_PREFIX_PATH=/usr/local -DCMAKE_MODULE_PATH=/usr/local/share/cmake/Modules/ -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON ..\ncmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${PREFIX} -DBUILD_SHARED_LIBS=ON ../\nmake -j 2 install\n# rm -rf ../build/ # not if we want to run the unit tests"
  },
  {
    "objectID": "posts/2022-06-10-conda-packages-conda-forge-2.html#building-locally-on-linux64",
    "href": "posts/2022-06-10-conda-packages-conda-forge-2.html#building-locally-on-linux64",
    "title": "Conda package for compiled native libraries",
    "section": "Building locally on linux64",
    "text": "Building locally on linux64\nNote: although it may have changed by the time you read this, at the time I write I may have to export DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64 to force that image being used. See previous post.\nconda activate cf\nexport DOCKER_IMAGE=quay.io/condaforge/linux-anvil-cos7-x86_64\n\ncd ~/src/staged-recipes\npython ./build-locally.py linux64\nand the build seems OK…\n+ touch /home/conda/staged-recipes/build_artifacts/conda-forge-build-done"
  },
  {
    "objectID": "posts/2022-06-10-conda-packages-conda-forge-2.html#building-locally-on-win64",
    "href": "posts/2022-06-10-conda-packages-conda-forge-2.html#building-locally-on-win64",
    "title": "Conda package for compiled native libraries",
    "section": "Building locally on win64",
    "text": "Building locally on win64\nTrying to run build-locally.py with the option win64 returns: ValueError: only Linux/macOS configs currently supported, got win64\nYou’ll want to read or re-read the conda-forge documentation Using cmake and particularities on windows\nTrying for bld.bat:\nREM Build moirai.\n@REM Credits: some material in this file are courtesy of Kavid Kent (ex Australian Bureau of Meteorology)\n\nmkdir build\ncd build\n\n@REM following may be unncessary\nset PATH=\"%PREFIX%\\bin\";%PATH%\n\n:: Configure using the CMakeFiles\ncmake -G \"%CMAKE_GENERATOR%\" ^\n      -DCMAKE_INSTALL_PREFIX:PATH=\"%LIBRARY_PREFIX%\" ^\n      -DCMAKE_PREFIX_PATH:PATH=\"%LIBRARY_PREFIX%\" ^\n      -DCMAKE_BUILD_TYPE:STRING=Release ^\n      ..\n\nif %errorlevel% neq 0 exit 1\nmsbuild /p:Configuration=Release /v:q /clp:/v:q \"INSTALL.vcxproj\"\nif %errorlevel% neq 0 exit 1\n@REM del *.*\nSince the local build script cannot emulate a win64 build, I am trying to set up on a Windows box and see what a conda build gives.\nI first tried on my Windows desktop where I do have vs 2019 and 2022 installed. Not sure whether they can be used. This section of the conda-forge doc seem to suggest Microsoft Build Tools for Visual Studio 2017 is required, but the link to the Python wiki page on Windows compilers is covering many more versions, so this is rather confusing. See the Appendix for more details on what happens in this case.\nTrying again on a windows machine, but installing visual studio build tools 2017.\nAfter creating the conda environment cf, similar to the above on Linux:\ncd c:\\src\\staged-recipes\\recipes\nconda build moirai\n“mostly” works. for a minute or two it seems to freeze at a “number of files” line:\nPackaging moirai\nINFO:conda_build.build:Packaging moirai\nPackaging moirai-1.1-h82bb817_0\nINFO:conda_build.build:Packaging moirai-1.1-h82bb817_0\nnumber of files: 11\nthen:\nPGO: UNKNOWN is not implemented yet!\nPGO: UNKNOWN is not implemented yet!\nUnknown format\nUnknown format\nUnknown format\nUnknown format\n   INFO: sysroot: 'C:/Windows/' files: '['zh-CN/winhlp32.exe.mui', 'zh-CN/twain_32.dll.mui', 'zh-CN/regedit.exe.mui', 'zh-CN/notepad.exe.mui']'\n\n<edit: snip>\n\nImporting conda-verify failed.  Please be sure to test your packages.  conda install conda-verify to make this message go away.\nconda search -c conda-forge conda-verify indeed returns something. Noted…\n<edit: snip>\n\n## Package Plan ##\n\n  environment location: C:\\Users\\xxxyyy\\Miniconda3\\envs\\cf\\conda-bld\\moirai_1654825059872\\_test_env\n\n\nThe following NEW packages will be INSTALLED:\n\n    ca-certificates: 2022.4.26-haa95532_0\n    curl:            7.82.0-h2bbff1b_0\n    libcurl:         7.82.0-h86230a5_0\n    libssh2:         1.10.0-hcd4344a_0\n    moirai:          1.1-h82bb817_0         local\n    openssl:         1.1.1o-h2bbff1b_0\n    vc:              14.2-h21ff451_1\n    vs2015_runtime:  14.27.29016-h5e58377_2\n    zlib:            1.2.12-h8cc25b3_2\n<edit: snip>\n\nset PREFIX=C:\\Users\\xxxyyy\\Miniconda3\\envs\\cf\\conda-bld\\moirai_1654825059872\\_test_env\nset SRC_DIR=C:\\Users\\xxxyyy\\Miniconda3\\envs\\cf\\conda-bld\\moirai_1654825059872\\test_tmp\n(cf) %SRC_DIR%>call \"%SRC_DIR%\\conda_test_env_vars.bat\"\n(cf) %SRC_DIR%>set \"CONDA_SHLVL=\"   &&\n(cf) %SRC_DIR%>conda activate \"%PREFIX%\"\n(%PREFIX%) %SRC_DIR%>IF 0 NEQ 0 exit /B 1\n(%PREFIX%) %SRC_DIR%>call \"%SRC_DIR%\\run_test.bat\"\n(%PREFIX%) %SRC_DIR%>ls\n(%PREFIX%) %SRC_DIR%>IF -1073741511 NEQ 0 exit /B 1\nThe latter fails because the command line ls borks with the error message (windows box error message) ls.exe - Entry Point not found. where ls returns C:\\Users\\xxxyyy\\Miniconda3\\envs\\cf\\Library\\usr\\bin\\ls.exe so this is something that came from conda.\nTaking a look at the resulting moirai-1.1-h82bb817_0.tar.bz2 file, the content looks sensible (include header files, bin/moirai.dll)\ninfo/hash_input.json\ninfo/index.json\ninfo/files\ninfo/paths.json\ninfo/about.json\ninfo/git\ninfo/recipe/build.sh\ninfo/recipe/meta.yaml.template\ninfo/licenses/LICENSE.txt\nLibrary/include/moirai/extern_c_api_as_opaque.h\nLibrary/include/moirai/extern_c_api_as_transparent.h\nLibrary/include/moirai/setup_modifiers.h\nLibrary/include/moirai/reference_handle_map_export.h\nLibrary/include/moirai/error_reporting.h\nLibrary/include/moirai/reference_handle.h\ninfo/recipe/conda_build_config.yaml\ninfo/recipe/meta.yaml\ninfo/test/run_test.bat\ninfo/recipe/bld.bat\nLibrary/bin/moirai.dll\nLibrary/include/moirai/reference_handle_test_helper.hpp\nLibrary/include/moirai/opaque_pointers.hpp\nLibrary/include/moirai/reference_type_converters.hpp\nLibrary/include/moirai/reference_handle.hpp"
  },
  {
    "objectID": "posts/2022-06-10-conda-packages-conda-forge-2.html#appendix-if-missing-ms-build-tools-2017",
    "href": "posts/2022-06-10-conda-packages-conda-forge-2.html#appendix-if-missing-ms-build-tools-2017",
    "title": "Conda package for compiled native libraries",
    "section": "Appendix: if missing ms build tools 2017",
    "text": "Appendix: if missing ms build tools 2017\nIf trying without having installed ms build tools 2017:\n%SRC_DIR%>CALL %BUILD_PREFIX%\\etc\\conda\\activate.d\\vs2017_get_vsinstall_dir.bat\nDid not find VSINSTALLDIR\nWindows SDK version found as: \"10.0.19041.0\"\nThe system cannot find the path specified.\nDid not find VSINSTALLDIR\nCMake Error at CMakeLists.txt:16 (PROJECT):\n  Generator\n\n    Visual Studio 15 2017 Win64\n\ncould not find any instance of Visual Studio.\n(cf) %SRC_DIR%>set \"SCRIPTS=%PREFIX%\\Scripts\"\n(cf) %SRC_DIR%>set \"LIBRARY_PREFIX=%PREFIX%\\Library\"\n(cf) %SRC_DIR%>set \"LIBRARY_BIN=%PREFIX%\\Library\\bin\"\n(cf) %SRC_DIR%>set \"LIBRARY_INC=%PREFIX%\\Library\\include\"\n(cf) %SRC_DIR%>set \"LIBRARY_LIB=%PREFIX%\\Library\\lib\"\n\n(cf) %SRC_DIR%>set \"c_compiler=vs2017\"\n(cf) %SRC_DIR%>set \"fortran_compiler=gfortran\"\n(cf) %SRC_DIR%>set \"vc=14\"\n(cf) %SRC_DIR%>set \"cxx_compiler=vs2017\"\ncall C:.bat\nNote:\nbase                  *  C:\\Users\\xxxyyy\\Miniconda3\\envs\\cf\n                         C:\\Users\\xxxyyy\\Miniconda3\\envs\\cf\\conda-bld\\moirai_1654760072022\\_build_env\n                         C:\\Users\\xxxyyy\\Miniconda3\\envs\\cf\\conda-bld\\moirai_1654760072022\\_h_env\nIf I try to start with Visual Studio 2022 Developer Command Prompt v17.1.5. Then conda environment activation, still:\n%SRC_DIR%>CALL %BUILD_PREFIX%\\etc\\conda\\activate.d\\vs2017_get_vsinstall_dir.bat\nDid not find VSINSTALLDIR\nWindows SDK version found as: \"10.0.19041.0\"\n**********************************************************************\n** Visual Studio 2022 Developer Command Prompt v17.1.5\n** Copyright (c) 2022 Microsoft Corporation\n**********************************************************************\n[ERROR:vcvars.bat] Toolset directory for version '14.16' was not found.\n[ERROR:VsDevCmd.bat] *** VsDevCmd.bat encountered errors. Environment may be incomplete and/or incorrect. ***\n[ERROR:VsDevCmd.bat] In an uninitialized command prompt, please 'set VSCMD_DEBUG=[value]' and then re-run\n[ERROR:VsDevCmd.bat] vsdevcmd.bat [args] for additional details.\n[ERROR:VsDevCmd.bat] Where [value] is:\n[ERROR:VsDevCmd.bat]    1 : basic debug logging\n[ERROR:VsDevCmd.bat]    2 : detailed debug logging\n[ERROR:VsDevCmd.bat]    3 : trace level logging. Redirection of output to a file when using this level is recommended.\n[ERROR:VsDevCmd.bat] Example: set VSCMD_DEBUG=3\n[ERROR:VsDevCmd.bat]          vsdevcmd.bat > vsdevcmd.trace.txt 2>&1\nDid not find VSINSTALLDIR\nCMake Error at CMakeLists.txt:16 (PROJECT):\n  Generator\n\n    Visual Studio 15 2017 Win64\n\n  could not find any instance of Visual Studio.\n-- Configuring incomplete, errors occurred!"
  },
  {
    "objectID": "posts/2022-06-10-conda-packages-conda-forge-2.html#resources",
    "href": "posts/2022-06-10-conda-packages-conda-forge-2.html#resources",
    "title": "Conda package for compiled native libraries",
    "section": "Resources",
    "text": "Resources\n\nReference Counting in Library Design – Optionally and with Union-Find Optimization"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "J-M’s ‘lab notebook’",
    "section": "",
    "text": "Because peer reviewed publication is not the sole vehicle for useful content.\nAlso, Why you (yes, you) should blog.\nMy workplace profile"
  },
  {
    "objectID": "about.html#source",
    "href": "about.html#source",
    "title": "J-M’s ‘lab notebook’",
    "section": "Source",
    "text": "Source\nThis blog site is built from this github repository with Quarto"
  }
]